{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vPSvsQ_V9v4",
        "outputId": "7fea6f01-1153-4b9e-b97d-0a648b039307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DXxAI873XvxU"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "import seaborn as sb\n",
        "import warnings \n",
        "\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4PSQcawHYhHo"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"drive/MyDrive/ST4248/data/resale_train_feature_selection.csv\")\n",
        "test_df = pd.read_csv(\"drive/MyDrive/ST4248/data/resale_test_feature_selection.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9cxcJZia3ZW"
      },
      "source": [
        "# Price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oytmvw_Gxwla"
      },
      "source": [
        "## Data Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z_sYbY5Qa2le"
      },
      "outputs": [],
      "source": [
        "X_train = train_df.drop(\"resale_price\", axis = 1)\n",
        "X_test = test_df.drop(\"resale_price\", axis = 1)\n",
        "\n",
        "y_train = train_df[\"resale_price\"]\n",
        "y_test = test_df[\"resale_price\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "wHUFQOmVRN2R",
        "outputId": "9119db77-38f0-450d-c9c6-d5f5651abd56"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-970b0057-ac3b-44ae-b216-786c5d52b4a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>floor_area_sqm</th>\n",
              "      <th>nearest_mrt_dist</th>\n",
              "      <th>total_resales_in_town</th>\n",
              "      <th>remaining_lease</th>\n",
              "      <th>town_BUKIT MERAH</th>\n",
              "      <th>street_name_ANG MO KIO ST 51</th>\n",
              "      <th>street_name_DAWSON RD</th>\n",
              "      <th>flat_type_3 ROOM</th>\n",
              "      <th>storey_range_01 TO 03</th>\n",
              "      <th>storey_range_04 TO 06</th>\n",
              "      <th>storey_range_19 TO 21</th>\n",
              "      <th>town_PASIR RIS</th>\n",
              "      <th>flat_model_Apartment</th>\n",
              "      <th>flat_model_Maisonette</th>\n",
              "      <th>flat_model_Premium Apartment</th>\n",
              "      <th>total_nearby_mrt</th>\n",
              "      <th>nearest_mall_dist</th>\n",
              "      <th>street_name_CANTONMENT RD</th>\n",
              "      <th>flat_model_Model A</th>\n",
              "      <th>flat_model_Improved</th>\n",
              "      <th>flat_model_DBSS</th>\n",
              "      <th>flat_model_New Generation</th>\n",
              "      <th>flat_model_Simplified</th>\n",
              "      <th>storey_range_16 TO 18</th>\n",
              "      <th>storey_range_07 TO 09</th>\n",
              "      <th>flat_model_Standard</th>\n",
              "      <th>flat_model_Type S1</th>\n",
              "      <th>nearest_bus_stop_Blk 18</th>\n",
              "      <th>street_name_JLN BAHAGIA</th>\n",
              "      <th>nearest_primary_school_ZHANGDE PRIMARY SCHOOL</th>\n",
              "      <th>town_CHOA CHU KANG</th>\n",
              "      <th>flat_type_2 ROOM</th>\n",
              "      <th>town_BUKIT PANJANG</th>\n",
              "      <th>town_HOUGANG</th>\n",
              "      <th>town_MARINE PARADE</th>\n",
              "      <th>town_SEMBAWANG</th>\n",
              "      <th>town_BUKIT BATOK</th>\n",
              "      <th>flat_type_EXECUTIVE</th>\n",
              "      <th>flat_type_5 ROOM</th>\n",
              "      <th>total_resales_in_block</th>\n",
              "      <th>nearest_mrt_Outram Park</th>\n",
              "      <th>town_ANG MO KIO</th>\n",
              "      <th>total_resales_in_street</th>\n",
              "      <th>nearest_primary_school_RADIN MAS PRIMARY SCHOOL</th>\n",
              "      <th>nearest_mrt_Admiralty</th>\n",
              "      <th>nearest_mrt_Kallang</th>\n",
              "      <th>nearest_mrt_Sengkang</th>\n",
              "      <th>nearest_school_WOODLANDS SECONDARY SCHOOL</th>\n",
              "      <th>nearest_mrt_Tiong Bahru</th>\n",
              "      <th>nearest_mrt_Toa Payoh</th>\n",
              "      <th>town_BEDOK</th>\n",
              "      <th>nearest_bus_stop_Opp Blk 565</th>\n",
              "      <th>nearest_school_BLANGAH RISE PRIMARY SCHOOL</th>\n",
              "      <th>nearest_school_GAN ENG SENG PRIMARY SCHOOL</th>\n",
              "      <th>nearest_school_KHENG CHENG SCHOOL</th>\n",
              "      <th>nearest_mall_Sun Plaza</th>\n",
              "      <th>street_name_ANG MO KIO ST 44</th>\n",
              "      <th>town_CENTRAL AREA</th>\n",
              "      <th>storey_range_10 TO 12</th>\n",
              "      <th>street_name_CLEMENTI AVE 3</th>\n",
              "      <th>flat_type_4 ROOM</th>\n",
              "      <th>street_name_MCNAIR RD</th>\n",
              "      <th>town_WOODLANDS</th>\n",
              "      <th>town_TAMPINES</th>\n",
              "      <th>town_SENGKANG</th>\n",
              "      <th>town_QUEENSTOWN</th>\n",
              "      <th>street_name_ANG MO KIO AVE 3</th>\n",
              "      <th>storey_range_22 TO 24</th>\n",
              "      <th>storey_range_25 TO 27</th>\n",
              "      <th>storey_range_28 TO 30</th>\n",
              "      <th>storey_range_31 TO 33</th>\n",
              "      <th>storey_range_37 TO 39</th>\n",
              "      <th>town_JURONG EAST</th>\n",
              "      <th>street_name_TELOK BLANGAH ST 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.132448</td>\n",
              "      <td>-0.676490</td>\n",
              "      <td>-0.443615</td>\n",
              "      <td>-1.143025</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>1.693641</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>-0.333963</td>\n",
              "      <td>-0.040003</td>\n",
              "      <td>1.391526</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.792027</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>2.632126</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.231190</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.777346</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>4.795832</td>\n",
              "      <td>0.073626</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>2.071076</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>-0.861523</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.010290</td>\n",
              "      <td>0.201327</td>\n",
              "      <td>1.252320</td>\n",
              "      <td>1.352967</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>2.178515</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>-0.333963</td>\n",
              "      <td>-0.040003</td>\n",
              "      <td>-1.475172</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>1.262584</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>-0.379921</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.231190</td>\n",
              "      <td>5.979130</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>0.477218</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>2.439851</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>-0.482841</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>-0.861523</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.166660</td>\n",
              "      <td>-0.384369</td>\n",
              "      <td>-0.198538</td>\n",
              "      <td>-0.290599</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>2.178515</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>-0.333963</td>\n",
              "      <td>-0.040003</td>\n",
              "      <td>-1.093044</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.792027</td>\n",
              "      <td>1.849609</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>-0.379921</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>4.325448</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>1.849609</td>\n",
              "      <td>-0.463705</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.577086</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>-0.482841</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>-0.861523</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.372423</td>\n",
              "      <td>1.731095</td>\n",
              "      <td>1.252320</td>\n",
              "      <td>-0.613741</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>-0.333963</td>\n",
              "      <td>-0.824070</td>\n",
              "      <td>0.124256</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>1.262584</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>-0.379921</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>1.854113</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.231190</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.463705</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.281308</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>-0.482841</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>1.160735</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.087399</td>\n",
              "      <td>1.054745</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>0.857112</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>1.842152</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>2.994345</td>\n",
              "      <td>-0.824070</td>\n",
              "      <td>-0.447118</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.792027</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>-0.379921</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.231190</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-1.090987</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>0.724338</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>-0.482841</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>1.160735</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-970b0057-ac3b-44ae-b216-786c5d52b4a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-970b0057-ac3b-44ae-b216-786c5d52b4a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-970b0057-ac3b-44ae-b216-786c5d52b4a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   floor_area_sqm  ...  street_name_TELOK BLANGAH ST 31\n",
              "0       -1.132448  ...                        -0.075507\n",
              "1       -2.010290  ...                        -0.075507\n",
              "2        1.166660  ...                        -0.075507\n",
              "3        0.372423  ...                        -0.075507\n",
              "4       -0.087399  ...                        -0.075507\n",
              "\n",
              "[5 rows x 74 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "zi5U5-GZRO_W",
        "outputId": "cb91cbb5-fa7c-4f19-b132-3446e1d360b7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-82eae1f1-532d-43e7-aff5-21374a2dbf9c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>floor_area_sqm</th>\n",
              "      <th>nearest_mrt_dist</th>\n",
              "      <th>total_resales_in_town</th>\n",
              "      <th>remaining_lease</th>\n",
              "      <th>town_BUKIT MERAH</th>\n",
              "      <th>street_name_ANG MO KIO ST 51</th>\n",
              "      <th>street_name_DAWSON RD</th>\n",
              "      <th>flat_type_3 ROOM</th>\n",
              "      <th>storey_range_01 TO 03</th>\n",
              "      <th>storey_range_04 TO 06</th>\n",
              "      <th>storey_range_19 TO 21</th>\n",
              "      <th>town_PASIR RIS</th>\n",
              "      <th>flat_model_Apartment</th>\n",
              "      <th>flat_model_Maisonette</th>\n",
              "      <th>flat_model_Premium Apartment</th>\n",
              "      <th>total_nearby_mrt</th>\n",
              "      <th>nearest_mall_dist</th>\n",
              "      <th>street_name_CANTONMENT RD</th>\n",
              "      <th>flat_model_Model A</th>\n",
              "      <th>flat_model_Improved</th>\n",
              "      <th>flat_model_DBSS</th>\n",
              "      <th>flat_model_New Generation</th>\n",
              "      <th>flat_model_Simplified</th>\n",
              "      <th>storey_range_16 TO 18</th>\n",
              "      <th>storey_range_07 TO 09</th>\n",
              "      <th>flat_model_Standard</th>\n",
              "      <th>flat_model_Type S1</th>\n",
              "      <th>nearest_bus_stop_Blk 18</th>\n",
              "      <th>street_name_JLN BAHAGIA</th>\n",
              "      <th>nearest_primary_school_ZHANGDE PRIMARY SCHOOL</th>\n",
              "      <th>town_CHOA CHU KANG</th>\n",
              "      <th>flat_type_2 ROOM</th>\n",
              "      <th>town_BUKIT PANJANG</th>\n",
              "      <th>town_HOUGANG</th>\n",
              "      <th>town_MARINE PARADE</th>\n",
              "      <th>town_SEMBAWANG</th>\n",
              "      <th>town_BUKIT BATOK</th>\n",
              "      <th>flat_type_EXECUTIVE</th>\n",
              "      <th>flat_type_5 ROOM</th>\n",
              "      <th>total_resales_in_block</th>\n",
              "      <th>nearest_mrt_Outram Park</th>\n",
              "      <th>town_ANG MO KIO</th>\n",
              "      <th>total_resales_in_street</th>\n",
              "      <th>nearest_primary_school_RADIN MAS PRIMARY SCHOOL</th>\n",
              "      <th>nearest_mrt_Admiralty</th>\n",
              "      <th>nearest_mrt_Kallang</th>\n",
              "      <th>nearest_mrt_Sengkang</th>\n",
              "      <th>nearest_school_WOODLANDS SECONDARY SCHOOL</th>\n",
              "      <th>nearest_mrt_Tiong Bahru</th>\n",
              "      <th>nearest_mrt_Toa Payoh</th>\n",
              "      <th>town_BEDOK</th>\n",
              "      <th>nearest_bus_stop_Opp Blk 565</th>\n",
              "      <th>nearest_school_BLANGAH RISE PRIMARY SCHOOL</th>\n",
              "      <th>nearest_school_GAN ENG SENG PRIMARY SCHOOL</th>\n",
              "      <th>nearest_school_KHENG CHENG SCHOOL</th>\n",
              "      <th>nearest_mall_Sun Plaza</th>\n",
              "      <th>street_name_ANG MO KIO ST 44</th>\n",
              "      <th>town_CENTRAL AREA</th>\n",
              "      <th>storey_range_10 TO 12</th>\n",
              "      <th>street_name_CLEMENTI AVE 3</th>\n",
              "      <th>flat_type_4 ROOM</th>\n",
              "      <th>street_name_MCNAIR RD</th>\n",
              "      <th>town_WOODLANDS</th>\n",
              "      <th>town_TAMPINES</th>\n",
              "      <th>town_SENGKANG</th>\n",
              "      <th>town_QUEENSTOWN</th>\n",
              "      <th>street_name_ANG MO KIO AVE 3</th>\n",
              "      <th>storey_range_22 TO 24</th>\n",
              "      <th>storey_range_25 TO 27</th>\n",
              "      <th>storey_range_28 TO 30</th>\n",
              "      <th>storey_range_31 TO 33</th>\n",
              "      <th>storey_range_37 TO 39</th>\n",
              "      <th>town_JURONG EAST</th>\n",
              "      <th>street_name_TELOK BLANGAH ST 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.212805</td>\n",
              "      <td>0.004724</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>1.224825</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>-0.333963</td>\n",
              "      <td>-0.040003</td>\n",
              "      <td>-0.256180</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>1.262584</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>-0.379921</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.23119</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>0.477218</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>0.960961</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>2.071076</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>1.160735</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.087399</td>\n",
              "      <td>1.306314</td>\n",
              "      <td>0.242602</td>\n",
              "      <td>1.403110</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>-0.333963</td>\n",
              "      <td>-0.824070</td>\n",
              "      <td>-0.634544</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>1.262584</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>-0.379921</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.23119</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.150064</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.044685</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>-0.482841</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>1.160735</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.038007</td>\n",
              "      <td>-0.341318</td>\n",
              "      <td>0.644528</td>\n",
              "      <td>-0.808741</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>1.842152</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>-0.333963</td>\n",
              "      <td>0.744063</td>\n",
              "      <td>0.315323</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.792027</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>2.632126</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.23119</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-1.090987</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.222152</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>-0.482841</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>1.160735</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>3.613278</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.748640</td>\n",
              "      <td>1.265904</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>1.386396</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>2.994345</td>\n",
              "      <td>-0.824070</td>\n",
              "      <td>-1.260224</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.792027</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>-0.379921</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.23119</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>1.849609</td>\n",
              "      <td>1.418141</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>1.670828</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.235914</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>2.071076</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>-0.861523</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.547221</td>\n",
              "      <td>-0.978913</td>\n",
              "      <td>0.036737</td>\n",
              "      <td>-1.209882</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>1.693641</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.162716</td>\n",
              "      <td>-0.177705</td>\n",
              "      <td>-0.154259</td>\n",
              "      <td>-0.333963</td>\n",
              "      <td>-0.040003</td>\n",
              "      <td>0.653930</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.792027</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>-0.116198</td>\n",
              "      <td>2.632126</td>\n",
              "      <td>-0.216528</td>\n",
              "      <td>-0.200236</td>\n",
              "      <td>-0.539341</td>\n",
              "      <td>-0.174281</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.041274</td>\n",
              "      <td>-0.091039</td>\n",
              "      <td>-0.23119</td>\n",
              "      <td>-0.167248</td>\n",
              "      <td>-0.173416</td>\n",
              "      <td>-0.235244</td>\n",
              "      <td>-0.081007</td>\n",
              "      <td>-0.191657</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>-0.256634</td>\n",
              "      <td>-0.540655</td>\n",
              "      <td>0.163577</td>\n",
              "      <td>-0.069584</td>\n",
              "      <td>-0.208514</td>\n",
              "      <td>0.546871</td>\n",
              "      <td>-0.065344</td>\n",
              "      <td>-0.224305</td>\n",
              "      <td>-0.073584</td>\n",
              "      <td>-0.305965</td>\n",
              "      <td>-0.077382</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>4.238837</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.104347</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.135926</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.098646</td>\n",
              "      <td>-0.482841</td>\n",
              "      <td>-0.055926</td>\n",
              "      <td>-0.861523</td>\n",
              "      <td>-0.053315</td>\n",
              "      <td>-0.309827</td>\n",
              "      <td>-0.276757</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82eae1f1-532d-43e7-aff5-21374a2dbf9c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82eae1f1-532d-43e7-aff5-21374a2dbf9c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82eae1f1-532d-43e7-aff5-21374a2dbf9c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   floor_area_sqm  ...  street_name_TELOK BLANGAH ST 31\n",
              "0       -0.212805  ...                        -0.075507\n",
              "1       -0.087399  ...                        -0.075507\n",
              "2        0.038007  ...                        -0.075507\n",
              "3        0.748640  ...                        -0.075507\n",
              "4       -0.547221  ...                        -0.075507\n",
              "\n",
              "[5 rows x 74 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPKYTh4wtinr"
      },
      "source": [
        "## Make the Deep Neural Network\n",
        " * Define a sequential model\n",
        " * Add some dense layers\n",
        " * Use '**relu**' as the activation function in the hidden layers\n",
        " * Use a '**normal**' initializer as the kernal_intializer \n",
        "           Initializers define the way to set the initial random weights of Keras layers.\n",
        " * We will use mean_absolute_error as a loss function\n",
        " * Define the output layer with only one node\n",
        " * Use 'linear 'as the activation function for the output layer\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CsJsrFlIvmzf"
      },
      "outputs": [],
      "source": [
        "NN_model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrZVKcMbwCcI"
      },
      "source": [
        "**The Input Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ILFBftZnvqFj"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6RW_lRRwG2i"
      },
      "source": [
        "**The Hidden Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yz61h9vLv7xz"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHSMp0zJwKRc"
      },
      "source": [
        "**The Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S9v_kTrsv-38"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHpg10glxNam"
      },
      "source": [
        "**Compile the network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROXr07cAv-pW",
        "outputId": "e4d419e7-3e2a-467e-b362-6ae8f06eb361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               9600      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 174,465\n",
            "Trainable params: 174,465\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYgT8VgWxicp"
      },
      "source": [
        "## Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGEt2nCHzMZ0",
        "outputId": "5d797a09-92e3-4a23-bd03-4be939f651b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "89/89 [==============================] - 2s 10ms/step - loss: 536743.7500 - mean_absolute_error: 536743.7500 - val_loss: 397827.2500 - val_mean_absolute_error: 397827.2500\n",
            "Epoch 2/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 134036.4688 - mean_absolute_error: 134036.4688 - val_loss: 65537.7031 - val_mean_absolute_error: 65537.7031\n",
            "Epoch 3/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 55781.7734 - mean_absolute_error: 55781.7734 - val_loss: 50988.5625 - val_mean_absolute_error: 50988.5625\n",
            "Epoch 4/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 45372.0234 - mean_absolute_error: 45372.0234 - val_loss: 43434.9531 - val_mean_absolute_error: 43434.9531\n",
            "Epoch 5/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 39795.4375 - mean_absolute_error: 39795.4375 - val_loss: 42792.1250 - val_mean_absolute_error: 42792.1250\n",
            "Epoch 6/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 37335.9023 - mean_absolute_error: 37335.9023 - val_loss: 39419.2227 - val_mean_absolute_error: 39419.2227\n",
            "Epoch 7/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 35173.9805 - mean_absolute_error: 35173.9805 - val_loss: 38083.0508 - val_mean_absolute_error: 38083.0508\n",
            "Epoch 8/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 34055.7461 - mean_absolute_error: 34055.7461 - val_loss: 38170.1797 - val_mean_absolute_error: 38170.1797\n",
            "Epoch 9/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 32821.7812 - mean_absolute_error: 32821.7812 - val_loss: 37313.2422 - val_mean_absolute_error: 37313.2422\n",
            "Epoch 10/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 32454.8320 - mean_absolute_error: 32454.8320 - val_loss: 37916.9727 - val_mean_absolute_error: 37916.9727\n",
            "Epoch 11/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 31329.4902 - mean_absolute_error: 31329.4902 - val_loss: 34978.5430 - val_mean_absolute_error: 34978.5430\n",
            "Epoch 12/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 30670.2988 - mean_absolute_error: 30670.2988 - val_loss: 36257.5391 - val_mean_absolute_error: 36257.5391\n",
            "Epoch 13/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 29704.6738 - mean_absolute_error: 29704.6738 - val_loss: 35076.2344 - val_mean_absolute_error: 35076.2344\n",
            "Epoch 14/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 29120.7930 - mean_absolute_error: 29120.7930 - val_loss: 34109.9805 - val_mean_absolute_error: 34109.9805\n",
            "Epoch 15/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 29343.7988 - mean_absolute_error: 29343.7988 - val_loss: 35167.6758 - val_mean_absolute_error: 35167.6758\n",
            "Epoch 16/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 28614.3164 - mean_absolute_error: 28614.3164 - val_loss: 35708.6992 - val_mean_absolute_error: 35708.6992\n",
            "Epoch 17/500\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 28752.0020 - mean_absolute_error: 28752.0020 - val_loss: 33750.8086 - val_mean_absolute_error: 33750.8086\n",
            "Epoch 18/500\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 28038.8613 - mean_absolute_error: 28038.8613 - val_loss: 33827.9414 - val_mean_absolute_error: 33827.9414\n",
            "Epoch 19/500\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 27363.0000 - mean_absolute_error: 27363.0000 - val_loss: 33416.3164 - val_mean_absolute_error: 33416.3164\n",
            "Epoch 20/500\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 27333.7012 - mean_absolute_error: 27333.7012 - val_loss: 33659.0352 - val_mean_absolute_error: 33659.0352\n",
            "Epoch 21/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 26951.1211 - mean_absolute_error: 26951.1211 - val_loss: 32829.2930 - val_mean_absolute_error: 32829.2930\n",
            "Epoch 22/500\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 27073.0469 - mean_absolute_error: 27073.0469 - val_loss: 31989.0566 - val_mean_absolute_error: 31989.0566\n",
            "Epoch 23/500\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 26632.6523 - mean_absolute_error: 26632.6523 - val_loss: 32519.9102 - val_mean_absolute_error: 32519.9102\n",
            "Epoch 24/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 26278.2148 - mean_absolute_error: 26278.2148 - val_loss: 33132.0625 - val_mean_absolute_error: 33132.0625\n",
            "Epoch 25/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 26038.7324 - mean_absolute_error: 26038.7324 - val_loss: 32302.4727 - val_mean_absolute_error: 32302.4727\n",
            "Epoch 26/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 25580.3223 - mean_absolute_error: 25580.3223 - val_loss: 32720.1055 - val_mean_absolute_error: 32720.1055\n",
            "Epoch 27/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 25704.9023 - mean_absolute_error: 25704.9023 - val_loss: 32626.1641 - val_mean_absolute_error: 32626.1641\n",
            "Epoch 28/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 25580.7266 - mean_absolute_error: 25580.7266 - val_loss: 35342.5352 - val_mean_absolute_error: 35342.5352\n",
            "Epoch 29/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 25420.6465 - mean_absolute_error: 25420.6465 - val_loss: 33327.8789 - val_mean_absolute_error: 33327.8789\n",
            "Epoch 30/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 25275.8535 - mean_absolute_error: 25275.8535 - val_loss: 31874.9434 - val_mean_absolute_error: 31874.9434\n",
            "Epoch 31/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 25043.0312 - mean_absolute_error: 25043.0312 - val_loss: 31320.0742 - val_mean_absolute_error: 31320.0742\n",
            "Epoch 32/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 24744.3789 - mean_absolute_error: 24744.3789 - val_loss: 31514.7285 - val_mean_absolute_error: 31514.7285\n",
            "Epoch 33/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 24165.0625 - mean_absolute_error: 24165.0625 - val_loss: 32155.9883 - val_mean_absolute_error: 32155.9883\n",
            "Epoch 34/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 24252.4941 - mean_absolute_error: 24252.4941 - val_loss: 31432.2383 - val_mean_absolute_error: 31432.2383\n",
            "Epoch 35/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 23999.3027 - mean_absolute_error: 23999.3027 - val_loss: 33049.9805 - val_mean_absolute_error: 33049.9805\n",
            "Epoch 36/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 23982.7305 - mean_absolute_error: 23982.7305 - val_loss: 31487.5781 - val_mean_absolute_error: 31487.5781\n",
            "Epoch 37/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 24322.3555 - mean_absolute_error: 24322.3555 - val_loss: 33516.5195 - val_mean_absolute_error: 33516.5195\n",
            "Epoch 38/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 23679.9453 - mean_absolute_error: 23679.9453 - val_loss: 32871.5000 - val_mean_absolute_error: 32871.5000\n",
            "Epoch 39/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 23556.6250 - mean_absolute_error: 23556.6250 - val_loss: 30752.9609 - val_mean_absolute_error: 30752.9609\n",
            "Epoch 40/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 23518.9199 - mean_absolute_error: 23518.9199 - val_loss: 32345.5645 - val_mean_absolute_error: 32345.5645\n",
            "Epoch 41/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 23070.4883 - mean_absolute_error: 23070.4883 - val_loss: 30845.8066 - val_mean_absolute_error: 30845.8066\n",
            "Epoch 42/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 22954.0977 - mean_absolute_error: 22954.0977 - val_loss: 30502.7188 - val_mean_absolute_error: 30502.7188\n",
            "Epoch 43/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 22816.7461 - mean_absolute_error: 22816.7461 - val_loss: 31475.0488 - val_mean_absolute_error: 31475.0488\n",
            "Epoch 44/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 22818.9258 - mean_absolute_error: 22818.9258 - val_loss: 30092.5801 - val_mean_absolute_error: 30092.5801\n",
            "Epoch 45/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 22412.9922 - mean_absolute_error: 22412.9922 - val_loss: 30922.7441 - val_mean_absolute_error: 30922.7441\n",
            "Epoch 46/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 22150.3691 - mean_absolute_error: 22150.3691 - val_loss: 32033.3770 - val_mean_absolute_error: 32033.3770\n",
            "Epoch 47/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 22709.7773 - mean_absolute_error: 22709.7773 - val_loss: 31897.3145 - val_mean_absolute_error: 31897.3145\n",
            "Epoch 48/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 22014.7383 - mean_absolute_error: 22014.7383 - val_loss: 30474.3730 - val_mean_absolute_error: 30474.3730\n",
            "Epoch 49/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 22316.7773 - mean_absolute_error: 22316.7773 - val_loss: 30950.2520 - val_mean_absolute_error: 30950.2520\n",
            "Epoch 50/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 21939.8398 - mean_absolute_error: 21939.8398 - val_loss: 31307.8242 - val_mean_absolute_error: 31307.8242\n",
            "Epoch 51/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 21831.7852 - mean_absolute_error: 21831.7852 - val_loss: 32170.1895 - val_mean_absolute_error: 32170.1895\n",
            "Epoch 52/500\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 21413.4785 - mean_absolute_error: 21413.4785 - val_loss: 30861.6836 - val_mean_absolute_error: 30861.6836\n",
            "Epoch 53/500\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 22337.4668 - mean_absolute_error: 22337.4668 - val_loss: 30669.6309 - val_mean_absolute_error: 30669.6309\n",
            "Epoch 54/500\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 21663.7246 - mean_absolute_error: 21663.7246 - val_loss: 31811.0703 - val_mean_absolute_error: 31811.0703\n",
            "Epoch 55/500\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 21624.6523 - mean_absolute_error: 21624.6523 - val_loss: 31012.5605 - val_mean_absolute_error: 31012.5605\n",
            "Epoch 56/500\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 21280.9004 - mean_absolute_error: 21280.9004 - val_loss: 32712.7754 - val_mean_absolute_error: 32712.7754\n",
            "Epoch 57/500\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 21788.9570 - mean_absolute_error: 21788.9570 - val_loss: 30042.5918 - val_mean_absolute_error: 30042.5918\n",
            "Epoch 58/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 21782.7070 - mean_absolute_error: 21782.7070 - val_loss: 30108.7227 - val_mean_absolute_error: 30108.7227\n",
            "Epoch 59/500\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 20880.8301 - mean_absolute_error: 20880.8301 - val_loss: 30175.8223 - val_mean_absolute_error: 30175.8223\n",
            "Epoch 60/500\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 21838.8613 - mean_absolute_error: 21838.8613 - val_loss: 31387.2656 - val_mean_absolute_error: 31387.2656\n",
            "Epoch 61/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 21248.0703 - mean_absolute_error: 21248.0703 - val_loss: 30671.6523 - val_mean_absolute_error: 30671.6523\n",
            "Epoch 62/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20596.7246 - mean_absolute_error: 20596.7246 - val_loss: 30159.4355 - val_mean_absolute_error: 30159.4355\n",
            "Epoch 63/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20720.5781 - mean_absolute_error: 20720.5781 - val_loss: 30237.0078 - val_mean_absolute_error: 30237.0078\n",
            "Epoch 64/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20755.3086 - mean_absolute_error: 20755.3086 - val_loss: 29967.3848 - val_mean_absolute_error: 29967.3848\n",
            "Epoch 65/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 21059.2930 - mean_absolute_error: 21059.2930 - val_loss: 30934.1582 - val_mean_absolute_error: 30934.1582\n",
            "Epoch 66/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 20880.7363 - mean_absolute_error: 20880.7363 - val_loss: 30158.5996 - val_mean_absolute_error: 30158.5996\n",
            "Epoch 67/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20768.1309 - mean_absolute_error: 20768.1309 - val_loss: 30145.0996 - val_mean_absolute_error: 30145.0996\n",
            "Epoch 68/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20404.3027 - mean_absolute_error: 20404.3027 - val_loss: 29845.6777 - val_mean_absolute_error: 29845.6777\n",
            "Epoch 69/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20520.2500 - mean_absolute_error: 20520.2500 - val_loss: 30837.2891 - val_mean_absolute_error: 30837.2891\n",
            "Epoch 70/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 20707.2070 - mean_absolute_error: 20707.2070 - val_loss: 30003.7031 - val_mean_absolute_error: 30003.7031\n",
            "Epoch 71/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20700.1445 - mean_absolute_error: 20700.1445 - val_loss: 30383.7422 - val_mean_absolute_error: 30383.7422\n",
            "Epoch 72/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20701.9609 - mean_absolute_error: 20701.9609 - val_loss: 30090.0996 - val_mean_absolute_error: 30090.0996\n",
            "Epoch 73/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20464.7363 - mean_absolute_error: 20464.7363 - val_loss: 30254.2520 - val_mean_absolute_error: 30254.2520\n",
            "Epoch 74/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20439.5625 - mean_absolute_error: 20439.5625 - val_loss: 30272.2832 - val_mean_absolute_error: 30272.2832\n",
            "Epoch 75/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 20769.8945 - mean_absolute_error: 20769.8945 - val_loss: 31024.0645 - val_mean_absolute_error: 31024.0645\n",
            "Epoch 76/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20172.3340 - mean_absolute_error: 20172.3340 - val_loss: 31183.8477 - val_mean_absolute_error: 31183.8477\n",
            "Epoch 77/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20785.7500 - mean_absolute_error: 20785.7500 - val_loss: 29632.7109 - val_mean_absolute_error: 29632.7109\n",
            "Epoch 78/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20365.0625 - mean_absolute_error: 20365.0625 - val_loss: 30316.2344 - val_mean_absolute_error: 30316.2344\n",
            "Epoch 79/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20195.2168 - mean_absolute_error: 20195.2168 - val_loss: 29765.7285 - val_mean_absolute_error: 29765.7285\n",
            "Epoch 80/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19804.9629 - mean_absolute_error: 19804.9629 - val_loss: 29899.0293 - val_mean_absolute_error: 29899.0293\n",
            "Epoch 81/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 20031.4629 - mean_absolute_error: 20031.4629 - val_loss: 31321.6680 - val_mean_absolute_error: 31321.6680\n",
            "Epoch 82/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19769.4902 - mean_absolute_error: 19769.4902 - val_loss: 29776.5957 - val_mean_absolute_error: 29776.5957\n",
            "Epoch 83/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20147.9082 - mean_absolute_error: 20147.9082 - val_loss: 32082.4980 - val_mean_absolute_error: 32082.4980\n",
            "Epoch 84/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20164.7676 - mean_absolute_error: 20164.7676 - val_loss: 29920.5781 - val_mean_absolute_error: 29920.5781\n",
            "Epoch 85/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19695.2539 - mean_absolute_error: 19695.2539 - val_loss: 29587.1816 - val_mean_absolute_error: 29587.1816\n",
            "Epoch 86/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19649.5273 - mean_absolute_error: 19649.5273 - val_loss: 29613.9375 - val_mean_absolute_error: 29613.9375\n",
            "Epoch 87/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 19439.7559 - mean_absolute_error: 19439.7559 - val_loss: 29488.0293 - val_mean_absolute_error: 29488.0293\n",
            "Epoch 88/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19568.1738 - mean_absolute_error: 19568.1738 - val_loss: 29695.9238 - val_mean_absolute_error: 29695.9238\n",
            "Epoch 89/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19592.5430 - mean_absolute_error: 19592.5430 - val_loss: 29035.5742 - val_mean_absolute_error: 29035.5742\n",
            "Epoch 90/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19344.4219 - mean_absolute_error: 19344.4219 - val_loss: 30399.7930 - val_mean_absolute_error: 30399.7930\n",
            "Epoch 91/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19604.1914 - mean_absolute_error: 19604.1914 - val_loss: 29899.7109 - val_mean_absolute_error: 29899.7109\n",
            "Epoch 92/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19445.9863 - mean_absolute_error: 19445.9863 - val_loss: 29403.5352 - val_mean_absolute_error: 29403.5352\n",
            "Epoch 93/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19090.1992 - mean_absolute_error: 19090.1992 - val_loss: 31427.5645 - val_mean_absolute_error: 31427.5645\n",
            "Epoch 94/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 19396.9219 - mean_absolute_error: 19396.9219 - val_loss: 30558.3340 - val_mean_absolute_error: 30558.3340\n",
            "Epoch 95/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19620.0898 - mean_absolute_error: 19620.0898 - val_loss: 29362.0176 - val_mean_absolute_error: 29362.0176\n",
            "Epoch 96/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19273.5781 - mean_absolute_error: 19273.5781 - val_loss: 29546.0898 - val_mean_absolute_error: 29546.0898\n",
            "Epoch 97/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18798.7559 - mean_absolute_error: 18798.7559 - val_loss: 29553.0703 - val_mean_absolute_error: 29553.0703\n",
            "Epoch 98/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19024.7207 - mean_absolute_error: 19024.7207 - val_loss: 30121.7363 - val_mean_absolute_error: 30121.7363\n",
            "Epoch 99/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19475.0410 - mean_absolute_error: 19475.0410 - val_loss: 30107.3340 - val_mean_absolute_error: 30107.3340\n",
            "Epoch 100/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 18896.8555 - mean_absolute_error: 18896.8555 - val_loss: 29839.0898 - val_mean_absolute_error: 29839.0898\n",
            "Epoch 101/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19501.1680 - mean_absolute_error: 19501.1680 - val_loss: 30025.8008 - val_mean_absolute_error: 30025.8008\n",
            "Epoch 102/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 19327.9922 - mean_absolute_error: 19327.9922 - val_loss: 29791.1699 - val_mean_absolute_error: 29791.1699\n",
            "Epoch 103/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 19201.6797 - mean_absolute_error: 19201.6797 - val_loss: 30085.8789 - val_mean_absolute_error: 30085.8789\n",
            "Epoch 104/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18974.0703 - mean_absolute_error: 18974.0703 - val_loss: 29767.4766 - val_mean_absolute_error: 29767.4766\n",
            "Epoch 105/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18954.0078 - mean_absolute_error: 18954.0078 - val_loss: 30503.9746 - val_mean_absolute_error: 30503.9746\n",
            "Epoch 106/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18568.6074 - mean_absolute_error: 18568.6074 - val_loss: 30658.3633 - val_mean_absolute_error: 30658.3633\n",
            "Epoch 107/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18877.8789 - mean_absolute_error: 18877.8789 - val_loss: 29700.5527 - val_mean_absolute_error: 29700.5527\n",
            "Epoch 108/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18866.0527 - mean_absolute_error: 18866.0527 - val_loss: 29953.5469 - val_mean_absolute_error: 29953.5469\n",
            "Epoch 109/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18814.5508 - mean_absolute_error: 18814.5508 - val_loss: 29792.2910 - val_mean_absolute_error: 29792.2910\n",
            "Epoch 110/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18883.0781 - mean_absolute_error: 18883.0781 - val_loss: 30003.3965 - val_mean_absolute_error: 30003.3965\n",
            "Epoch 111/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18694.3750 - mean_absolute_error: 18694.3750 - val_loss: 28953.8984 - val_mean_absolute_error: 28953.8984\n",
            "Epoch 112/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18387.8379 - mean_absolute_error: 18387.8379 - val_loss: 30122.9863 - val_mean_absolute_error: 30122.9863\n",
            "Epoch 113/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18784.9375 - mean_absolute_error: 18784.9375 - val_loss: 29577.6855 - val_mean_absolute_error: 29577.6855\n",
            "Epoch 114/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 18574.3379 - mean_absolute_error: 18574.3379 - val_loss: 31041.9004 - val_mean_absolute_error: 31041.9004\n",
            "Epoch 115/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18563.8984 - mean_absolute_error: 18563.8984 - val_loss: 30429.9863 - val_mean_absolute_error: 30429.9863\n",
            "Epoch 116/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19280.3301 - mean_absolute_error: 19280.3301 - val_loss: 29289.2969 - val_mean_absolute_error: 29289.2969\n",
            "Epoch 117/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19027.9785 - mean_absolute_error: 19027.9785 - val_loss: 30285.2969 - val_mean_absolute_error: 30285.2969\n",
            "Epoch 118/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18014.3281 - mean_absolute_error: 18014.3281 - val_loss: 30282.2070 - val_mean_absolute_error: 30282.2070\n",
            "Epoch 119/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18530.9531 - mean_absolute_error: 18530.9531 - val_loss: 29547.2695 - val_mean_absolute_error: 29547.2695\n",
            "Epoch 120/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18261.7637 - mean_absolute_error: 18261.7637 - val_loss: 30668.4824 - val_mean_absolute_error: 30668.4824\n",
            "Epoch 121/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18651.2344 - mean_absolute_error: 18651.2344 - val_loss: 29818.0684 - val_mean_absolute_error: 29818.0684\n",
            "Epoch 122/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18350.7051 - mean_absolute_error: 18350.7051 - val_loss: 29614.3438 - val_mean_absolute_error: 29614.3438\n",
            "Epoch 123/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 18461.4258 - mean_absolute_error: 18461.4258 - val_loss: 29675.6094 - val_mean_absolute_error: 29675.6094\n",
            "Epoch 124/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 18144.7656 - mean_absolute_error: 18144.7656 - val_loss: 30174.6992 - val_mean_absolute_error: 30174.6992\n",
            "Epoch 125/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18165.0000 - mean_absolute_error: 18165.0000 - val_loss: 29543.7441 - val_mean_absolute_error: 29543.7441\n",
            "Epoch 126/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18153.5332 - mean_absolute_error: 18153.5332 - val_loss: 29883.4922 - val_mean_absolute_error: 29883.4922\n",
            "Epoch 127/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18281.9863 - mean_absolute_error: 18281.9863 - val_loss: 31263.8438 - val_mean_absolute_error: 31263.8438\n",
            "Epoch 128/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18682.1895 - mean_absolute_error: 18682.1895 - val_loss: 30072.8535 - val_mean_absolute_error: 30072.8535\n",
            "Epoch 129/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17985.0938 - mean_absolute_error: 17985.0938 - val_loss: 29972.2188 - val_mean_absolute_error: 29972.2188\n",
            "Epoch 130/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18112.1152 - mean_absolute_error: 18112.1152 - val_loss: 31340.4473 - val_mean_absolute_error: 31340.4473\n",
            "Epoch 131/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18112.3496 - mean_absolute_error: 18112.3496 - val_loss: 29693.2637 - val_mean_absolute_error: 29693.2637\n",
            "Epoch 132/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17895.7266 - mean_absolute_error: 17895.7266 - val_loss: 30021.0898 - val_mean_absolute_error: 30021.0898\n",
            "Epoch 133/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18629.5820 - mean_absolute_error: 18629.5820 - val_loss: 32139.9258 - val_mean_absolute_error: 32139.9258\n",
            "Epoch 134/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18204.3691 - mean_absolute_error: 18204.3691 - val_loss: 30121.2207 - val_mean_absolute_error: 30121.2207\n",
            "Epoch 135/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18304.1152 - mean_absolute_error: 18304.1152 - val_loss: 30179.5918 - val_mean_absolute_error: 30179.5918\n",
            "Epoch 136/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18230.7812 - mean_absolute_error: 18230.7812 - val_loss: 29445.9102 - val_mean_absolute_error: 29445.9102\n",
            "Epoch 137/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17786.1738 - mean_absolute_error: 17786.1738 - val_loss: 29902.6992 - val_mean_absolute_error: 29902.6992\n",
            "Epoch 138/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18129.1309 - mean_absolute_error: 18129.1309 - val_loss: 30022.1699 - val_mean_absolute_error: 30022.1699\n",
            "Epoch 139/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17843.1133 - mean_absolute_error: 17843.1133 - val_loss: 29982.8887 - val_mean_absolute_error: 29982.8887\n",
            "Epoch 140/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17547.9844 - mean_absolute_error: 17547.9844 - val_loss: 30441.9941 - val_mean_absolute_error: 30441.9941\n",
            "Epoch 141/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18001.1191 - mean_absolute_error: 18001.1191 - val_loss: 31643.8867 - val_mean_absolute_error: 31643.8867\n",
            "Epoch 142/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18082.9180 - mean_absolute_error: 18082.9180 - val_loss: 29712.8848 - val_mean_absolute_error: 29712.8848\n",
            "Epoch 143/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 18232.9375 - mean_absolute_error: 18232.9375 - val_loss: 29294.5078 - val_mean_absolute_error: 29294.5078\n",
            "Epoch 144/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18016.1680 - mean_absolute_error: 18016.1680 - val_loss: 29899.8613 - val_mean_absolute_error: 29899.8613\n",
            "Epoch 145/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17669.6152 - mean_absolute_error: 17669.6152 - val_loss: 30025.7676 - val_mean_absolute_error: 30025.7676\n",
            "Epoch 146/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17681.7461 - mean_absolute_error: 17681.7461 - val_loss: 30161.3379 - val_mean_absolute_error: 30161.3379\n",
            "Epoch 147/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 17659.8691 - mean_absolute_error: 17659.8691 - val_loss: 30711.6719 - val_mean_absolute_error: 30711.6719\n",
            "Epoch 148/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17676.7246 - mean_absolute_error: 17676.7246 - val_loss: 29667.9414 - val_mean_absolute_error: 29667.9414\n",
            "Epoch 149/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17916.0059 - mean_absolute_error: 17916.0059 - val_loss: 30695.6855 - val_mean_absolute_error: 30695.6855\n",
            "Epoch 150/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 17804.2559 - mean_absolute_error: 17804.2559 - val_loss: 31471.0059 - val_mean_absolute_error: 31471.0059\n",
            "Epoch 151/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18184.3750 - mean_absolute_error: 18184.3750 - val_loss: 30349.6973 - val_mean_absolute_error: 30349.6973\n",
            "Epoch 152/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 17635.4824 - mean_absolute_error: 17635.4824 - val_loss: 30116.3477 - val_mean_absolute_error: 30116.3477\n",
            "Epoch 153/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17874.1426 - mean_absolute_error: 17874.1426 - val_loss: 30537.2598 - val_mean_absolute_error: 30537.2598\n",
            "Epoch 154/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 18244.9629 - mean_absolute_error: 18244.9629 - val_loss: 30374.6621 - val_mean_absolute_error: 30374.6621\n",
            "Epoch 155/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 17886.4570 - mean_absolute_error: 17886.4570 - val_loss: 29807.7246 - val_mean_absolute_error: 29807.7246\n",
            "Epoch 156/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17748.2383 - mean_absolute_error: 17748.2383 - val_loss: 29671.4414 - val_mean_absolute_error: 29671.4414\n",
            "Epoch 157/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17460.2305 - mean_absolute_error: 17460.2305 - val_loss: 30704.6523 - val_mean_absolute_error: 30704.6523\n",
            "Epoch 158/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18340.5371 - mean_absolute_error: 18340.5371 - val_loss: 29859.9492 - val_mean_absolute_error: 29859.9492\n",
            "Epoch 159/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17662.4824 - mean_absolute_error: 17662.4824 - val_loss: 29430.5176 - val_mean_absolute_error: 29430.5176\n",
            "Epoch 160/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17054.2910 - mean_absolute_error: 17054.2910 - val_loss: 30271.9551 - val_mean_absolute_error: 30271.9551\n",
            "Epoch 161/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17372.8340 - mean_absolute_error: 17372.8340 - val_loss: 29636.3398 - val_mean_absolute_error: 29636.3398\n",
            "Epoch 162/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 17441.0156 - mean_absolute_error: 17441.0156 - val_loss: 30338.6426 - val_mean_absolute_error: 30338.6426\n",
            "Epoch 163/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17850.9629 - mean_absolute_error: 17850.9629 - val_loss: 29332.3027 - val_mean_absolute_error: 29332.3027\n",
            "Epoch 164/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17847.1973 - mean_absolute_error: 17847.1973 - val_loss: 30250.2031 - val_mean_absolute_error: 30250.2031\n",
            "Epoch 165/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17219.1504 - mean_absolute_error: 17219.1504 - val_loss: 29074.0293 - val_mean_absolute_error: 29074.0293\n",
            "Epoch 166/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17783.4258 - mean_absolute_error: 17783.4258 - val_loss: 29449.9707 - val_mean_absolute_error: 29449.9707\n",
            "Epoch 167/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17494.4238 - mean_absolute_error: 17494.4238 - val_loss: 30771.2246 - val_mean_absolute_error: 30771.2246\n",
            "Epoch 168/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17608.5078 - mean_absolute_error: 17608.5078 - val_loss: 29456.2578 - val_mean_absolute_error: 29456.2578\n",
            "Epoch 169/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17273.3574 - mean_absolute_error: 17273.3574 - val_loss: 30232.5742 - val_mean_absolute_error: 30232.5742\n",
            "Epoch 170/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 17109.4590 - mean_absolute_error: 17109.4590 - val_loss: 29314.8379 - val_mean_absolute_error: 29314.8379\n",
            "Epoch 171/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 17130.7422 - mean_absolute_error: 17130.7422 - val_loss: 30032.9609 - val_mean_absolute_error: 30032.9609\n",
            "Epoch 172/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17626.6855 - mean_absolute_error: 17626.6855 - val_loss: 29469.2852 - val_mean_absolute_error: 29469.2852\n",
            "Epoch 173/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17560.3066 - mean_absolute_error: 17560.3066 - val_loss: 30200.0996 - val_mean_absolute_error: 30200.0996\n",
            "Epoch 174/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17899.7227 - mean_absolute_error: 17899.7227 - val_loss: 30163.5781 - val_mean_absolute_error: 30163.5781\n",
            "Epoch 175/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17642.5430 - mean_absolute_error: 17642.5430 - val_loss: 29507.5957 - val_mean_absolute_error: 29507.5957\n",
            "Epoch 176/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16941.8750 - mean_absolute_error: 16941.8750 - val_loss: 29798.6035 - val_mean_absolute_error: 29798.6035\n",
            "Epoch 177/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16972.1172 - mean_absolute_error: 16972.1172 - val_loss: 30011.9141 - val_mean_absolute_error: 30011.9141\n",
            "Epoch 178/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17374.8301 - mean_absolute_error: 17374.8301 - val_loss: 30435.7188 - val_mean_absolute_error: 30435.7188\n",
            "Epoch 179/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16855.3145 - mean_absolute_error: 16855.3145 - val_loss: 31332.0547 - val_mean_absolute_error: 31332.0547\n",
            "Epoch 180/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16940.7773 - mean_absolute_error: 16940.7773 - val_loss: 29660.1562 - val_mean_absolute_error: 29660.1562\n",
            "Epoch 181/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17232.2559 - mean_absolute_error: 17232.2559 - val_loss: 29490.3477 - val_mean_absolute_error: 29490.3477\n",
            "Epoch 182/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17111.8047 - mean_absolute_error: 17111.8047 - val_loss: 30225.4863 - val_mean_absolute_error: 30225.4863\n",
            "Epoch 183/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17238.8359 - mean_absolute_error: 17238.8359 - val_loss: 30217.4648 - val_mean_absolute_error: 30217.4648\n",
            "Epoch 184/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16754.7891 - mean_absolute_error: 16754.7891 - val_loss: 29716.2715 - val_mean_absolute_error: 29716.2715\n",
            "Epoch 185/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17171.1660 - mean_absolute_error: 17171.1660 - val_loss: 29853.1465 - val_mean_absolute_error: 29853.1465\n",
            "Epoch 186/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17204.9277 - mean_absolute_error: 17204.9277 - val_loss: 29986.6621 - val_mean_absolute_error: 29986.6621\n",
            "Epoch 187/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16976.5410 - mean_absolute_error: 16976.5410 - val_loss: 29449.8184 - val_mean_absolute_error: 29449.8184\n",
            "Epoch 188/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16896.2148 - mean_absolute_error: 16896.2148 - val_loss: 29312.7930 - val_mean_absolute_error: 29312.7930\n",
            "Epoch 189/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16473.3184 - mean_absolute_error: 16473.3184 - val_loss: 30385.8945 - val_mean_absolute_error: 30385.8945\n",
            "Epoch 190/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16821.6074 - mean_absolute_error: 16821.6074 - val_loss: 29855.5801 - val_mean_absolute_error: 29855.5801\n",
            "Epoch 191/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16898.9785 - mean_absolute_error: 16898.9785 - val_loss: 29667.6367 - val_mean_absolute_error: 29667.6367\n",
            "Epoch 192/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16947.1172 - mean_absolute_error: 16947.1172 - val_loss: 30130.8809 - val_mean_absolute_error: 30130.8809\n",
            "Epoch 193/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17066.3594 - mean_absolute_error: 17066.3594 - val_loss: 29751.0957 - val_mean_absolute_error: 29751.0957\n",
            "Epoch 194/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16612.5605 - mean_absolute_error: 16612.5605 - val_loss: 30245.2715 - val_mean_absolute_error: 30245.2715\n",
            "Epoch 195/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16674.7441 - mean_absolute_error: 16674.7441 - val_loss: 29274.3379 - val_mean_absolute_error: 29274.3379\n",
            "Epoch 196/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16699.9141 - mean_absolute_error: 16699.9141 - val_loss: 29432.8184 - val_mean_absolute_error: 29432.8184\n",
            "Epoch 197/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16945.4180 - mean_absolute_error: 16945.4180 - val_loss: 30142.1309 - val_mean_absolute_error: 30142.1309\n",
            "Epoch 198/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16874.4434 - mean_absolute_error: 16874.4434 - val_loss: 29577.9668 - val_mean_absolute_error: 29577.9668\n",
            "Epoch 199/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16878.7109 - mean_absolute_error: 16878.7109 - val_loss: 30107.4297 - val_mean_absolute_error: 30107.4297\n",
            "Epoch 200/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16689.9199 - mean_absolute_error: 16689.9199 - val_loss: 29401.3164 - val_mean_absolute_error: 29401.3164\n",
            "Epoch 201/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16588.2148 - mean_absolute_error: 16588.2148 - val_loss: 29618.6855 - val_mean_absolute_error: 29618.6855\n",
            "Epoch 202/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16507.8242 - mean_absolute_error: 16507.8242 - val_loss: 29230.6934 - val_mean_absolute_error: 29230.6934\n",
            "Epoch 203/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16716.3457 - mean_absolute_error: 16716.3457 - val_loss: 30327.9707 - val_mean_absolute_error: 30327.9707\n",
            "Epoch 204/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 16328.6572 - mean_absolute_error: 16328.6572 - val_loss: 30518.5996 - val_mean_absolute_error: 30518.5996\n",
            "Epoch 205/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17051.1328 - mean_absolute_error: 17051.1328 - val_loss: 29520.2969 - val_mean_absolute_error: 29520.2969\n",
            "Epoch 206/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 16435.1758 - mean_absolute_error: 16435.1758 - val_loss: 29735.2148 - val_mean_absolute_error: 29735.2148\n",
            "Epoch 207/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16848.0820 - mean_absolute_error: 16848.0820 - val_loss: 29595.5781 - val_mean_absolute_error: 29595.5781\n",
            "Epoch 208/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16978.3516 - mean_absolute_error: 16978.3516 - val_loss: 29721.2578 - val_mean_absolute_error: 29721.2578\n",
            "Epoch 209/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16501.2754 - mean_absolute_error: 16501.2754 - val_loss: 29442.4824 - val_mean_absolute_error: 29442.4824\n",
            "Epoch 210/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16549.8574 - mean_absolute_error: 16549.8574 - val_loss: 29382.8379 - val_mean_absolute_error: 29382.8379\n",
            "Epoch 211/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16861.3984 - mean_absolute_error: 16861.3984 - val_loss: 29828.2598 - val_mean_absolute_error: 29828.2598\n",
            "Epoch 212/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16947.6777 - mean_absolute_error: 16947.6777 - val_loss: 29783.5410 - val_mean_absolute_error: 29783.5410\n",
            "Epoch 213/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16734.5234 - mean_absolute_error: 16734.5234 - val_loss: 30012.1582 - val_mean_absolute_error: 30012.1582\n",
            "Epoch 214/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16380.5186 - mean_absolute_error: 16380.5186 - val_loss: 29362.0566 - val_mean_absolute_error: 29362.0566\n",
            "Epoch 215/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16480.5820 - mean_absolute_error: 16480.5820 - val_loss: 30007.3145 - val_mean_absolute_error: 30007.3145\n",
            "Epoch 216/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16718.4023 - mean_absolute_error: 16718.4023 - val_loss: 29484.5234 - val_mean_absolute_error: 29484.5234\n",
            "Epoch 217/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16688.6348 - mean_absolute_error: 16688.6348 - val_loss: 29613.4023 - val_mean_absolute_error: 29613.4023\n",
            "Epoch 218/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17453.2773 - mean_absolute_error: 17453.2773 - val_loss: 29802.1992 - val_mean_absolute_error: 29802.1992\n",
            "Epoch 219/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16622.1074 - mean_absolute_error: 16622.1074 - val_loss: 30533.6973 - val_mean_absolute_error: 30533.6973\n",
            "Epoch 220/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16585.6973 - mean_absolute_error: 16585.6973 - val_loss: 30120.6543 - val_mean_absolute_error: 30120.6543\n",
            "Epoch 221/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16636.2773 - mean_absolute_error: 16636.2773 - val_loss: 29632.6660 - val_mean_absolute_error: 29632.6660\n",
            "Epoch 222/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16243.9619 - mean_absolute_error: 16243.9619 - val_loss: 30335.3535 - val_mean_absolute_error: 30335.3535\n",
            "Epoch 223/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16483.0371 - mean_absolute_error: 16483.0371 - val_loss: 29712.9180 - val_mean_absolute_error: 29712.9180\n",
            "Epoch 224/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16555.1992 - mean_absolute_error: 16555.1992 - val_loss: 30164.7402 - val_mean_absolute_error: 30164.7402\n",
            "Epoch 225/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16326.3008 - mean_absolute_error: 16326.3008 - val_loss: 29532.6738 - val_mean_absolute_error: 29532.6738\n",
            "Epoch 226/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16156.2852 - mean_absolute_error: 16156.2852 - val_loss: 33055.2070 - val_mean_absolute_error: 33055.2070\n",
            "Epoch 227/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17142.2832 - mean_absolute_error: 17142.2832 - val_loss: 29917.4414 - val_mean_absolute_error: 29917.4414\n",
            "Epoch 228/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16182.6631 - mean_absolute_error: 16182.6631 - val_loss: 29930.6836 - val_mean_absolute_error: 29930.6836\n",
            "Epoch 229/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16265.5098 - mean_absolute_error: 16265.5098 - val_loss: 30182.0391 - val_mean_absolute_error: 30182.0391\n",
            "Epoch 230/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16416.7949 - mean_absolute_error: 16416.7949 - val_loss: 29795.8672 - val_mean_absolute_error: 29795.8672\n",
            "Epoch 231/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16205.5078 - mean_absolute_error: 16205.5078 - val_loss: 29148.3340 - val_mean_absolute_error: 29148.3340\n",
            "Epoch 232/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16772.6016 - mean_absolute_error: 16772.6016 - val_loss: 30525.2695 - val_mean_absolute_error: 30525.2695\n",
            "Epoch 233/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16884.5820 - mean_absolute_error: 16884.5820 - val_loss: 29636.4668 - val_mean_absolute_error: 29636.4668\n",
            "Epoch 234/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16398.6953 - mean_absolute_error: 16398.6953 - val_loss: 30184.9453 - val_mean_absolute_error: 30184.9453\n",
            "Epoch 235/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16338.8887 - mean_absolute_error: 16338.8887 - val_loss: 29699.1074 - val_mean_absolute_error: 29699.1074\n",
            "Epoch 236/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16281.0303 - mean_absolute_error: 16281.0303 - val_loss: 29422.1504 - val_mean_absolute_error: 29422.1504\n",
            "Epoch 237/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16453.6309 - mean_absolute_error: 16453.6309 - val_loss: 31100.8887 - val_mean_absolute_error: 31100.8887\n",
            "Epoch 238/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16102.2930 - mean_absolute_error: 16102.2930 - val_loss: 29927.0488 - val_mean_absolute_error: 29927.0488\n",
            "Epoch 239/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16174.2100 - mean_absolute_error: 16174.2100 - val_loss: 31178.5586 - val_mean_absolute_error: 31178.5586\n",
            "Epoch 240/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16258.7031 - mean_absolute_error: 16258.7031 - val_loss: 30372.7109 - val_mean_absolute_error: 30372.7109\n",
            "Epoch 241/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16370.0283 - mean_absolute_error: 16370.0283 - val_loss: 29663.5859 - val_mean_absolute_error: 29663.5859\n",
            "Epoch 242/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16483.1836 - mean_absolute_error: 16483.1836 - val_loss: 29297.9102 - val_mean_absolute_error: 29297.9102\n",
            "Epoch 243/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16195.3350 - mean_absolute_error: 16195.3350 - val_loss: 30534.5176 - val_mean_absolute_error: 30534.5176\n",
            "Epoch 244/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16045.8496 - mean_absolute_error: 16045.8496 - val_loss: 29371.7871 - val_mean_absolute_error: 29371.7871\n",
            "Epoch 245/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16358.2510 - mean_absolute_error: 16358.2510 - val_loss: 29761.8242 - val_mean_absolute_error: 29761.8242\n",
            "Epoch 246/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16391.1699 - mean_absolute_error: 16391.1699 - val_loss: 29263.2266 - val_mean_absolute_error: 29263.2266\n",
            "Epoch 247/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16058.9346 - mean_absolute_error: 16058.9346 - val_loss: 29765.5645 - val_mean_absolute_error: 29765.5645\n",
            "Epoch 248/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16182.5273 - mean_absolute_error: 16182.5273 - val_loss: 29595.8301 - val_mean_absolute_error: 29595.8301\n",
            "Epoch 249/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16144.7188 - mean_absolute_error: 16144.7188 - val_loss: 30751.1875 - val_mean_absolute_error: 30751.1875\n",
            "Epoch 250/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15962.9189 - mean_absolute_error: 15962.9189 - val_loss: 29702.2285 - val_mean_absolute_error: 29702.2285\n",
            "Epoch 251/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16283.7744 - mean_absolute_error: 16283.7744 - val_loss: 30944.6914 - val_mean_absolute_error: 30944.6914\n",
            "Epoch 252/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16265.8965 - mean_absolute_error: 16265.8965 - val_loss: 30037.5742 - val_mean_absolute_error: 30037.5742\n",
            "Epoch 253/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16109.2617 - mean_absolute_error: 16109.2617 - val_loss: 29529.1016 - val_mean_absolute_error: 29529.1016\n",
            "Epoch 254/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16133.7588 - mean_absolute_error: 16133.7588 - val_loss: 30022.2598 - val_mean_absolute_error: 30022.2598\n",
            "Epoch 255/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16326.2383 - mean_absolute_error: 16326.2383 - val_loss: 30461.4160 - val_mean_absolute_error: 30461.4160\n",
            "Epoch 256/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15881.5547 - mean_absolute_error: 15881.5547 - val_loss: 29113.5957 - val_mean_absolute_error: 29113.5957\n",
            "Epoch 257/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15973.1514 - mean_absolute_error: 15973.1514 - val_loss: 30204.7168 - val_mean_absolute_error: 30204.7168\n",
            "Epoch 258/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15917.0420 - mean_absolute_error: 15917.0420 - val_loss: 29724.9258 - val_mean_absolute_error: 29724.9258\n",
            "Epoch 259/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16019.1992 - mean_absolute_error: 16019.1992 - val_loss: 29788.7051 - val_mean_absolute_error: 29788.7051\n",
            "Epoch 260/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15929.5889 - mean_absolute_error: 15929.5889 - val_loss: 29522.4785 - val_mean_absolute_error: 29522.4785\n",
            "Epoch 261/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15870.3770 - mean_absolute_error: 15870.3770 - val_loss: 29197.6270 - val_mean_absolute_error: 29197.6270\n",
            "Epoch 262/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15816.7158 - mean_absolute_error: 15816.7158 - val_loss: 29705.7715 - val_mean_absolute_error: 29705.7715\n",
            "Epoch 263/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16279.8877 - mean_absolute_error: 16279.8877 - val_loss: 29706.2949 - val_mean_absolute_error: 29706.2949\n",
            "Epoch 264/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15786.1064 - mean_absolute_error: 15786.1064 - val_loss: 29958.8887 - val_mean_absolute_error: 29958.8887\n",
            "Epoch 265/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15926.9355 - mean_absolute_error: 15926.9355 - val_loss: 29543.7246 - val_mean_absolute_error: 29543.7246\n",
            "Epoch 266/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16163.0508 - mean_absolute_error: 16163.0508 - val_loss: 29614.8008 - val_mean_absolute_error: 29614.8008\n",
            "Epoch 267/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15868.1660 - mean_absolute_error: 15868.1660 - val_loss: 30090.4043 - val_mean_absolute_error: 30090.4043\n",
            "Epoch 268/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16230.8721 - mean_absolute_error: 16230.8721 - val_loss: 30118.9805 - val_mean_absolute_error: 30118.9805\n",
            "Epoch 269/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16562.1895 - mean_absolute_error: 16562.1895 - val_loss: 29575.4258 - val_mean_absolute_error: 29575.4258\n",
            "Epoch 270/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15601.7012 - mean_absolute_error: 15601.7012 - val_loss: 29127.5469 - val_mean_absolute_error: 29127.5469\n",
            "Epoch 271/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15801.4072 - mean_absolute_error: 15801.4072 - val_loss: 29934.5176 - val_mean_absolute_error: 29934.5176\n",
            "Epoch 272/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15704.6211 - mean_absolute_error: 15704.6211 - val_loss: 30568.2188 - val_mean_absolute_error: 30568.2188\n",
            "Epoch 273/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16919.2988 - mean_absolute_error: 16919.2988 - val_loss: 30606.7246 - val_mean_absolute_error: 30606.7246\n",
            "Epoch 274/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16010.9980 - mean_absolute_error: 16010.9980 - val_loss: 30480.1211 - val_mean_absolute_error: 30480.1211\n",
            "Epoch 275/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 16565.3418 - mean_absolute_error: 16565.3418 - val_loss: 29738.7090 - val_mean_absolute_error: 29738.7090\n",
            "Epoch 276/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15815.4561 - mean_absolute_error: 15815.4561 - val_loss: 29577.4824 - val_mean_absolute_error: 29577.4824\n",
            "Epoch 277/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15664.5176 - mean_absolute_error: 15664.5176 - val_loss: 29641.9629 - val_mean_absolute_error: 29641.9629\n",
            "Epoch 278/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15800.9453 - mean_absolute_error: 15800.9453 - val_loss: 30709.9922 - val_mean_absolute_error: 30709.9922\n",
            "Epoch 279/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16065.2490 - mean_absolute_error: 16065.2490 - val_loss: 29130.3848 - val_mean_absolute_error: 29130.3848\n",
            "Epoch 280/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 15683.0088 - mean_absolute_error: 15683.0088 - val_loss: 29061.7227 - val_mean_absolute_error: 29061.7227\n",
            "Epoch 281/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15643.8184 - mean_absolute_error: 15643.8184 - val_loss: 29755.3379 - val_mean_absolute_error: 29755.3379\n",
            "Epoch 282/500\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16006.3545 - mean_absolute_error: 16006.3545 - val_loss: 29665.9121 - val_mean_absolute_error: 29665.9121\n",
            "Epoch 283/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16503.9590 - mean_absolute_error: 16503.9590 - val_loss: 31266.0645 - val_mean_absolute_error: 31266.0645\n",
            "Epoch 284/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15743.4561 - mean_absolute_error: 15743.4561 - val_loss: 29775.5215 - val_mean_absolute_error: 29775.5215\n",
            "Epoch 285/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15579.6729 - mean_absolute_error: 15579.6729 - val_loss: 29110.8223 - val_mean_absolute_error: 29110.8223\n",
            "Epoch 286/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15534.8662 - mean_absolute_error: 15534.8662 - val_loss: 30183.3730 - val_mean_absolute_error: 30183.3730\n",
            "Epoch 287/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15945.6172 - mean_absolute_error: 15945.6172 - val_loss: 29730.9570 - val_mean_absolute_error: 29730.9570\n",
            "Epoch 288/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15856.5107 - mean_absolute_error: 15856.5107 - val_loss: 29768.6289 - val_mean_absolute_error: 29768.6289\n",
            "Epoch 289/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15519.5674 - mean_absolute_error: 15519.5674 - val_loss: 30023.9043 - val_mean_absolute_error: 30023.9043\n",
            "Epoch 290/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16114.4336 - mean_absolute_error: 16114.4336 - val_loss: 29744.5430 - val_mean_absolute_error: 29744.5430\n",
            "Epoch 291/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15722.1045 - mean_absolute_error: 15722.1045 - val_loss: 29610.1113 - val_mean_absolute_error: 29610.1113\n",
            "Epoch 292/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 15588.3330 - mean_absolute_error: 15588.3330 - val_loss: 29259.1191 - val_mean_absolute_error: 29259.1191\n",
            "Epoch 293/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15628.4492 - mean_absolute_error: 15628.4492 - val_loss: 29291.6426 - val_mean_absolute_error: 29291.6426\n",
            "Epoch 294/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15703.8770 - mean_absolute_error: 15703.8770 - val_loss: 29667.1953 - val_mean_absolute_error: 29667.1953\n",
            "Epoch 295/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15651.8301 - mean_absolute_error: 15651.8301 - val_loss: 30202.7734 - val_mean_absolute_error: 30202.7734\n",
            "Epoch 296/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15736.0820 - mean_absolute_error: 15736.0820 - val_loss: 30242.6738 - val_mean_absolute_error: 30242.6738\n",
            "Epoch 297/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15563.4688 - mean_absolute_error: 15563.4688 - val_loss: 30565.1621 - val_mean_absolute_error: 30565.1621\n",
            "Epoch 298/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15566.6182 - mean_absolute_error: 15566.6182 - val_loss: 29718.7715 - val_mean_absolute_error: 29718.7715\n",
            "Epoch 299/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15141.9805 - mean_absolute_error: 15141.9805 - val_loss: 29457.2188 - val_mean_absolute_error: 29457.2188\n",
            "Epoch 300/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15163.6992 - mean_absolute_error: 15163.6992 - val_loss: 29493.2949 - val_mean_absolute_error: 29493.2949\n",
            "Epoch 301/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15606.5771 - mean_absolute_error: 15606.5771 - val_loss: 30224.4727 - val_mean_absolute_error: 30224.4727\n",
            "Epoch 302/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15439.2158 - mean_absolute_error: 15439.2158 - val_loss: 29417.0957 - val_mean_absolute_error: 29417.0957\n",
            "Epoch 303/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 16922.5312 - mean_absolute_error: 16922.5312 - val_loss: 30851.5469 - val_mean_absolute_error: 30851.5469\n",
            "Epoch 304/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15975.4121 - mean_absolute_error: 15975.4121 - val_loss: 29720.1250 - val_mean_absolute_error: 29720.1250\n",
            "Epoch 305/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15311.6006 - mean_absolute_error: 15311.6006 - val_loss: 31617.1191 - val_mean_absolute_error: 31617.1191\n",
            "Epoch 306/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15333.6611 - mean_absolute_error: 15333.6611 - val_loss: 29269.6172 - val_mean_absolute_error: 29269.6172\n",
            "Epoch 307/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15672.2363 - mean_absolute_error: 15672.2363 - val_loss: 29915.1895 - val_mean_absolute_error: 29915.1895\n",
            "Epoch 308/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15340.5127 - mean_absolute_error: 15340.5127 - val_loss: 29912.9629 - val_mean_absolute_error: 29912.9629\n",
            "Epoch 309/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15706.0977 - mean_absolute_error: 15706.0977 - val_loss: 29292.3398 - val_mean_absolute_error: 29292.3398\n",
            "Epoch 310/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15376.2764 - mean_absolute_error: 15376.2764 - val_loss: 29478.0547 - val_mean_absolute_error: 29478.0547\n",
            "Epoch 311/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15574.3105 - mean_absolute_error: 15574.3105 - val_loss: 29466.0898 - val_mean_absolute_error: 29466.0898\n",
            "Epoch 312/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15693.1211 - mean_absolute_error: 15693.1211 - val_loss: 29687.9141 - val_mean_absolute_error: 29687.9141\n",
            "Epoch 313/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15573.1436 - mean_absolute_error: 15573.1436 - val_loss: 31380.9688 - val_mean_absolute_error: 31380.9688\n",
            "Epoch 314/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15554.3740 - mean_absolute_error: 15554.3740 - val_loss: 29474.9746 - val_mean_absolute_error: 29474.9746\n",
            "Epoch 315/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15287.9004 - mean_absolute_error: 15287.9004 - val_loss: 29922.1152 - val_mean_absolute_error: 29922.1152\n",
            "Epoch 316/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15067.9395 - mean_absolute_error: 15067.9395 - val_loss: 30480.3223 - val_mean_absolute_error: 30480.3223\n",
            "Epoch 317/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15613.2617 - mean_absolute_error: 15613.2617 - val_loss: 29580.2852 - val_mean_absolute_error: 29580.2852\n",
            "Epoch 318/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15583.8828 - mean_absolute_error: 15583.8828 - val_loss: 29426.6094 - val_mean_absolute_error: 29426.6094\n",
            "Epoch 319/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15388.0752 - mean_absolute_error: 15388.0752 - val_loss: 29810.3457 - val_mean_absolute_error: 29810.3457\n",
            "Epoch 320/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15464.0127 - mean_absolute_error: 15464.0127 - val_loss: 30931.4707 - val_mean_absolute_error: 30931.4707\n",
            "Epoch 321/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15454.2764 - mean_absolute_error: 15454.2764 - val_loss: 29400.8633 - val_mean_absolute_error: 29400.8633\n",
            "Epoch 322/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15088.6240 - mean_absolute_error: 15088.6240 - val_loss: 30201.3281 - val_mean_absolute_error: 30201.3281\n",
            "Epoch 323/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15208.1514 - mean_absolute_error: 15208.1514 - val_loss: 29939.6230 - val_mean_absolute_error: 29939.6230\n",
            "Epoch 324/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15008.3457 - mean_absolute_error: 15008.3457 - val_loss: 29723.8438 - val_mean_absolute_error: 29723.8438\n",
            "Epoch 325/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15209.8779 - mean_absolute_error: 15209.8779 - val_loss: 29691.7656 - val_mean_absolute_error: 29691.7656\n",
            "Epoch 326/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15428.6914 - mean_absolute_error: 15428.6914 - val_loss: 29413.2578 - val_mean_absolute_error: 29413.2578\n",
            "Epoch 327/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15418.3809 - mean_absolute_error: 15418.3809 - val_loss: 30540.8477 - val_mean_absolute_error: 30540.8477\n",
            "Epoch 328/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15100.4170 - mean_absolute_error: 15100.4170 - val_loss: 29745.4863 - val_mean_absolute_error: 29745.4863\n",
            "Epoch 329/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15322.8535 - mean_absolute_error: 15322.8535 - val_loss: 29839.3438 - val_mean_absolute_error: 29839.3438\n",
            "Epoch 330/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15741.7334 - mean_absolute_error: 15741.7334 - val_loss: 30149.3340 - val_mean_absolute_error: 30149.3340\n",
            "Epoch 331/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15100.9863 - mean_absolute_error: 15100.9863 - val_loss: 29397.9922 - val_mean_absolute_error: 29397.9922\n",
            "Epoch 332/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15285.9775 - mean_absolute_error: 15285.9775 - val_loss: 29758.1328 - val_mean_absolute_error: 29758.1328\n",
            "Epoch 333/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15582.2676 - mean_absolute_error: 15582.2676 - val_loss: 29439.0059 - val_mean_absolute_error: 29439.0059\n",
            "Epoch 334/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15282.4238 - mean_absolute_error: 15282.4238 - val_loss: 29221.7422 - val_mean_absolute_error: 29221.7422\n",
            "Epoch 335/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15364.6133 - mean_absolute_error: 15364.6133 - val_loss: 29918.2090 - val_mean_absolute_error: 29918.2090\n",
            "Epoch 336/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15253.6836 - mean_absolute_error: 15253.6836 - val_loss: 30051.6934 - val_mean_absolute_error: 30051.6934\n",
            "Epoch 337/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15254.8799 - mean_absolute_error: 15254.8799 - val_loss: 30424.6055 - val_mean_absolute_error: 30424.6055\n",
            "Epoch 338/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15011.0518 - mean_absolute_error: 15011.0518 - val_loss: 29974.0859 - val_mean_absolute_error: 29974.0859\n",
            "Epoch 339/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15153.6865 - mean_absolute_error: 15153.6865 - val_loss: 29724.9102 - val_mean_absolute_error: 29724.9102\n",
            "Epoch 340/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15362.4248 - mean_absolute_error: 15362.4248 - val_loss: 31911.4219 - val_mean_absolute_error: 31911.4219\n",
            "Epoch 341/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15589.8467 - mean_absolute_error: 15589.8467 - val_loss: 29469.9414 - val_mean_absolute_error: 29469.9414\n",
            "Epoch 342/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15078.3105 - mean_absolute_error: 15078.3105 - val_loss: 30317.6172 - val_mean_absolute_error: 30317.6172\n",
            "Epoch 343/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15151.7227 - mean_absolute_error: 15151.7227 - val_loss: 29401.1328 - val_mean_absolute_error: 29401.1328\n",
            "Epoch 344/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14884.2773 - mean_absolute_error: 14884.2773 - val_loss: 29473.5801 - val_mean_absolute_error: 29473.5801\n",
            "Epoch 345/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15699.3320 - mean_absolute_error: 15699.3320 - val_loss: 30005.9121 - val_mean_absolute_error: 30005.9121\n",
            "Epoch 346/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14890.9668 - mean_absolute_error: 14890.9668 - val_loss: 29653.0176 - val_mean_absolute_error: 29653.0176\n",
            "Epoch 347/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15225.8428 - mean_absolute_error: 15225.8428 - val_loss: 29652.7793 - val_mean_absolute_error: 29652.7793\n",
            "Epoch 348/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15261.9160 - mean_absolute_error: 15261.9160 - val_loss: 29764.9238 - val_mean_absolute_error: 29764.9238\n",
            "Epoch 349/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14919.1582 - mean_absolute_error: 14919.1582 - val_loss: 29485.6875 - val_mean_absolute_error: 29485.6875\n",
            "Epoch 350/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15025.2178 - mean_absolute_error: 15025.2178 - val_loss: 29780.9863 - val_mean_absolute_error: 29780.9863\n",
            "Epoch 351/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14748.1797 - mean_absolute_error: 14748.1797 - val_loss: 30482.2461 - val_mean_absolute_error: 30482.2461\n",
            "Epoch 352/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14992.7812 - mean_absolute_error: 14992.7812 - val_loss: 30076.2090 - val_mean_absolute_error: 30076.2090\n",
            "Epoch 353/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15006.9639 - mean_absolute_error: 15006.9639 - val_loss: 31449.9180 - val_mean_absolute_error: 31449.9180\n",
            "Epoch 354/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14903.8174 - mean_absolute_error: 14903.8174 - val_loss: 29648.3066 - val_mean_absolute_error: 29648.3066\n",
            "Epoch 355/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15169.9570 - mean_absolute_error: 15169.9570 - val_loss: 29834.1504 - val_mean_absolute_error: 29834.1504\n",
            "Epoch 356/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14618.5654 - mean_absolute_error: 14618.5654 - val_loss: 29238.5098 - val_mean_absolute_error: 29238.5098\n",
            "Epoch 357/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14887.2969 - mean_absolute_error: 14887.2969 - val_loss: 30556.3711 - val_mean_absolute_error: 30556.3711\n",
            "Epoch 358/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14727.0742 - mean_absolute_error: 14727.0742 - val_loss: 29550.7031 - val_mean_absolute_error: 29550.7031\n",
            "Epoch 359/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14665.4746 - mean_absolute_error: 14665.4746 - val_loss: 29128.3691 - val_mean_absolute_error: 29128.3691\n",
            "Epoch 360/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15070.4453 - mean_absolute_error: 15070.4453 - val_loss: 29935.4531 - val_mean_absolute_error: 29935.4531\n",
            "Epoch 361/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15364.8447 - mean_absolute_error: 15364.8447 - val_loss: 29504.3633 - val_mean_absolute_error: 29504.3633\n",
            "Epoch 362/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15471.9521 - mean_absolute_error: 15471.9521 - val_loss: 29326.8184 - val_mean_absolute_error: 29326.8184\n",
            "Epoch 363/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14489.8086 - mean_absolute_error: 14489.8086 - val_loss: 29526.2461 - val_mean_absolute_error: 29526.2461\n",
            "Epoch 364/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14763.5469 - mean_absolute_error: 14763.5469 - val_loss: 29599.0801 - val_mean_absolute_error: 29599.0801\n",
            "Epoch 365/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15203.0635 - mean_absolute_error: 15203.0635 - val_loss: 30721.5781 - val_mean_absolute_error: 30721.5781\n",
            "Epoch 366/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15089.3447 - mean_absolute_error: 15089.3447 - val_loss: 29153.5352 - val_mean_absolute_error: 29153.5352\n",
            "Epoch 367/500\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 14880.5312 - mean_absolute_error: 14880.5312 - val_loss: 29815.9746 - val_mean_absolute_error: 29815.9746\n",
            "Epoch 368/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14757.9707 - mean_absolute_error: 14757.9707 - val_loss: 30333.7812 - val_mean_absolute_error: 30333.7812\n",
            "Epoch 369/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14361.6836 - mean_absolute_error: 14361.6836 - val_loss: 30710.8242 - val_mean_absolute_error: 30710.8242\n",
            "Epoch 370/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14536.8408 - mean_absolute_error: 14536.8408 - val_loss: 30372.9258 - val_mean_absolute_error: 30372.9258\n",
            "Epoch 371/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14853.6982 - mean_absolute_error: 14853.6982 - val_loss: 29905.8008 - val_mean_absolute_error: 29905.8008\n",
            "Epoch 372/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14558.6885 - mean_absolute_error: 14558.6885 - val_loss: 29744.3164 - val_mean_absolute_error: 29744.3164\n",
            "Epoch 373/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14399.0947 - mean_absolute_error: 14399.0947 - val_loss: 29792.8008 - val_mean_absolute_error: 29792.8008\n",
            "Epoch 374/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14775.7920 - mean_absolute_error: 14775.7920 - val_loss: 29210.4355 - val_mean_absolute_error: 29210.4355\n",
            "Epoch 375/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14944.2832 - mean_absolute_error: 14944.2832 - val_loss: 30262.0898 - val_mean_absolute_error: 30262.0898\n",
            "Epoch 376/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15106.5645 - mean_absolute_error: 15106.5645 - val_loss: 30115.6211 - val_mean_absolute_error: 30115.6211\n",
            "Epoch 377/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15572.6523 - mean_absolute_error: 15572.6523 - val_loss: 29331.9414 - val_mean_absolute_error: 29331.9414\n",
            "Epoch 378/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14857.8711 - mean_absolute_error: 14857.8711 - val_loss: 29545.2754 - val_mean_absolute_error: 29545.2754\n",
            "Epoch 379/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14617.2207 - mean_absolute_error: 14617.2207 - val_loss: 29809.0742 - val_mean_absolute_error: 29809.0742\n",
            "Epoch 380/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14610.3232 - mean_absolute_error: 14610.3232 - val_loss: 29760.7930 - val_mean_absolute_error: 29760.7930\n",
            "Epoch 381/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14520.1943 - mean_absolute_error: 14520.1943 - val_loss: 30229.7227 - val_mean_absolute_error: 30229.7227\n",
            "Epoch 382/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14567.5332 - mean_absolute_error: 14567.5332 - val_loss: 29553.6973 - val_mean_absolute_error: 29553.6973\n",
            "Epoch 383/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14352.1572 - mean_absolute_error: 14352.1572 - val_loss: 29670.4707 - val_mean_absolute_error: 29670.4707\n",
            "Epoch 384/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14530.4521 - mean_absolute_error: 14530.4521 - val_loss: 29302.7656 - val_mean_absolute_error: 29302.7656\n",
            "Epoch 385/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14275.7168 - mean_absolute_error: 14275.7168 - val_loss: 29514.5293 - val_mean_absolute_error: 29514.5293\n",
            "Epoch 386/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14596.9365 - mean_absolute_error: 14596.9365 - val_loss: 29980.4590 - val_mean_absolute_error: 29980.4590\n",
            "Epoch 387/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14488.2676 - mean_absolute_error: 14488.2676 - val_loss: 29125.5547 - val_mean_absolute_error: 29125.5547\n",
            "Epoch 388/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15081.6855 - mean_absolute_error: 15081.6855 - val_loss: 30016.8730 - val_mean_absolute_error: 30016.8730\n",
            "Epoch 389/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14429.5439 - mean_absolute_error: 14429.5439 - val_loss: 29873.8867 - val_mean_absolute_error: 29873.8867\n",
            "Epoch 390/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14713.1953 - mean_absolute_error: 14713.1953 - val_loss: 31375.0684 - val_mean_absolute_error: 31375.0684\n",
            "Epoch 391/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14887.6953 - mean_absolute_error: 14887.6953 - val_loss: 30529.1777 - val_mean_absolute_error: 30529.1777\n",
            "Epoch 392/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14805.3467 - mean_absolute_error: 14805.3467 - val_loss: 29681.5020 - val_mean_absolute_error: 29681.5020\n",
            "Epoch 393/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14455.2285 - mean_absolute_error: 14455.2285 - val_loss: 30790.9316 - val_mean_absolute_error: 30790.9316\n",
            "Epoch 394/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14650.5752 - mean_absolute_error: 14650.5752 - val_loss: 30680.1328 - val_mean_absolute_error: 30680.1328\n",
            "Epoch 395/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14263.1904 - mean_absolute_error: 14263.1904 - val_loss: 29294.3008 - val_mean_absolute_error: 29294.3008\n",
            "Epoch 396/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14411.3936 - mean_absolute_error: 14411.3936 - val_loss: 29749.7168 - val_mean_absolute_error: 29749.7168\n",
            "Epoch 397/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14373.3027 - mean_absolute_error: 14373.3027 - val_loss: 29337.9062 - val_mean_absolute_error: 29337.9062\n",
            "Epoch 398/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15599.2441 - mean_absolute_error: 15599.2441 - val_loss: 29905.1836 - val_mean_absolute_error: 29905.1836\n",
            "Epoch 399/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14282.5186 - mean_absolute_error: 14282.5186 - val_loss: 29780.4219 - val_mean_absolute_error: 29780.4219\n",
            "Epoch 400/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14267.0488 - mean_absolute_error: 14267.0488 - val_loss: 29729.8691 - val_mean_absolute_error: 29729.8691\n",
            "Epoch 401/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14512.4688 - mean_absolute_error: 14512.4688 - val_loss: 29747.4863 - val_mean_absolute_error: 29747.4863\n",
            "Epoch 402/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14295.1797 - mean_absolute_error: 14295.1797 - val_loss: 29516.1270 - val_mean_absolute_error: 29516.1270\n",
            "Epoch 403/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14225.8955 - mean_absolute_error: 14225.8955 - val_loss: 29209.2715 - val_mean_absolute_error: 29209.2715\n",
            "Epoch 404/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14247.7295 - mean_absolute_error: 14247.7295 - val_loss: 29331.1387 - val_mean_absolute_error: 29331.1387\n",
            "Epoch 405/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14621.8838 - mean_absolute_error: 14621.8838 - val_loss: 30335.9199 - val_mean_absolute_error: 30335.9199\n",
            "Epoch 406/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14172.1611 - mean_absolute_error: 14172.1611 - val_loss: 31205.9414 - val_mean_absolute_error: 31205.9414\n",
            "Epoch 407/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14723.1299 - mean_absolute_error: 14723.1299 - val_loss: 29726.3633 - val_mean_absolute_error: 29726.3633\n",
            "Epoch 408/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14509.9990 - mean_absolute_error: 14509.9990 - val_loss: 29669.6230 - val_mean_absolute_error: 29669.6230\n",
            "Epoch 409/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14551.7646 - mean_absolute_error: 14551.7646 - val_loss: 29606.3262 - val_mean_absolute_error: 29606.3262\n",
            "Epoch 410/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 14385.9971 - mean_absolute_error: 14385.9971 - val_loss: 30075.5352 - val_mean_absolute_error: 30075.5352\n",
            "Epoch 411/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14410.7451 - mean_absolute_error: 14410.7451 - val_loss: 29508.4355 - val_mean_absolute_error: 29508.4355\n",
            "Epoch 412/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14762.0684 - mean_absolute_error: 14762.0684 - val_loss: 30055.1055 - val_mean_absolute_error: 30055.1055\n",
            "Epoch 413/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14122.4688 - mean_absolute_error: 14122.4688 - val_loss: 29712.2949 - val_mean_absolute_error: 29712.2949\n",
            "Epoch 414/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14628.4814 - mean_absolute_error: 14628.4814 - val_loss: 29222.3730 - val_mean_absolute_error: 29222.3730\n",
            "Epoch 415/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14662.6133 - mean_absolute_error: 14662.6133 - val_loss: 30228.9199 - val_mean_absolute_error: 30228.9199\n",
            "Epoch 416/500\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13987.7559 - mean_absolute_error: 13987.7559 - val_loss: 29355.4043 - val_mean_absolute_error: 29355.4043\n",
            "Epoch 417/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14157.2812 - mean_absolute_error: 14157.2812 - val_loss: 29218.3281 - val_mean_absolute_error: 29218.3281\n",
            "Epoch 418/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14310.9268 - mean_absolute_error: 14310.9268 - val_loss: 29988.1367 - val_mean_absolute_error: 29988.1367\n",
            "Epoch 419/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14082.7656 - mean_absolute_error: 14082.7656 - val_loss: 30155.1270 - val_mean_absolute_error: 30155.1270\n",
            "Epoch 420/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14356.8389 - mean_absolute_error: 14356.8389 - val_loss: 30255.7988 - val_mean_absolute_error: 30255.7988\n",
            "Epoch 421/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14154.1279 - mean_absolute_error: 14154.1279 - val_loss: 30095.6152 - val_mean_absolute_error: 30095.6152\n",
            "Epoch 422/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 14195.4346 - mean_absolute_error: 14195.4346 - val_loss: 29946.6973 - val_mean_absolute_error: 29946.6973\n",
            "Epoch 423/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14520.4092 - mean_absolute_error: 14520.4092 - val_loss: 30627.7090 - val_mean_absolute_error: 30627.7090\n",
            "Epoch 424/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14101.9209 - mean_absolute_error: 14101.9209 - val_loss: 30110.2500 - val_mean_absolute_error: 30110.2500\n",
            "Epoch 425/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14379.5596 - mean_absolute_error: 14379.5596 - val_loss: 30237.2148 - val_mean_absolute_error: 30237.2148\n",
            "Epoch 426/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13978.6338 - mean_absolute_error: 13978.6338 - val_loss: 29595.2949 - val_mean_absolute_error: 29595.2949\n",
            "Epoch 427/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14159.5381 - mean_absolute_error: 14159.5381 - val_loss: 30313.1152 - val_mean_absolute_error: 30313.1152\n",
            "Epoch 428/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14641.8643 - mean_absolute_error: 14641.8643 - val_loss: 29920.5977 - val_mean_absolute_error: 29920.5977\n",
            "Epoch 429/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14156.1768 - mean_absolute_error: 14156.1768 - val_loss: 29198.5898 - val_mean_absolute_error: 29198.5898\n",
            "Epoch 430/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14281.6602 - mean_absolute_error: 14281.6602 - val_loss: 29277.6660 - val_mean_absolute_error: 29277.6660\n",
            "Epoch 431/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14464.4336 - mean_absolute_error: 14464.4336 - val_loss: 29681.1621 - val_mean_absolute_error: 29681.1621\n",
            "Epoch 432/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14206.4053 - mean_absolute_error: 14206.4053 - val_loss: 29766.4199 - val_mean_absolute_error: 29766.4199\n",
            "Epoch 433/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13967.6934 - mean_absolute_error: 13967.6934 - val_loss: 29805.5410 - val_mean_absolute_error: 29805.5410\n",
            "Epoch 434/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14227.7012 - mean_absolute_error: 14227.7012 - val_loss: 30528.2754 - val_mean_absolute_error: 30528.2754\n",
            "Epoch 435/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14274.7568 - mean_absolute_error: 14274.7568 - val_loss: 30821.5781 - val_mean_absolute_error: 30821.5781\n",
            "Epoch 436/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14140.4121 - mean_absolute_error: 14140.4121 - val_loss: 29913.3711 - val_mean_absolute_error: 29913.3711\n",
            "Epoch 437/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14455.5830 - mean_absolute_error: 14455.5830 - val_loss: 29792.0254 - val_mean_absolute_error: 29792.0254\n",
            "Epoch 438/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 15050.7432 - mean_absolute_error: 15050.7432 - val_loss: 30100.2852 - val_mean_absolute_error: 30100.2852\n",
            "Epoch 439/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14145.5049 - mean_absolute_error: 14145.5049 - val_loss: 29867.9668 - val_mean_absolute_error: 29867.9668\n",
            "Epoch 440/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14279.9092 - mean_absolute_error: 14279.9092 - val_loss: 30367.8750 - val_mean_absolute_error: 30367.8750\n",
            "Epoch 441/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14053.3779 - mean_absolute_error: 14053.3779 - val_loss: 30221.3086 - val_mean_absolute_error: 30221.3086\n",
            "Epoch 442/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13974.2598 - mean_absolute_error: 13974.2598 - val_loss: 29941.8359 - val_mean_absolute_error: 29941.8359\n",
            "Epoch 443/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14068.1172 - mean_absolute_error: 14068.1172 - val_loss: 30120.6992 - val_mean_absolute_error: 30120.6992\n",
            "Epoch 444/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14235.9463 - mean_absolute_error: 14235.9463 - val_loss: 29740.2910 - val_mean_absolute_error: 29740.2910\n",
            "Epoch 445/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14287.6357 - mean_absolute_error: 14287.6357 - val_loss: 32734.4453 - val_mean_absolute_error: 32734.4453\n",
            "Epoch 446/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14477.8145 - mean_absolute_error: 14477.8145 - val_loss: 29498.9629 - val_mean_absolute_error: 29498.9629\n",
            "Epoch 447/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13935.8428 - mean_absolute_error: 13935.8428 - val_loss: 29141.7344 - val_mean_absolute_error: 29141.7344\n",
            "Epoch 448/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13996.3232 - mean_absolute_error: 13996.3232 - val_loss: 29334.5977 - val_mean_absolute_error: 29334.5977\n",
            "Epoch 449/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14027.2432 - mean_absolute_error: 14027.2432 - val_loss: 29759.6914 - val_mean_absolute_error: 29759.6914\n",
            "Epoch 450/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14025.9746 - mean_absolute_error: 14025.9746 - val_loss: 29631.7617 - val_mean_absolute_error: 29631.7617\n",
            "Epoch 451/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13832.8350 - mean_absolute_error: 13832.8350 - val_loss: 29743.8691 - val_mean_absolute_error: 29743.8691\n",
            "Epoch 452/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14008.6445 - mean_absolute_error: 14008.6445 - val_loss: 29333.3965 - val_mean_absolute_error: 29333.3965\n",
            "Epoch 453/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 13864.2324 - mean_absolute_error: 13864.2324 - val_loss: 29569.1836 - val_mean_absolute_error: 29569.1836\n",
            "Epoch 454/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13674.2480 - mean_absolute_error: 13674.2480 - val_loss: 30095.2383 - val_mean_absolute_error: 30095.2383\n",
            "Epoch 455/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13745.4170 - mean_absolute_error: 13745.4170 - val_loss: 29643.7422 - val_mean_absolute_error: 29643.7422\n",
            "Epoch 456/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13861.8936 - mean_absolute_error: 13861.8936 - val_loss: 29920.9062 - val_mean_absolute_error: 29920.9062\n",
            "Epoch 457/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14078.8262 - mean_absolute_error: 14078.8262 - val_loss: 29738.5332 - val_mean_absolute_error: 29738.5332\n",
            "Epoch 458/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13650.3740 - mean_absolute_error: 13650.3740 - val_loss: 29207.4512 - val_mean_absolute_error: 29207.4512\n",
            "Epoch 459/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13717.9111 - mean_absolute_error: 13717.9111 - val_loss: 29962.7812 - val_mean_absolute_error: 29962.7812\n",
            "Epoch 460/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14061.9414 - mean_absolute_error: 14061.9414 - val_loss: 29741.0020 - val_mean_absolute_error: 29741.0020\n",
            "Epoch 461/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13676.3086 - mean_absolute_error: 13676.3086 - val_loss: 29876.4922 - val_mean_absolute_error: 29876.4922\n",
            "Epoch 462/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13929.9873 - mean_absolute_error: 13929.9873 - val_loss: 29451.4141 - val_mean_absolute_error: 29451.4141\n",
            "Epoch 463/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13755.4453 - mean_absolute_error: 13755.4453 - val_loss: 30025.0684 - val_mean_absolute_error: 30025.0684\n",
            "Epoch 464/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13733.4814 - mean_absolute_error: 13733.4814 - val_loss: 29905.5781 - val_mean_absolute_error: 29905.5781\n",
            "Epoch 465/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13492.8291 - mean_absolute_error: 13492.8291 - val_loss: 30581.5332 - val_mean_absolute_error: 30581.5332\n",
            "Epoch 466/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13824.4121 - mean_absolute_error: 13824.4121 - val_loss: 30257.1445 - val_mean_absolute_error: 30257.1445\n",
            "Epoch 467/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13962.5312 - mean_absolute_error: 13962.5312 - val_loss: 29905.5176 - val_mean_absolute_error: 29905.5176\n",
            "Epoch 468/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13695.4609 - mean_absolute_error: 13695.4609 - val_loss: 30272.3477 - val_mean_absolute_error: 30272.3477\n",
            "Epoch 469/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13978.7949 - mean_absolute_error: 13978.7949 - val_loss: 30167.4727 - val_mean_absolute_error: 30167.4727\n",
            "Epoch 470/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13516.0254 - mean_absolute_error: 13516.0254 - val_loss: 29680.3398 - val_mean_absolute_error: 29680.3398\n",
            "Epoch 471/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13503.4092 - mean_absolute_error: 13503.4092 - val_loss: 29647.0703 - val_mean_absolute_error: 29647.0703\n",
            "Epoch 472/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13710.1484 - mean_absolute_error: 13710.1484 - val_loss: 29592.9121 - val_mean_absolute_error: 29592.9121\n",
            "Epoch 473/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14031.2568 - mean_absolute_error: 14031.2568 - val_loss: 30544.3730 - val_mean_absolute_error: 30544.3730\n",
            "Epoch 474/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14290.5645 - mean_absolute_error: 14290.5645 - val_loss: 29872.5156 - val_mean_absolute_error: 29872.5156\n",
            "Epoch 475/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13834.7402 - mean_absolute_error: 13834.7402 - val_loss: 29536.1641 - val_mean_absolute_error: 29536.1641\n",
            "Epoch 476/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13430.7451 - mean_absolute_error: 13430.7451 - val_loss: 29935.2500 - val_mean_absolute_error: 29935.2500\n",
            "Epoch 477/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13582.2041 - mean_absolute_error: 13582.2041 - val_loss: 30209.9434 - val_mean_absolute_error: 30209.9434\n",
            "Epoch 478/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13747.6328 - mean_absolute_error: 13747.6328 - val_loss: 29526.4219 - val_mean_absolute_error: 29526.4219\n",
            "Epoch 479/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14174.8857 - mean_absolute_error: 14174.8857 - val_loss: 30025.4453 - val_mean_absolute_error: 30025.4453\n",
            "Epoch 480/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 13637.3184 - mean_absolute_error: 13637.3184 - val_loss: 30471.3027 - val_mean_absolute_error: 30471.3027\n",
            "Epoch 481/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13347.7646 - mean_absolute_error: 13347.7646 - val_loss: 30204.5352 - val_mean_absolute_error: 30204.5352\n",
            "Epoch 482/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13222.0557 - mean_absolute_error: 13222.0557 - val_loss: 30176.9746 - val_mean_absolute_error: 30176.9746\n",
            "Epoch 483/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13768.6787 - mean_absolute_error: 13768.6787 - val_loss: 30043.8691 - val_mean_absolute_error: 30043.8691\n",
            "Epoch 484/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14564.9043 - mean_absolute_error: 14564.9043 - val_loss: 30989.4766 - val_mean_absolute_error: 30989.4766\n",
            "Epoch 485/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13898.0215 - mean_absolute_error: 13898.0215 - val_loss: 30657.9824 - val_mean_absolute_error: 30657.9824\n",
            "Epoch 486/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13957.4307 - mean_absolute_error: 13957.4307 - val_loss: 30604.5664 - val_mean_absolute_error: 30604.5664\n",
            "Epoch 487/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13379.6758 - mean_absolute_error: 13379.6758 - val_loss: 30420.9980 - val_mean_absolute_error: 30420.9980\n",
            "Epoch 488/500\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14578.1377 - mean_absolute_error: 14578.1377 - val_loss: 30448.4277 - val_mean_absolute_error: 30448.4277\n",
            "Epoch 489/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13853.6631 - mean_absolute_error: 13853.6631 - val_loss: 31068.9434 - val_mean_absolute_error: 31068.9434\n",
            "Epoch 490/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 14054.1260 - mean_absolute_error: 14054.1260 - val_loss: 29579.4863 - val_mean_absolute_error: 29579.4863\n",
            "Epoch 491/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 14082.3770 - mean_absolute_error: 14082.3770 - val_loss: 30611.3164 - val_mean_absolute_error: 30611.3164\n",
            "Epoch 492/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13628.2412 - mean_absolute_error: 13628.2412 - val_loss: 30033.6270 - val_mean_absolute_error: 30033.6270\n",
            "Epoch 493/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13434.8789 - mean_absolute_error: 13434.8789 - val_loss: 29278.8730 - val_mean_absolute_error: 29278.8730\n",
            "Epoch 494/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13814.3193 - mean_absolute_error: 13814.3193 - val_loss: 29567.8867 - val_mean_absolute_error: 29567.8867\n",
            "Epoch 495/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 14199.3477 - mean_absolute_error: 14199.3477 - val_loss: 30521.5488 - val_mean_absolute_error: 30521.5488\n",
            "Epoch 496/500\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 13351.9854 - mean_absolute_error: 13351.9854 - val_loss: 29647.7539 - val_mean_absolute_error: 29647.7539\n",
            "Epoch 497/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13662.4775 - mean_absolute_error: 13662.4775 - val_loss: 30003.7598 - val_mean_absolute_error: 30003.7598\n",
            "Epoch 498/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13145.5674 - mean_absolute_error: 13145.5674 - val_loss: 30127.0547 - val_mean_absolute_error: 30127.0547\n",
            "Epoch 499/500\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 13655.3057 - mean_absolute_error: 13655.3057 - val_loss: 29614.0684 - val_mean_absolute_error: 29614.0684\n",
            "Epoch 500/500\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 13309.9414 - mean_absolute_error: 13309.9414 - val_loss: 29546.2148 - val_mean_absolute_error: 29546.2148\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f659cd2dcd0>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Y42dnt8evM"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_XboCvj9Js1",
        "outputId": "cd8d8164-cccb-40e1-9299-f84bdd54ffa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "111/111 [==============================] - 0s 1ms/step\n",
            "28/28 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "y_train_pred = NN_model.predict(X_train)\n",
        "y_test_pred = NN_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "F3R1G3Ju0iMI"
      },
      "outputs": [],
      "source": [
        "y_train_pred = y_train_pred.flatten()\n",
        "y_test_pred = y_test_pred.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tJAk-m_cgYyH"
      },
      "outputs": [],
      "source": [
        "train_n = X_train.shape[0]\n",
        "train_p = X_train.shape[1]\n",
        "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
        "train_rmse = mean_squared_error(y_train, y_train_pred, squared = False)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_adj_r2 = 1 - (1 - train_r2) * (train_n - 1) / (train_n - train_p - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WCr5L48yhgPP"
      },
      "outputs": [],
      "source": [
        "test_n = X_test.shape[0]\n",
        "test_p = X_test.shape[1]\n",
        "test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
        "test_rmse = mean_squared_error(y_test, y_test_pred, squared = False)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_adj_r2 = 1 - (1 - test_r2) * (test_n - 1) / (test_n - test_p - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFGOvIbd-tiQ",
        "outputId": "118bae98-765e-4336-b016-84a779b1231b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train MAPE: 2.92%\n",
            "Train RMSE: 26642.287082060706\n",
            "Train R2: 0.975277327887201\n",
            "Train Adj R2: 0.9747475052007408\n",
            "\n",
            "Test MAPE: 5.23%\n",
            "Test RMSE: 42732.13023848114\n",
            "Test R2: 0.9331120891819689\n",
            "Test Adj R2: 0.9269786252407863\n"
          ]
        }
      ],
      "source": [
        "print(\"Train MAPE: {:.2f}%\".format(train_mape * 100))\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Train R2:\", train_r2)\n",
        "print(\"Train Adj R2:\", train_adj_r2)\n",
        "print()\n",
        "print(\"Test MAPE: {:.2f}%\".format(test_mape * 100))\n",
        "print(\"Test RMSE:\", test_rmse)\n",
        "print(\"Test R2:\", test_r2)\n",
        "print(\"Test Adj R2:\", test_adj_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We0T1y5qg920"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "V-5bY8wRlhYW"
      },
      "outputs": [],
      "source": [
        "train_residuals = y_train - y_train_pred\n",
        "test_residuals = y_test - y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "hFIQdHAWMI5L",
        "outputId": "91fcefe7-69f9-4b54-c68d-f7d1c32ea81b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEaCAYAAACrcqiAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABRuElEQVR4nO2deXQUVdr/v9Xd2cje2ZqA7CAEgUQDKCqbDOo4ryijIsOgLHnRibLDKKvIJqOGHVdWeRnlwE/B8cyogxFQUQySAAICkU1IYpZOSAgJSXfV749KN71UVVf1Wp08n3M8kuqquvf2cp96dobjOA4EQRAEoUI0gZ4AQRAEQYhBQoogCIJQLSSkCIIgCNVCQoogCIJQLSSkCIIgCNVCQoogCIJQLSSkCCLAdOjQAUuXLpU8Z9y4cRg2bJjXxx48eDCysrI8usfWrVuh0+m8NCOCsIeEFEGIMG7cODAMA4ZhoNVq0bZtWzzzzDO4evWqV8fJy8vD9OnTvXpPb2J5DxiGQWRkJPr06YNNmzZ5dM+srCwMHjzYOxMkmjUkpAhCgvvvvx/FxcW4fPky/vnPfyI/Px9PPvmkV8dISkpCZGSkV+/pbdavX4/i4mIUFBTg4YcfRlZWFnbt2hXoaREtABJSBCFBaGgoDAYD2rRpg4EDB2LSpEn4/vvvUV1dbT3nv//9L+69915ERESgTZs2GD9+PCoqKqyvnzx5Eg8++CDi4uIQGRmJHj16YPv27dbXHc19RqMRo0aNQmRkJFJSUjB//nw4FoYRMtMtXboUHTp0sP599OhRPPzww0hOTkZUVBT69u2Lzz//3K33ITY2FgaDAV27dsWKFSvQpUsXfPzxx6Ln//vf/8Zdd92FsLAwJCcnIzs7G7W1tQCARYsWYdOmTThw4IBVQ9u6datb8yKaPySkCEImRUVF2L17N7RaLbRaLQAgNzcXI0aMwNNPP43jx49jz549uHjxIkaOHGkVLKNHj0ZCQgIOHTqEEydOYOXKlYiPjxcdZ+LEifjpp5/wr3/9C7m5ubh48SI++eQTxfOtrq7GqFGj8PXXX+Po0aN48MEH8eijj+Ls2bPuvQE2REREoLGxUfC148eP49FHH8XAgQNx7NgxbNu2DZ999hmef/55AMCsWbPwl7/8Bffccw+Ki4tRXFyMUaNGeTwnonlC3k6CkGD//v2IiooCy7Koq6sDAMycOdNqnlu8eDGmTJmCyZMnW6/Ztm0b2rdvj2PHjiE9PR2XLl3CjBkzkJaWBgDo1KmT6HiFhYXYs2cPvvzySwwdOhQAsHnzZnTs2FHx3B19PkuXLsW//vUv7Nq1C/PmzVN8PwAwmUzYunUrTpw4gezsbMFz3njjDdx5551YtWoVAKB79+5Yt24dHn/8cSxduhTt27dHRESEVUslCClIkyIICfr374+CggL8+OOPWLBgAe655x4701xeXh5Wr16NqKgo638WYXTu3DkAvOZgCRRYtGgRjh49KjreqVOnAAADBgywHgsNDUXfvn0Vz72srAzZ2dno3r074uLiEBUVhZMnT+LSpUuK75WVlYWoqCiEh4dj+vTpePnll/Hcc88Jnnvy5EkMHDjQ7tigQYPAcZx1fQQhF9KkCEKCiIgIdOnSBQBwxx134Ndff8XkyZPx/vvvAwBYlsVLL72EsWPHOl1r0RIWLFiAMWPG4PPPP0dubi6WL1+Ov//97y7DzqXQaDROfipH89u4ceNw+fJlvP766+jYsSMiIiLw9NNPo6GhQfF4y5Ytw4gRIxAVFYWUlBQwDOP23AlCCaRJEYQCFi1ahC1btuDIkSMAgMzMTJw8eRJdunRx+i8qKsp6XadOnZCdnY3du3dj8eLFePvttwXvb9HCDh06ZD3W0NCAvLw8u/OSk5NRVFRkd8xRQzt48CCys7Px6KOPolevXmjdujXOnz/v1rpTUlLQpUsXGAwGlwKqZ8+eOHjwoN0xS5BEz549AfDaodlsdmsuRMuChBRBKKBr1674n//5H6tPZ/Hixdi7dy9mzJiBgoIC/Prrr/j8888xceJE1NXV4fr163jhhReQm5uLCxcuID8/H59//rlVGDnSpUsXPProo3jhhRfw9ddf49SpU8jKykJNTY3decOGDcO+ffuwa9cuFBYWYsWKFfjmm2/szrn99tuxY8cOnDhxAgUFBRg9erRfBMPs2bNx9OhRTJ8+Hb/88gs+//xzTJ48GWPGjEG7du0AAB07dsQvv/yCkydPory8HDdv3vT5vIjghIQUQShk9uzZ+PLLL7F//34MGTIEubm5OH78OO6//3707t0b06dPR3R0NEJCQqDT6VBZWYmJEyeiR48eePDBB5GSkoJ//vOfovffvHkz0tPT8ac//QmDBg1CmzZt8Pjjj9ud8+yzz+KFF17ACy+8gMzMTPz222+YMmWK3TlbtmwBy7Lo168fHnvsMTz00ENu+baU0rt3b3z66ac4ePAg+vTpg7Fjx+KRRx7BO++8Yz1n4sSJ6Nu3LwYMGICkpCR8+OGHPp8XEZww1JmXIAiCUCukSREEQRCqhYQUQRAEoVpISBEEQRCqhYQUQRAEoVpISBEEQRCqhSpOuIljIqUjiYmJKC8v99Ns/AetK7igdQUXzX1dqampiq8lTYogCIJQLSSkCIIgCNVCQoogCIJQLSSkCIIgCNVCQoogCIJQLRTdRxCEx7BlJcDeHeCqjGDi9MCIMdAkUdddwnNISBEE4RFsWQm4VQuBshIAAAcA58+Anb6YBBXhMWTuIwjCM/busAooK02aFUF4CmlSBEF4BFdlFD5eVgJ2Yw6ZAAmPICFFEIRHMHF6CDalu3oJ3PkzAMgESLgPmfsIgvCMEWMAR8ETFg7crLc/RiZAwg1IkyIIwiM0SQaw0xfbRfdxpcXAhbNO54qZBuVgG0F4LaU12IeeIK2sBUBCiiAIj9EkGYCsmda/2Y054ASEFBOnd+v+jhGE9WdOAKePk/mwBUDmPoIgvI+QCTDJwB93B4ogbLGQJkUQhNcRMgF6Et0nGkHogfmQCA5ISBEE4RMcTYCeIBZB6K75kAgeyNxHEIT68bb5kAgaSJMiCEL1OJoPw1Na4yZF97UISEgRBBEU2JoPY5tpm3XCGTL3EQRBEKqFhBRBEAShWlRh7isvL8eGDRtQVVUFhmEwbNgw/PGPf8T169exatUqlJWVISkpCdOnT0dUVBQ4jsOWLVuQn5+PsLAwZGdno1OnTgCA/fv34+OPPwYAjBw5EoMHDwYAnD9/Hhs2bEBDQwMyMjIwfvx4MAwjOgZBEAQReFShSWm1WowdOxarVq3CsmXL8MUXX+DKlSvYs2cPevXqhbVr16JXr17Ys2cPACA/Px8lJSVYu3YtJk2ahI0bNwIArl+/jt27d2P58uVYvnw5du/ejevXrwMA3n//fTz33HNYu3YtSkpKUFBQAACiYxAEQRCBRxVCKj4+3qoJRUREoE2bNjAajcjLy8OgQYMAAIMGDUJeXh4A4MiRIxg4cCAYhkG3bt1QW1uLyspKFBQUoHfv3oiKikJUVBR69+6NgoICVFZWoq6uDt26dQPDMBg4cKD1XmJjEARBEIFHFeY+W0pLS3HhwgV06dIF165dQ3x8PAAgLi4O165dAwAYjUYkJiZar0lISIDRaITRaERCQoL1uF6vFzxuOR+A6BiO7Nu3D/v27QMArFixwm58IXQ6nctzghFaV3BB6wouaF0C13p5Lh5RX1+PnJwcjBs3Dq1atbJ7jWEYMAzj0/Glxhg2bBiGDRtm/dtV+GtiMw2RpXUFF7Su4KK5rys1NVXxtaow9wGAyWRCTk4O7r//fvTv3x8AEBsbi8rKSgBAZWUlYmJiAPAaku0HWVFRAb1eD71ej4qKCutxo9EoeNxyvtQYBEEQROBRhZDiOA7vvPMO2rRpgz/96U/W45mZmThw4AAA4MCBA+jbt6/1+MGDB8FxHM6ePYtWrVohPj4e6enpOHbsGK5fv47r16/j2LFjSE9PR3x8PCIiInD27FlwHIeDBw8iMzNTcgyCIAgi8KjC3HfmzBkcPHgQ7dq1w+zZswEAo0ePxmOPPYZVq1YhNzfXGh4OABkZGTh69CimTJmC0NBQZGdnAwCioqLw5z//GXPmzAEAPPHEE9Zw8qysLLz11ltoaGhAeno6MjIyAEB0DIIgCCLwMBzHCRUXJlxQVFQk+Xpzty03N2hdwQWtK7hoFj4pgiAIgnCEhBRBEAShWlThkyKCG7apjbc3OrASBEHYQkKK8Ai2rATcqoVAWQkA8N1Tz58BO30xCSqCIDyGzH2EZ+zdYRVQVpo0K4IgCE8hIUV4BFdlVHScIAhCCSSkCI9g4vSKjhMEQSiBhBThGSPGAI6+pyQDf5wgCMJDKHCC8AhNkgHs9MUU3UcQhE8gIUV4jCbJAGTNDPQ0CIJohpC5jyAIglAtpEkRBOF1KMGb8BYkpAiC8CqU4E14EzL3EQThXSjBm/AiJKQIgvAqlOBNeBMSUgRBeBVK8Ca8CQkpgiC8CyV4E16EAicIgvAqlOBNeBMSUgRBeB1K8Ca8BZn7CIIgCNVCQoogCIJQLWTuI4gghCo6EC0FElIEEWRQRQeiJUFCiiCCDamKDh4GK5CGRqgNElJE0EAbKI+vKjqQhkaoERJSLZRg2/DVuoEG4n1k4vT8+gWOiyFrnj7U0AjCXUhItUDUuuFLosINNGDv44gxwPkz9u+HREUHufOkmnuEGiEh1RKRueELPX0jMdHPk+VR5QYaIMGpuKKDzHm6o6HJIdi0dkJdkJBqgcjZ8MWevk2L1wO6UD/M0h5fbaCeEEjBqaSig+x5KtTQHBF8qAGCT2snVAUJqRaIrA1f5Om79sP3gLEv+nR+gni4gVrw5lO9GgWnEHLn6UnNPbGHGqS2U52ZlgguSEi1RGRs+GJP32Zjua9nJ4g3ipZK+WbcMmN6SXD6HAXzdLvmnphJsb5O8HTycxFyISHVApGz4Ys9fWv1iWD9N1U7PC5aKuWb6fGaW/MJhmrf/pinUqGjNm2TUC8kpFooLjd8kafvyNGTUOXz2fkGX/iQgqXat6/nKfZQg063A0WX1a9tEqqFhBQhiNjTt86QCpTLM/mpLaorWHxIQYnIQw0zKov/t4q+B0RwQUKKEMWTp29V5mIJbaRh4eBKi3Ft1SKwDz1Bm6ebuDQpBoG2SagT1Qipt956C0ePHkVsbCxycnIAANevX8eqVatQVlaGpKQkTJ8+HVFRUeA4Dlu2bEF+fj7CwsKQnZ2NTp06AQD279+Pjz/+GAAwcuRIDB48GABw/vx5bNiwAQ0NDcjIyMD48ePBMIzoGISHSPh/2BFjAvJkbbeRlpUAVy8BN+uBC2dRf+EscPo4hUZ7QLCYPongQjX9pAYPHoy5c+faHduzZw969eqFtWvXolevXtizZw8AID8/HyUlJVi7di0mTZqEjRs3AuCF2u7du7F8+XIsX74cu3fvxvXr1wEA77//Pp577jmsXbsWJSUlKCgokByD8AxR/0+ThsUdPgCcOQHu8AFwqxbypkE/oEkyQJM1E0ySgRdQtliCKHwEW1YCdmMOzG/OA7sxx29rJohgRjVCKi0tzUmDycvLw6BBgwAAgwYNQl5eHgDgyJEjGDhwIBiGQbdu3VBbW4vKykoUFBSgd+/eiIqKQlRUFHr37o2CggJUVlairq4O3bp1A8MwGDhwoPVeYmMQniHq57lWKR5h50f8nYjLBlg4E0SwohohJcS1a9cQHx8PAIiLi8O1a9cAAEajEYk2eS0JCQkwGo0wGo1ISEiwHtfr9YLHLedLjUF4yIgxfBSXLUkGICZO8HR/582ICVGfBVGImD+5nRt9Mx5BNBNU45NyBcMwYBgmYGPs27cP+/btAwCsWLHCTkgKodPpXJ4TjMheV2IiTIvXo/bD92A2lkOrT0Tk6Emo/fA93v/jQHhKa8S6+X6ZSoqcxtEZUqWvGTcZxsLT4CpKrceYhGTox02GTuY8lIxrrK1Bo9ALpwoQZ2pwOV93sf283Hmf1EqL/30FGZ6sS9VCKjY2FpWVlYiPj0dlZSViYmIA8BpSuU0YdEVFBfR6PfR6PU6dOmU9bjQakZaWBr1ej4qKCqfzpcZwZNiwYRg2bJj173IXYdiJiYkuzwlGFK1LF2otocQCqALAPvQEcPq4U6jyzYeecOv9cowibARQf/o4GBcBEGylERxnH5DOcRyMlUZoZNQmVDouGxktfKPGBhi3roPGRwEHls/L3fdJrdDvK7iwrCs1VflDkarNfZmZmThw4AAA4MCBA+jbt6/1+MGDB8FxHM6ePYtWrVohPj4e6enpOHbsGK5fv47r16/j2LFjSE9PR3x8PCIiInD27FlwHIeDBw8iMzNTcoxgI6ic8qntgOhY/r8+/TzbKKWqSLi6zlhmf8xYJt83pnTcEWMAnfAzIVdaLG9Mmdh+F66tWmTNV1ODL5AglKIaTWr16tU4deoUampq8Pzzz+Opp57CY489hlWrViE3N9caHg4AGRkZOHr0KKZMmYLQ0FBkZ2cDAKKiovDnP/8Zc+bMAQA88cQT1mCMrKwsvPXWW2hoaEB6ejoyMjIAQHSMYCKQOUlKEnYd5wmAr0bgAUoCIFiLD+j8GeDGdeHrrl4CuzHH5XpExz1+BOzGHKfrNEkGmCNjgGsC11VXCd7LMmcl4fqO73H9mRO85holbCGgGnqE2lGNkJo2bZrg8YULFzodYxgGWVlZgucPHToUQ4cOdTreuXNna/6VLdHR0YJjBBUB6mukWDj6YJ5yq0iwZSXg3pznrD05cuUiuCsXAUivR7QMUF0tH8EndJ0+SVhIxcYLTsWthw+x95gVrrjorUARqzAtLeaFbkw8mGQDVZcgPEbV5j5CHq60CZ+ZAhWakHwS9i0WRehYG07IvCcHsfUIjetwHffabLv3m0kWPp9xp1mhCKLvZX0doAuxP+alGnp24fUXzgIVpcCFMxRmT3gF1WhShPtIaRO+NAUqFTq+qJ0nt8K3J4JQ6Fq7cY8fAepqnS+suWanVSlt7eGOUBfV8Gprbv07JARIywAzKss7Wo6QMLVAvaMUo7aal4GGhFRzQGrz86EpUKnQ4e4bDuR9C7Bm27PB9cr0aB5yyvGIbt4yEFuPZVx2Yw4vjMRoer81WTMVtcxwS6gLfRccaWwEEx7hNK67m6OrBwDye8nHVFKkvpqXAYaEVDNASpswe9nEZiopArt1HX99eATvZ7E1o0loBsy3X4KzE1AAwAHbN4DtdLtvf4QjxgDnTgmY/BjAVhRotPZCVI5JTIZgsLzfiurbjRgDnD0JVNqEJMcnSs7H6btQcgXctUrR+VjwRON29QBAVeblU/vhe9TJ2AESUs0Esc3PmyY2tqwEVWteBff71VsH4xOBPv2A+jqXT9+igvFmvc9/hJokA9hZy25F9wF8r6NhI8B8+yV0tTUwRUaDu284L0xdaBNOWsczk/nrThUANc5VS9zeqB2Ty2UktNt+F0K2r0f9wS9dz8cTjVtKSFPvKEWIdb5uydooCanmjtAGogsBV18HtqzEugHLMvXs3QGzrYACgMpyMN16QvPifJdTkXri9sePUJNkAGzmabtmbUprmB56AtokA9C9l+R9xLQOTF8M3DccWL/Evnituxu1VC6XTIEeOXoS6gWSpx3n40lQi532Zonui43nA0L84E8JlA/HF+Nq9YmClUlasjZKQqqZY9lAuJ0bgVP5QGMjYGoEjv0Irugyv7kAskw9HkfnjRgDFBx2rj4O//8IxfKJpMxb1k1JSFuy5GAVXbZfX1g48MxktzYvb0RD6gypwDOTga1rgBu1QKtIwfl4qnEHqk1HoHIEfTWu3IeKlgSFoLcANEkGMOERvICyxWLOkRnqLLphXbkI88tZMC+fLRnizmsyC/iN25ZA/AgVhnfbhVkLmPMACJu8btaD+dbZ3CYHbxTBNZUUAR+s48PC62r5/3+wzukz4u4bzvvjbNFo+eNeQC1pEF7DR+PqDKlgpi8G038QcHsvMP0HBW3pKm9BmlQLQapCgmi5HsdrRoyB9mKhs8mvtob/r6IU3IUzkk+U2u69wL6y1m/mGTGTjGItRSrM2gViVShcmosUhqwLIdcRLxjUwpp5AevC/OkKNaVBeAvRcU8VwPzmPI++19Q80h4SUi0EqQoJUtfYokkyIG7RGhi3rhMNEAAg6XAX2pgBwLx+qV1AgzdyeAQ3x6OHYE7L4CMTBRANn3e16SUZ+JqEx350fk2gCoXg3AoOw5zaDkxya+sGJxa1KdcfItcR79PNXkVpEN5C9PdUc43vFwa0+NBxb0FCqqUgJ3/GFpEndp0hFZqsmTC/OQ84c0L0ctHaeY4b89mTgNkMVNuESR/7EdxvF8DOWubyBy65WQttjo28Pw4xccrC58U2pehYMGnp1uu4osvyEluF5tbUyp67cNZ+g3PYyJVoJnId8a4Swj3RfH0qAL2gbXptXEdaeOi4tyAh1UKQVSEB4E1/Pe90qcm4lRsjtDFXirQlMJaBy5kPc2KK6MboarOW3ASrq4DuvcF0TQNXZURoTCwaGhrAbVsH1kbDs2zOgjlhYeFAYor1T7v3+NiPfCkiByxzcrlBS21wCjQTOY54tqwEXH0dX4nC1m+ZZOB9Uh6a6nyp7citOOJtHMdF0WVBy0JLDh33FpJC6m9/+5usm7z99ttemQzhW2RVSDCZBKsROCH1JCmScKr4B1tRyvu5ACdTmVSUnWWzdlll4uolaGYuBVtWAvOaV4EmXxsH8Im/HOecSNunHz/m1Uv2ms9Ph2DuyZcaskYxCmDZmOVUwLB9v+wquNeKVHAXeH8tjnixTVywMr0uBLCsZe8OcJ6a6nys7QTKh2M7rthvqiWHjnsLSSE1efJkf82D8CcjxgA/HeJD0QVQkhvD7dwInDwKmEy3XhRJOPWkNJFlY2RHjHHeVB3g8r6F+VQB0Ka9s/ZjS+11q8BzCgYRuqYpJwzhEeAs/jMLNmH9SG0nGGaPsPBbG7MMc5Flg2PLSsC9MVdc63Q43xGuogwoPA3cqAVX/jtQUXarOK6QVmZqBCyV4L1gqguUtuNXAmV2bAFICqm0tDR/zYOA/5ISNUkGmHtmCDv5oSw3hg2PAGcroADeVPfabLBNvhrrGpT6xRzgykrAyImyY828tvPLcd73pAsRFsisGdzCF5xD4qXmcKrAXiA7UlbC5yMJ0aa99b2w27jLSm5pZhZsN7i9O1wKKLENsf5EPm+us0Tu1dUCqxbCPH0xtN17iQubilL+YSC1neDLYt8Rse+wqG/NoQKI14re+pkWIYgDhCKf1MWLF3H69GnU1NTYtd4eNWqU1yfW0vB3UiIzKos3Uzk23NMnOW12thvPtZTWYB96wjon0U3OoQK4ZaOy/pClogPFuHIRnFJVrLoKiIwW1RphahR/TQg5cxZpqOjYksPOXCTxgOJSa+nTT3BzZ8tKcG3ZTIeCvuD/3roGWLFRWrstK+GFVJJBloag5DssqB0qCJhRIxQ67htkC6l9+/Zh27Zt6N27NwoKCpCeno7jx49b27ATHuLFMF3ZGpljf6HQMGDkM7z5q+la7r7hfDKoSGUGlyY8hzVYfsiuogMFabgJXL2o7BpAMIDBJRqNaKNAlwhJUltTH0Q+I5HP2WWQikhFc27VQmGzI3BL23Ol3dbXSfq07FDyHRbTDpvKPrFNFfxJKyFkC6m9e/di7ty56NGjB8aPH4/Zs2cjPz8f3333nS/n1+yxblbHjwi+rsT2bzWfnMy3ageiJY52bnT2uzTcBLZvANe0sVlyd5w2OtuNR0EFcFvc9k813FR+jVnCPOeIRgtotUBjg/JxAD46UsgcmNpONFjBldbM3TccOPq96JwEvyOuzKKtIgHY+BZz5vOBKg4wcXrZGoIS/5XU95orK/E4opBoPsgui1RdXY0ePXoA4Nu3syyLjIwM/PTTTz6bXHPHrtSOSEi4XP+QdeM79qOz+cqhXAtbVsILMiEcBZLIk7ht6wlrGZfIaOF7lv8O8y8n7MricPcN5zU3IWLi+Ei6QMCa3RdQABDeSvh4ddWtte/cKLukDltWwmuyEnMS+o5IPtxotMC4qbf+TDKAmblUXodjCUS/q+ERTiWRJL/X1yoDU+qIUCWyNSm9Xo/S0lIkJyejdevWOHLkCKKjo6ETKalDyMDV0y7DgDvxE8zrl7p2KLu4l20reS5nvjI/jNDUbCLPsHcHuKuXxQMGKkqBVQvANZnPrCHeIaHCmpFWB0yYzvtNBJ7uVU19HS9gbU1ZGq19OL1YGSqhz8/VdyQ+EVx9HczLZ/MJ0TFxfMUKkYoaCA0DJi+E1qHUkVLHv5C5krtvuLPmHZ8I/HYBXJPWbq0Y/8xk515ZAO8TjYkT/Nwp56hlIlvCjBgxAlevXkVycjKeeOIJrFy5EiaTCePHj/fl/Jo1Ln90HMc74WU4lF3eKzzilrbl6cavTwJ333C+lJGNaVESR/+OWFg4wG9caxbxfpyQUM80G39jauQ/s9Aw3q9lFtDMxKIDhZoTlkoIqJBQ/n21jdKsKOUDYvRJzsKySeuV1R/LhYByMlda8socK8CntAF+OWZ/g7ISvibg7OWC0X3Yu4NfgwOUc9QykS2kBg8ebP13RkYGtmzZApPJhPBw+eG7hD2K/DIu+gjJupcHRVLtSG5tF0zhE5RG3akJsWAFV1QZYZ4ymvcXjZsKJiFJOlCksQG4JvJwYiwD+vRDeK87Uf97sWTdP8fgGJc+IKHvkdBDx8160flzVUa+d5dAHzKWco4IG2QLKdbhSVij0SA0NBQsy0KjoY4fbiH0Y2QY4egwiNfDw94dfBKpVERafR1f+sYbFJ4OXgGiZswmoM7E+ydz5oHT6pQFfThSX4fYRavRWM5rU2xZiZP2Kys4xgFvmN2ktKKWlnMkleJBKBBSo0ePFn1t586dXplMS8NlQqcDjj9swZI2Lq51u+KDLSSgPEOnA1pF2xfVFcITAQX774vkd8VFcIzQfWV/jzrdzte1U6gVtZScI3eab7Y0ZAup9evX2/1dWVmJPXv2UJ6Uh9jV03MstWNLkx+I3Zhzy0wjFAUlOIgG3MVC0SRTws+wrO8/C0dB4I6pVyz4QsgCoE9yrnWYZOB9TE3jtwStSArBNjU+bGPSXJAtpJKSkpz+fvHFFzFnzhwMHTrU6xNraYiaUDRaoNddwLARwAfrrBFgijQilrUWTyVUAMu6nygsRngEnwJgie5zEARumegunwdbVuIkUMTMcVxFmWibevN9w4Gta8BdPg8UnoZ53FS+AaafSoEFGrHcOETFCp5PkYy38Ch+/MaNG6iurvbWXFo0ou0M+t4HjUXT8mWgAhHc3N4L2qYgBGvRXBsfh6hWJEVlObhFk2EWCVm3fdK35nNZIkfravk29dMX88JLqH7ghGl2moSvk3YDKhDFNCaRhxWKZLyFbCG1bt06MDbVrW/evInTp0/j/vvv98nEWhwuIproyYqQ5LcL/CYMOPs4Thx1X3NruGlXkFYUKbNV4Wnh+oEfrHfOk7NUTRGI+vMEqSoflvn7UniJ/n5j4viAJ4pkFEW2kDIY7D+0sLAw/OEPf0Dv3r29PqmWiKuIJo/aXBDNH2MZv7lfueicBydWQV1u9KBNQVrBl8tK+ILBAnBVRuC6iLWlUSQA51S+oJnRI0SEKLdzo11gh6+0OVFLSXJr4H9nWX/34SmtcZOi++xgOE5xXWkCQFFRkeTriYmJKC930V5BAWxZCbhXp7ifg0M0fxw767pCaYi7LoRPUu6aZq2A4jLCNFYvnsslkajN9B8kWnAXUP77Ei1oHB0rWNne1fhKYctKwL05zz6fTJ8ExiFB39v7hlqwrCs1NVXxtZKaVG5urqybUOCE79EkGWBObQcIZOITBABlAgqA4oQES4L1sR/BnSyA+baOfBi9WAUTjVZcQAF8RQqxormnCmB+c57XzG9KLRE+Ma876gOkH8hCUkh988031n9zHIczZ84gLi4OCQkJqKioQFVVFbp3705CyseYfznBm1uqKgI9FaK5oNF4ljRnagAuSKRMMIyzH8qWiEhxMyDAazdnTnjP/Cbm801tJ9j80+uBC0KtSSrLKdRcBpJC6pVXXrH+e/Pmzejbty8eeeQR67F///vfKCmhiDNfYv7lhH1kFNF8iIwGamsCM7a3Q+AdcaUlKClM7YW8ITGfLwC+WouPAxeUtDHxlOYW1i/7m/LNN99g06ZNdsceeughTJw4ERMmTPD6xIgmtq4hAdVc0WjUU0BXowU41j8mqNAwvvCuELoQwYomkiXBHISO2AYtVsXCHyWYRAMnvKyx+bvDtz+QLaTi4uJw5MgR9OvXz3rsyJEjiImJ8cnEmhtuP92Itb8ggp+aa7xZTA2ER4j2NPM6DTeFW7SEhPDlogT8WI6buamkyHkzPnuSfz8d2oK42qD9UoLJX0VzxaIYX5sNNi09KLUq2UJq/PjxyMnJwaeffoqEhASUl5fjypUrmDFjhi/n1yxw5+nGItTc6kRLBA9qcZ6roWRWY6NwoEVYOF+p3YbaD99z3oyFQu1VUmLIX0VzRc2HNdf45qpBqFUpCkGvrq5GQUEBjEYj4uPjceeddyI6WqQba5BRUFCALVu2gGVZPPDAA3jsscckz1cSgs5uzOG/II4kJAOJKU5fWMFwVYIIFkTCuj0iyQA8MxnMt1/ym3zJFb52pRxu7wXtrGWiL9taOayVOerrAuLP8TQEXXSvsUFueL03fVs+C0F3JCYmBgMHDlQ8iNphWRabNm3C/PnzkZCQgDlz5iAzMxNt27b1yv1Fn25su7XaPOFwOzeSgCKCl7obQJ9+fJfiwtMeV3MHwGtE65eAa8oTVKJ/Cvl9rBtwaQlQJNx9ICj9OUJmRQfkBGuoybclKaSWLVuGefPmAQAWLlxoVxbJlldffdX7M/MjhYWFMBgMSElJAQAMGDAAeXl5XhNSsnI0bMvBnDvllXEJIiA05VJJ9UZzC1eJ7PGJdj4pAIJ+HyUtboLNn2NnVjxVIJyoLCdYQ0XV2SWF1KBBg6z/bs65UEajEQkJCda/ExIScO7cObtz9u3bh3379gEAVqxYgcTERMl76nQ66zmmcZNRdbEQZleVyE8VIM7UgAo1RHsRhKf4y9+m1QIaDUJu64BWT4zHzX17YTaWQ6tPROToSdAZeBOTqaQItR++h5vH8gC5pkLA6s/RXixE3KI11vt5G1NJEapXvwpNRZnT3BWRmAj0eA2mkiJULZpqt+9oU9ogbtxk6FzsX8baGgilhmsrK5Dg4lohbPdDpVBZJAA//PADCgoK8PzzzwMADh48iHPnzmHixImi1ygti2Rn9y7/XTxLv08/4Oej3jGREERLQ6DUEKBQe5LAV/4cwfmFhQMvLpAu7KtwHtx9w2/59STmJerbCgsH88paxRqlX3xS3377LTp06IC2bduiqKgI7777LjQaDbKystCmTRvFA6sJvV6Piopb1RwqKiqg13s3f8E2zJUtKwH3ygvCZWyO56kn4osggg1jGbB3B9imhoLWzbm+zmMBBfjQnyNkXrtZD6xfAlahUBASkNqmOotYtdC+J53YvEaMAQoOO5tYb9b73eSnkXvizp07ERUVBQD44IMP0LlzZ/To0QMbNwpXRg4mOnfujOLiYpSWlsJkMuHQoUM+7TisSTIAaRnCL5KAIgiP4JqEBHf4AF9a6fAB4GS+9EVh4UDHbrwlo08/PkJRiPLfrS1RRJHy54jNWUz4WYSCTFiBtXOrFt5KaZE5L42lZJTQXI8fAbsxx/X74CVka1LV1dWIi4tDQ0MDzpw5g5kzZ0Kr1UqaxIIFrVaLCRMmYNmyZWBZFkOGDMFtt93m0zGZUVnO5VgIgvCcy+edq1YIVLEAwFe46JlhrepuQdQ8WFHKb/oSWpFoCaSyEt6MprAVj6LSSRKCSGlpJia5NTihgtZ1tX7NuZKtScXExKCkpAQFBQXo3LkzQkJC0Ki46rJ6ufPOO7FmzRqsW7cOI0eO9Pl4miQDmOmLxZ/YbAkJ9fl8CKLZICaQQkKEzy267HRY05SXBY3AFulCKxKNnrt6SVjDAXjzWli4svsJICWIxO4jev8RY/joSDFcvA/eQraQ+vOf/4yXXnoJb7/9Nh599FEAwIkTJ9C+fXufTa65o0kygElLlz4pyQAYgtvnRxBeIzoWmLkM4QOH8+a5WD0QFc33xnJFWgafQO9IWQm4nPlO5ivm2y9FC/HaCgO2SUMyvzmP15TuG+68uYeFO/t3bDZ5TZIBeHEBGEsysQWFpZMkBZGQ0JG4v+VBmuk/iK9aL4A/OobLNvcNHjwY99xzDwC+Ky8AdO3aFdOmTfPJxFoMQsl3YeFAaju+a6fFAfzbhcDNkSBUApOWDk33Xoi9bwgabaJnzTPGSle5SDLwJvZt64QjawXMeFIbsEUYiAVJ2FXHiNODKy0W7AVnO4a2ey/ErdoO49Z17ld5kKgR6E5pJkvAl1i0n9dbmgigqOJEQ0MD8vPzUVlZiREjRsBsNoMi2D1DzheHHTEGyP+B6vgRLRtdiHsFWfv0u9VJWCqx3iFZVdRPFBZuV3FdyAfEfPulXag6uzFH2L9T/rtdc0ddjzs86gjsaj9xu5iuvwrkCiBbSJ06dQo5OTno1KkTzpw5gxEjRqCkpASffvopXn75ZV/Osdnj6oujSTLAPHkhsHIB306BIFoiPTPEn/o73S7YvNCC9ToXZYPstCcxK8eLC1xqW07Hhe6l0TqVRjMtXg/oPPNB+6Kqu78K5AqOLffErVu3Ytq0aZg3bx60Wi0AoEuXLvj11199NjniFtruvYAZS5ydq2HhfPM8gmjONJnrxGBGZYn7pU7mW/1N1oAlId8U7M1Xdj6Z23uB6T8IzCtroe3ey+qHEgq6cLyP0L2QkOzcJ66shK/urlI0SQZosmZCO2sZNFkz/VYiSrYmVVZWhl697DOfdTodzGLNywivo+3eC+wra4W7i3ohm57wMxoN8MQEYNcm/+THRUUD193sBCzSjNDnaHVAq0jRnB0LmiQDzHfcKaxNmRqdE1DbdgCqK+0T6gXMV0JaicvqFSJmMNt7md+cJ+gbMxvdr4DeXJGtSbVt2xYFBQV2x06cOIF27aS/PIR3EXqasT4dxiW4vgGhHlgW+M8uPkLN18QngpmbA3Tv7d71StIgQsPcG0MIs4kPiDj2o33ItgDMqCzhMHPcMr9ZBcyxH28JKF0I77eSm/Mj5IcCgOhYXtuScR+xgAOt3r36ds0Z2UJq7NixWLduHdavX4+Ghga89957eOutt/DXv/7Vl/MjZKJJMgDtO4u8qAVi4vw6H0ImNdeAqgr+M/LxONyymcBvF/hNWSmthEOQBWnbwfPcPjfyk6QquViFgpCAMTWCCY+Qbb4SjfpLbSffDCYSDh45epKsObQkZAupbt264Y033sBtt92GIUOGIDk5GVOnTsWnn37qy/kRSqivEz7eNQ3My6/zrQwIdcKaeeEhJ9/HHUyNQG0N/59Ss118IjBuqnzB89t5wJNK/gnJQPuugi+5ysthRmUB+iT7g/qkW2ZxhVUXBMdQmhQrgKC/a/pin1VYD2Zc/iJu3ryJTz75BBcvXkTr1q3x5JNPorq6Gtu3b8fHH3/cLJsgqhmp6spiIbNMnJ6Pzpm9HNzrc/gnd0J9eOLz8ZXPKFYPTJgObfdeME95BVj7qmsBJKcSjUYjmihr6VbNXTjj9JIsQeDo37P5W+o3IhuZ4diuKqH7IgqvOeJSSG3atAkXLlxAnz59UFBQgMuXL6OoqAiDBg3Cc889h5iYGH/Mk4CM6soufjyaJAPMKakkpJoLCcm3NvT6OskQbLe5ZgQ+WAd2+uJbgmrrGuBGLR9ZyrJ8AIIFOcLS8p3cvEpQUFkDgtzJy9m7A6h0CD6oLL8VOOGFfB9ZuY0q6mwb7LgUUseOHcPrr7+O2NhYPPzww8jOzsYrr7yCtLQ0f8yPsMVFt0yhHw9333Bg7w6YS0v4zUTMJEgEH4kp0M5aBqBpU/RVweKm7xg7YgzwgU3Fhrpa3pTW1CrepbAMCQHSbhVzNcfqgfVL7MsFKaiOIKSpuDLnOd4XTWWIuG3rwCrI/XGpBamos22w41JI1dfXIzaWL4KakJCA8PBwElABQqk9nauvAzavAuf4ZEk0C5xMVKntbj2EtGkPhIVDV3cDpgtnpU10ISEuTXRclRGM0MZrLAPTNQ2aF+cDEBGWIpXGxVIqbKsjsCPGgLG83iQoNUkGmEqKhMsRiYSqO+Y/IWumT7Udb/i+CB6XQspsNuPnn3+2O+b49x133OHdWRGCuLKne6v7qKrRJ/EbrlSdtpaATgeuvs4aku30uVeUgpm+GAk97kDp6Z/B7dzIb+IA0KYDEBYmT/tpgonTi2+8pwrAlpVY0yGUVCaQ0kikhEjt57uFNZXUdrw2Jsec50Ntxyu+LwKADCEVGxuLt99+2/p3VFSU3d8Mw2D9+vW+mR1hjyt7ulj+hq/R6YCed/JP8eW/Cxfw9BStDrjjTmvVAe7NeXwX1paKycTnDhVd5jdmgc2Wy5kPY+u2QGS0kxZji0tTYdN3jNm7Q7iWXc01u+KsXgsIkBAi5lqRpOTqKj5nUIaQ9Km2E8Bad80Nl0Jqw4YN/pgHIQNXT6mKf1xaLaCkYoiYU7xLT2htzT3e0uYs1QY63e7clG7WMnAfbADOHA++bsZSkW1KKSvhgxiEqChFo+WB4eghmNOcTW6AuJ/GomlZfURSde984G+REiLalNYQNFA2lSmSU6TVl9pOIGvdNTd8lJRB+Aqpp1Sp7p5OJBn4Ejl1IhucELd1hKbmGtjy3+2P/35V1NzjkWbVKhLaldsB3OrZY/uD185cYuc8x9mfg0NgWTQfx/5CABDeCoiMAupuADeuy7ufnPMaG62al5DPRY72Y/lsuddmC5pbve1vkRIikaMnof7wQef30NJuXY6w9LG2QyHm3kF2Mi8RBAhlsVuirzrdzocsd+zGJxA+M5kvOaMAJrk1dB0FkiwtIb5N2JZuYmYudb9MTpOWZ9HOhLqa2o6FVlHujeNnmDbtgRcXOBcLTjKAWbiaf8+6pjlXhhCqwgAoE8wedlOVatTpdX+LRJM+nSEVSBVuuCpXWIol1JK2oy5Ik2pGKDExsBtzwIn1p+rcHTCW2+eb6ELA1deh0R07fngr93phmRqtmpIsB3eXHr7JFfImTb2ItEkG+cWCGYYvmyT0UCEjMs8Rx66yik1SXkpmdYWr7zOTbHA/4ddmDNJ21A0JqWaG3B+dpFCpMgITpgP79gKn8vlN0NRkLhK5RHRj2LvDPtnTgpykz4abt0x5ImuwM/cFokq3Utq0l2xAx27McRbIHCcsoBKS+Tp5CgWzq66yrkKw3U5mPXwQ5th4IGsm33pGBpLfZwpOaBGQkGqhSPqvKkr5pM3UdvKe0m07lTogKgxlChRrUrLQi+ERvgu5DwsX9hlZCA0T1g4TkoHYeODKRcHXGReahCK/TmIK3xLd8iAhCAPYvnuuokGbogLNTZUsxLQft5JZwfEVLFYthLmpgoUnUHBCy4CEVEtlxBig4LD4RlxWIr86hY124IiiYA6R60WfmC3z9DZJBt5nt2+vuJbSpj1wvdp+/JAQoG2HW2HyjgJU5CnfTht0DEqRwFKT0ZyWITrP0L73oaHhJlB4mv+sb9SC27kR7KgscYHo0C1WTLOSMudJClvWzJdWWrFR9lrFIHNd84cCJ1oomiSDsPPeDSS1AyHntxCR0c69gGxK5Ag5uGUL0dAw3iwmgSbZYHdvbfdefFh9n36C5zPJrfk59Ol3K8DBEkG3aiF/jgynvGNQiOxIyJAQq8BjRmUJV7jXJyHif57m23NYqp/X1vBzfGPurVBzKUQCLaSCWQAZfiGxsHmCcIA0qRaMpSwNlzNfeHPsdDufdyKlrbjwAcgNSWfuuJO/j0SJHCf/jRwtTaMFJi8E8+2X4K5cFD6HYaBr3xWNj491EiLMqCznRFfb+nLhEeAcTZdNG7sma6brp3x3E7A79+CTWi3vlcWHaKkq0ZRbdvPz3cJJz5XlQLtOztUZBBDUilwFs0jlVAHK+lMRLRoSUi0cTZIB7MylgqYpq9lq50bgZL69H0mkHpvYGJbNWjDZ12bTF2rVLepzGDEGOHvSPgoxKoZPAm64yW+E46bywjghCTj6vXANO45DQ943wMVzTqYtdxOouVMFML85z6WfxK3copBQoLQY3C/H+XsAwPkzgpqaZDvy+jq76gyiDxACWpHcQq7cBxuAX47Zn6TR8v2pCEIGJKQI1w7oF+dbhYWutgamyGi3HdSKwuTlRJ8xjP1FoWFgZi0TrJbtsvttWQm412aDTUt3qcVZEPW51VzjzWBCc5ZzvRgaDa9FOW78IhUftPpE4coMuOXTkvMAIXfeToVcZy6B+ZcTt9p7tIoEHh8L5tsvYf7sIwp2IFzCcFwwpOirj6KiIsnXExMTUV7e/KqP+3Nd7MYc3ufhSEIyn/C6d4fw6336eV6mqckP5mrzlHt/pv8gwVI9bFmJ/DqEYeG8H/Gzj3j/lSO397K27rAQZ2pAxby/Od9fqwW69uTvaVP+CID8BwjHeeuT7B4QhLRgQDigRGkSLf2+ggvLulJTlXceJk2KUB3Wze34EeETKkr5jS4qVvj1k/nWahRu+3xk1qJz8rkVXVZUMkiTZID5to7SQqqpgK/FtMp++6Ww9lX+O8zLZwHVVUBMPJhkAzBuMphZy3iTbeFpvoQSx/HVPJrMhYCDyVButJxEB1xBLfjcKb5moWPTTeqzREhA0X2EqrCLGpOqK1hWIpwkDPC+s6aINE/qyXGlxbLOsysD5U7JIKkoxchoMIvfgvbF+fa+OEetQ6Pl/UkXzjb9/wy4wwdQtYj3/WhfnM8Hp0gZTpSUTJLqgGt5XaD3lFhXaOqzRIhBmhShLpRoPjFxvKASSGS1bHqyfD5i1S+KLt/SyGxwGcwhswqC1VfWVLlbCOaOO11XLZco4mv+/SrfrDBrpixBIFdYiAZOHD/Cm2llCngLtlUwKDmXsIWEFCFIoDYLJU/UTHJrcDFxgomsVs3FVSg0wwBTF4HZsJRv/meLTUVt6/tx9TJQdMnaasMxMEJ2ySChiElH4hNFw/ttAx7Mb86TzK9SIrDl1L1jy0rEk47ranktWEn+XZMQ92WnXCJ4ISFFOCG1WSBRIGlU4b2lNnDRjdSxTFHTxsYAonlMgE0o9OtzhE1Nt/eCtnsvMO06wXT2pNPLlvqAksERTUKHDY+w78cksn7Je+lCgIhWgj20xHApfC6chfnVKXxrFp2Ob5gohIy6d9b5u0o6vlnvurQUwAfBNAkhwbqF5K9q8ZCQIgAIlOZx3IQsm0WP1zwaw+WTspi57JnJfEKugHBzpblokgxg//4aX2XB1o8SnwjmmRcBADpDG0EhxcTp5ZkgT+Y7J/UKrdHVvTp3t0boCfXQEhRarrTFhpt8LUFbQkL4UHaH6D6XQlGJObZNezBJhluC+/J5+/ffIarPp51yiaCFhBQhO4za481CRssNSXOZSEFS2Q37Zi8XFWaRoyeh/vRxQY2M27bO9dqkzHY2a3T1HrpTodzuPSstBq5ect0apbERTGy8/Ei+JhSZY5sCSiy4q0V7vU8VEVSQkCJkPx17ulnIfVL2VdFQqfvqDKl21Rfs2qa7MqcxjMvGg7L8QjIqlIuZvpx8VEJ5VCJzUoJSc6zYHAWh1huEACSkCHmblRc2C7U/KYtuolLmtLBwoGM3u5wjISQDOUJCgDT7ElOemL7kVrFw6313wxwrl5baesNWw7yW0hrsQ080+zUrIeBC6vvvv8euXbtw9epVLF++HJ07d7a+9sknnyA3NxcajQbjx49Heno6AKCgoABbtmwBy7J44IEH8NhjjwEASktLsXr1atTU1KBTp06YPHkydDodGhsbsX79epw/fx7R0dGYNm0akpOTJcdoSYhuagnJfM8ib20WQfqkbLd5lpUA1yqBmDgwya3FqyjYIhDIIbYRuwpLlyVYXPmoHOakBHfMsUrvH8xBEkqjYh3NuvVnTgCnj1NEow0BL4t05coVaDQavPfeexg7dqxVSF25cgVr1qzB8uXLUVlZiSVLlmDNmjUAgKlTp2L+/PlISEjAnDlzMHXqVLRt2xYrV65E//79ce+99+K9995Dhw4dMHz4cHzxxRe4dOkSJk2ahO+++w4//vgjpk+fLjqGRuM6x7k5lUUSq9kmVKrG03WpNQ/Gm+uyRvcpCUiADN+ggvJBlvloKytgqijlIwbrbgCx8XxrFZW87+7iyeflq++gkt+R9RqR0l9iZbSClaAui9S2bVvB43l5eRgwYABCQkKQnJwMg8GAwsJCAIDBYEBKSgoAYMCAAcjLy0ObNm1w8uRJTJ3KZ9gPHjwYu3btwvDhw3HkyBE8+eSTAIC7774bmzdvBsdxomN069bNDytXD/40swT7k7IYXlmXmG8wOpavZKHgM7HMJyGIHpb8gWPNQUu5JtahKLFbKPQjAhTRKIeACykxjEYjunbtav1br9fDaOQ/uISEBOvxhIQEnDt3DjU1NWjVqhW0Wq3T+Uaj0XqNVqtFq1atUFNTIzmGI/v27cO+ffsAACtWrECii3whnU7n8hxVkZgoK7w86NYlEzWsy1hbI1ixPKR9Z+hflv5sTCVFqP3wPZiN5dDqExE5ehJ0hlRVrMsXuLuuyvfeQINjnURjGUI+2Y74uf/waE5in5+utgZ6kbleS2nNm/gcCE9pjdhm9Ll58j30i5BasmQJqqqqnI4//fTT6Nu3rz+m4DHDhg3DsGHDrH+7ejoNJnOfEmhdvoONjBY8boqMlpybo5mpEUD96eNgpi9Gco87Ar4uX+Du52UWCXBp+OW4x++TO58f+9ATgEDqw82HnmhWn5vqzX0LFixQfI1er0dFxa0KAUajEXo97zS2PV5RUQG9Xo/o6GjcuHEDZrMZWq3W7nzLvRISEmA2m3Hjxg1ER0dLjkEQfsfdwBIpM5MHydeEQtz4/BxN7eEprXGTovvsUG0V9MzMTBw6dAiNjY0oLS1FcXExunTpgs6dO6O4uBilpaUwmUw4dOgQMjMzwTAMevbsiR9++AEAsH//fmRmZgIA7rrrLuzfvx8A8MMPP6Bnz55gGEZ0DIIIBJomJzvTfxBwey8w/QfJCpTwl1/DUgHD/OY8sBtz+ACEYKPT7cqOK8Ddz8+2in7s9EUkoBwIuE/qxx9/xObNm1FdXY0VK1agQ4cOmDdvHm677Tbcc889mDFjBjQaDSZOnGiNupswYQKWLVsGlmUxZMgQ3HbbbQCAMWPGYPXq1fjoo4/QsWNHDB06FAAwdOhQrF+/HpMnT0ZUVBSmTZsGAJJjEEQgcCcAwx/5Z82l+CszKgucY3mm+EQwo7K8cv/mGhgUSAIegh6sNKcQdCXQulzjzzB70YrqTU/13vJJqS1UWo0h6N6guf++VOuTIoiWgj81DsG8HIHqFd6gOYVKk7YTXJBtiyC8iVQQgz/GamwEEx7hdYEoZjpUS0krovlCQoogvIg/NQ6/ajdCLeuDoKQVEfyQuY8gvIg/i+j6cyylVUnU7PchggsSUgThTfxZRNfPBXvl+nKaSyQgoQ5ISBGEF/F3HURVtrZwo4adryCNLvghIUU0C9S0GTlqHLLbwLtAdI0qi1RTSyQgaXTNAxJSRNCj5s3IW3NT8xodUU1zSxVpdIT7UHQfEfz4M+xbKd6am5rX6IhKIgHVotERnkGaFBH0qHkz8tbc1LxGR9TiK1ONRkd4BAkpIuhR82bkrbmpeY1CqMJX5qPoRzX5P1sCJKSI4MfPodiK8Nbc1LxGleILjS6YfIPNBRJSRNCjFvOSL+em5jWqGa9rdBSM4XdISBHNAlWYl0Tw1tzUvMaWQjD5BpsLFN1HEAQhEyq0639ISBEEQchFJeH1LQky9xEEQciEfIP+h4QUQRCEAsg36F/I3EcQBEGoFhJSBEEQhGohcx9BBAiqXEAQriEhRQSclrhZU+UCgpAHmfuIgGLZrLnDB4AzJ8AdPgBu1UJecDVngqmqOUEEEBJSRGBpoZs1VS4gCHmQkCICSkvdrKlyAUHIg4QUEVBa7GZNlQsIQhYUOEEElhbagoIqFxCEPEhIEQGlJW/WVLmAIFxDQooIOLRZq4+WmBZAqBMSUgRB2EE5XISaoMAJgiDsaaFpAYQ6ISFFEIQdLTUtgFAnJKQIgrCjxaYFEKqEhBRBEPZQDhehIihwgiAIO1pyWgChPkhIEQThBKUFEGoh4EJq+/bt+Omnn6DT6ZCSkoLs7GxERkYCAD755BPk5uZCo9Fg/PjxSE9PBwAUFBRgy5YtYFkWDzzwAB577DEAQGlpKVavXo2amhp06tQJkydPhk6nQ2NjI9avX4/z588jOjoa06ZNQ3JysuQYBEEQROAJuE+qd+/eyMnJwZtvvonWrVvjk08+AQBcuXIFhw4dwsqVKzFv3jxs2rQJLMuCZVls2rQJc+fOxapVq/Ddd9/hypUrAID/+7//wyOPPIJ169YhMjISubm5AIDc3FxERkZi3bp1eOSRR7Bjxw7JMQiCIAh1EHAh1adPH2i1WgBAt27dYDTyYa55eXkYMGAAQkJCkJycDIPBgMLCQhQWFsJgMCAlJQU6nQ4DBgxAXl4eOI7DyZMncffddwMABg8ejLy8PADAkSNHMHjwYADA3XffjZ9//hkcx4mOQRAEQaiDgJv7bMnNzcWAAQMAAEajEV27drW+ptfrrQIsISHBejwhIQHnzp1DTU0NWrVqZRV4tucbjUbrNVqtFq1atUJNTY3kGI7s27cP+/btAwCsWLECiYmJkmvR6XQuzwlGaF3BhdJ1mUqKUPvhezAby6HVJyJy9CToDKk+nKF70OcVXHiyLr8IqSVLlqCqqsrp+NNPP42+ffsCAD7++GNotVrcf//9/piSYoYNG4Zhw4ZZ/y4vL5c8PzEx0eU5wQitK7hQsi7HckiNAOpPHwejwnJI9HkFF5Z1paYqf+Dxi5BasGCB5Ov79+/HTz/9hIULF4JhGAC8VlNRUWE9x2g0Qq/nkwltj1dUVECv1yM6Oho3btyA2WyGVqu1O99yr4SEBJjNZty4cQPR0dGSYxBEi0OqHBJF+hEBIuA+qYKCAuzduxcvvfQSwsLCrMczMzNx6NAhNDY2orS0FMXFxejSpQs6d+6M4uJilJaWwmQy4dChQ8jMzATDMOjZsyd++OEHALzgy8zMBADcdddd2L9/PwDghx9+QM+ePcEwjOgYBNESoXJIhBoJuE9q06ZNMJlMWLJkCQCga9eumDRpEm677Tbcc889mDFjBjQaDSZOnAiNhpepEyZMwLJly8CyLIYMGYLbbrsNADBmzBisXr0aH330ETp27IihQ4cCAIYOHYr169dj8uTJiIqKwrRp0wBAcgyCaGkwcXq+4rnAcYIIFAzHcULfS8IFRUVFkq83d9tyc4PW5eyTAgAkGcgn5Uea+7pU65MiCEL9UDkkQo2QkCIIwgqVQyLUBjlgCIIgCNVCQoogCIJQLSSkCIIgCNVCQoogCIJQLSSkCIIgCNVCeVIEQRCEaiFNyke8/PLLgZ6CT6B1BRe0ruCC1uUMCSmCIAhCtZCQIgiCIFQLCSkfYdt7qjlB6wouaF3BBa3LGQqcIAiCIFQLaVIEQRCEaiEhRRAEQagWqoLuIQUFBdiyZQtYlsUDDzyAxx57zO71zz77DF999RW0Wi1iYmLwt7/9DUlJSYGZrAJcrcvCDz/8gJUrV+K1115D586d/TtJN5CzrkOHDmHXrl1gGAbt27fH1KlT/T9RhbhaV3l5OTZs2IDa2lqwLIu//OUvuPPOOwMzWZm89dZbOHr0KGJjY5GTk+P0Osdx2LJlC/Lz8xEWFobs7Gx06tQpADNVhqt1ffPNN9i7dy84jkNERASysrLQoUMH/09UIa7WZaGwsBDz58/HtGnTcPfdd7u+MUe4jdls5l588UWupKSEa2xs5GbNmsX99ttvduecOHGCq6+v5ziO47744gtu5cqVgZiqIuSsi+M47saNG9zChQu5uXPncoWFhQGYqTLkrKuoqIibPXs2V1NTw3Ecx1VVVQViqoqQs6533nmH++KLLziO47jffvuNy87ODsRUFXHy5Enu119/5WbMmCH4+k8//cQtW7aMY1mWO3PmDDdnzhw/z9A9XK3rl19+sX7/jh492mzWxXH8d3XRokXc8uXLue+//17Wfcnc5wGFhYUwGAxISUmBTqfDgAEDkJeXZ3fOHXfcgbCwMABA165dYTQaAzFVRchZFwDs3LkTI0aMQEhISABmqRw56/rqq6/w4IMPIioqCgAQGxsbiKkqQs66GIbBjRs3AAA3btxAfHx8IKaqiLS0NOvnIMSRI0cwcOBAMAyDbt26oba2FpWVlX6coXu4Wtftt99ufb1r166oqKjw19Q8wtW6AOA///kP+vfvj5iYGNn3JSHlAUajEQkJCda/ExISJIVQbm4u0tPT/TAzz5CzrvPnz6O8vFz1JiNb5KyrqKgIxcXFWLBgAebNm4eCggI/z1I5ctb15JNP4ptvvsHzzz+P1157DRMmTPD3NL2O0WhEYmKi9W9Xv79gJDc3FxkZGYGehlcwGo348ccfMXz4cEXXkZDyEwcPHsT58+fx6KOPBnoqHsOyLD744AM888wzgZ6K12FZFsXFxXjllVcwdepUvPvuu6itrQ30tDzmu+++w+DBg/HOO+9gzpw5WLduHViWDfS0CAl+/vlnfP311xgzZkygp+IVtm7dijFjxkCjUSZ2KHDCA/R6vZ0qXlFRAb1e73Te8ePH8cknn2DRokVBYRpzta76+nr89ttvePXVVwEAVVVVeP311/H3v/9d1cETcj4vvV6Prl27QqfTITk5Ga1bt0ZxcTG6dOni7+nKRs66cnNzMXfuXABAt27d0NjYiJqamqAwZ4qh1+tRXl5u/Vvs9xeMXLp0Ce+++y7mzJmD6OjoQE/HK/z6669Ys2YNAKC6uhr5+fnQaDTo16+f5HWkSXlA586dUVxcjNLSUphMJhw6dAiZmZl251y4cAHvv/8+/v73vwfNhuBqXa1atcKmTZuwYcMGbNiwAV27dlW9gALkfV79+vXDyZMnAfA/pOLiYqSkpARiurKRs67ExET8/PPPAIArV66gsbFRkV9AjWRmZuLgwYPgOA5nz55Fq1atgsLX5ory8nK8+eabePHFF5Gamhro6XgNy36xYcMG3H333cjKynIpoACqOOExR48exbZt28CyLIYMGYKRI0di586d6Ny5MzIzM7FkyRJcvnwZcXFxAPjN4qWXXgrspGXgal22LFq0CGPHjlW9kAJcr4vjOHzwwQcoKCiARqPByJEjce+99wZ62i5xta4rV67g3XffRX19PQDgr3/9K/r06RPgWUuzevVqnDp1yqrxPfXUUzCZTACA4cOHg+M4bNq0CceOHUNoaCiys7OD4jvoal3vvPMODh8+bPW3abVarFixIpBTloWrddmyYcMG3HXXXbJC0ElIEQRBEKqFzH0EQRCEaiEhRRAEQagWElIEQRCEaiEhRRAEQagWypMiCIIgJJFbPNaCN4s0kyZFEAFkw4YN+OijjwAAp0+f9lvF9aeeegolJSVeudeMGTOsuWVE82Tw4MHWZHBXFBcXY8+ePViyZAlWrlyJcePGeTQ2aVIE4YIXXngBVVVV0Gg0CA8PR3p6OiZOnIjw8HCvjtOjRw9rRr4U+/fvx1dffYUlS5Z4dXwLixYtwrlz56DRaBAaGooePXpg4sSJoomyK1eu9Mk8CPWQlpaG0tJSu2MlJSXYtGkTqqurERYWhueeew5t2rTxepFm0qQIQgYvvfQStm/fjn/84x84f/48/t//+39O55jN5gDMzDdMmDAB27dvx5o1a1BbW4tt27Y5ndOc1kso57333sOECRPwj3/8A2PHjsXGjRsBeL9IM2lSBKEAvV6P9PR0/PbbbwB4s9mECRPw73//G2azGRs2bMBPP/2Ejz76CGVlZWjbti3+93//F+3btwfAl8l65513UFxcjIyMDDAMY733yZMnsW7dOrzzzjsA+PI4W7duxenTp8FxHO699148+OCDeP/992EymTB27FhotVps3boVjY2N+PDDD/H999/DZDKhb9++GDduHEJDQwEAn376KT777DMwDINRo0bJXm9UVBT69++P//73vwB4rfIPf/gDvv32WxQVFWH79u2YMmUKnnvuOfTu3Rssy2LPnj34+uuvce3aNbRu3RqzZ89GYmIirl69is2bN+P8+fOIiYnBqFGjMGDAAK98LoR/qa+vx5kzZ+y0aEt1CdsizUajEa+88grefPNNREZGujUWCSmCUEB5eTny8/Ptao7l5eVh+fLlCA0NxYULF/D222/jpZdeQufOnXHw4EG8/vrrWL16NRiGwRtvvIE//vGPeOihh3DkyBGsWbMGI0aMcBqHZVn84x//QM+ePbFhwwZoNBqcP3/eKvQczX07duzA77//jjfeeANarRZr1qzB7t278Ze//AUFBQX417/+hQULFiA5ORnvvvuu7PVWV1fj8OHDdp1hv/vuO7z88suIiYmBVqu1O/+zzz7Dd999hzlz5qB169a4dOkSwsLCUF9fj6VLl+Kpp57C3LlzcfnyZSxduhTt2rVD27ZtFXwChBpgWRaRkZF44403nF7zdpFmMvcRhAzeeOMNjBs3DgsXLkRaWhpGjhxpfe3xxx9HVFQUQkNDsW/fPgwbNgxdu3aFRqPB4MGDodPpcO7cOZw9exZmsxmPPPIIdDod7r77btFac4WFhTAajRg7dizCw8MRGhqK7t27C57LcRy++uorPPvss4iKikJERARGjhyJ7777DgAfaTV48GC0a9cO4eHhePLJJ12ud8uWLRg3bhxmz56N+Ph4PPvss9bXHn74YSQmJlq1NFu++uorPP3000hNTQXDMOjQoQOio6Nx9OhRJCUlYciQIdBqtejYsSP69++P77//3uVcCPXRqlUrJCcnWz8/juNw8eJFAN4v0kyaFEHIYPbs2ejdu7fga7YNB8vLy3HgwAF8/vnn1mMmkwlGoxEMw0Cv19uZ+Gyb9tlSXl6OpKQkJ01FiOrqaty8eRMvv/yy9RjHcdZ+UZWVlejUqZP1taSkJJf3HD9+PB544AHB18TmDPDtMoQ2pLKyMpw7d84u0stsNmPgwIEu50IEHtvisc8//zyeeuopTJkyBe+//z4+/vhjmEwm3HvvvejQoQP69OmDY8eOYfr06dBoNPjrX//qUbsRElIE4SG2QichIQEjR46007QsnDp1CkajERzHWa+pqKiAwWBwOjcxMRHl5eUwm80uBVV0dDRCQ0OxcuVKwX5K8fHxdv2mbHsweZuEhAT8/vvvaNeundPxtLQ0LFiwwGdjE75j2rRpgsfnzZvndIxhGDz77LN22rcnkLmPILzIAw88gP/+9784d+4cOI5DfX09jh49irq6OnTr1g0ajQb/+c9/YDKZcPjwYRQWFgrep0uXLoiPj8eOHTtQX1+PhoYG/PLLLwCAuLg4GI1Gq6Nao9HggQcewNatW3Ht2jUAfKtuS1TVPffcg/379+PKlSu4efMmdu3a5dP179y5E8XFxeA4DpcuXUJNTQ3uuusuFBcX4+DBgzCZTDCZTCgsLMSVK1d8NheieUCaFEF4kc6dO+O5557D5s2bUVxcbPUl9ejRAzqdDrNmzcK7776Ljz76CBkZGaJN3zQaDV566SVs3rwZ2dnZYBgG9957L7p374477rjDGkCh0WiwadMmjBkzBrt378a8efNQU1MDvV6PP/zhD0hPT0dGRgYeeeQRvPrqq9BoNBg1ahS+/fZbn6z/T3/6ExobG7F06VLU1NSgTZs2mDVrFqKjozF//nxs27YN27ZtA8dxaN++vdeetonmC/WTIgiCIFQLmfsIgiAI1UJCiiAIglAtJKQIgiAI1UJCiiAIglAtJKQIgiAI1UJCiiAIglAtJKQIgiAI1UJCiiAIglAt/x+YRQiFMXbP2AAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#idk residual plot is vs predicted value or actual value lol\n",
        "plt.scatter(y_train_pred, train_residuals)\n",
        "plt.xlabel(\"Predicted Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "LkhIV_-mMI5N",
        "outputId": "a29dfcf8-890f-4dc1-e2de-945947341fe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_train, train_residuals)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "Kn7jQRh7MI5O",
        "outputId": "edda08da-d720-47ce-f450-d505d1c933ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Actual vs Predicted Price')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABGYElEQVR4nO3deVxU9foH8M+ZGfZ9BgRxF1Ejt5RrN7XUIpcWbVMrNzRMMxXNNhXDNBNzT7FcSM363Ut2u2m37HpRU1NTc1dccMuFQZYBRTZhzvf3x8CRgTMzZ2B2nvfr5avmnDNnnoN4njnf5flyjDEGQgghBIDM3gEQQghxHJQUCCGECCgpEEIIEVBSIIQQIqCkQAghREBJgRBCiICSAnFosbGxiImJsXcYVvXbb7+B4zjcvHlT9LWtWfNn3qdPH8TFxVnl3MQyKCkQ3Lp1Cx4eHggPD0dFRYVZ7/3999/BcRyuXbtmneDs4Nq1a+A4TvgTEBCARx99FFu3brXJ5/fo0QNqtRrh4eGSjv/mm2/AcZyVo9JX/efj4+ODzp07IyUlxeT7fvjhByxdutQGEZK6oqRAkJKSgueeew6BgYH46aef7B2Ow9i6dSvUajX++OMPPPTQQ3j55Zfxxx9/iB57//59i32uu7s7wsLCIJM59j/PVatWQa1W48SJExg4cCDi4uKwZcsW0WOrfj5KpRL+/v62DJOYybF/64jV8TyPlJQUxMbGYvTo0Vi7dm2tY7KzszFmzBiEhobC09MT7dq1w1dffYVr167h8ccfBwC0atUKHMehT58+AMSbIGp+o7169SpeeuklhIeHw9vbGx07dsTmzZvNin/48OHo169fre0DBw7EiBEjAAA3b97Eyy+/jODgYHh6eqJ169ZYtGiRyXMrlUqEhYXhoYcewrp16+Du7i48LbRs2RIJCQmYOHEiVCqV8HM4evQo+vXrB19fX4SEhOCll17CX3/9pXfelStXomnTpvD29kb//v1x/fp1vf1izUeXL1/GK6+8AqVSCW9vb3Tq1An/+c9/8Ntvv2HkyJEAHnx7j42N1fus9u3bw9PTE5GRkZg/f77e06BGo8GwYcPg4+OD0NBQJCQkQGqRg4CAAISFhSEyMhJJSUlo06YNfvjhBwC6ZqI33ngDs2fPRuPGjdG8eXNhe83mo+TkZERFRcHDwwONGjXCyy+/LOwrLy/HnDlz0KpVK3h6euLhhx/GmjVrJMVH6kZh7wCIfW3fvh1lZWUYOHAgunXrhtmzZ+PatWto2bIlAKCkpAS9e/eGl5cXvv32W7Ru3RqXLl2CRqNBs2bNsHXrVgwePBiHDx9Gs2bN4O7uLvmz7927hyeffBKJiYnw9fXFL7/8gjFjxqBp06bo27evpHOMHj0aAwcORGZmptDcolar8b///Q+//PILAGDixIkoLi5GWloaAgMDcfXqVWRlZZn1c1IoFHBzc0N5ebmw7fPPP8c777yDgwcPoqKiAunp6ejduzemT5+Ozz//HOXl5Zg7dy6efvppnDp1Cp6enti6dSumTZuGzz77DM899xz27duH9957z+hnZ2VloUePHujYsSO2bduGxo0b48yZM5DJZOjRowdWrVqFSZMmQa1WAwC8vLwAAHPmzMGGDRuwfPlydOnSBefOncOECRNQWlqKefPmAQDeeOMNnD59Gj/99BNCQ0OxYMECbNu2Dd27dzfr51P1udV/Pt999x2GDx+OnTt3QqvVir4nMTERS5YsQVJSEvr164d79+5h+/btwv5x48bh2LFjWLNmDSIjI3H48GGMHz8eCoUCb7zxhtkxEgkYadAGDRrE3nnnHeF1//792axZs4TX69evZx4eHuzGjRui79+3bx8DwK5evaq3ffTo0eypp57S27Z582Zm6ldu0KBBLC4uzuh5qtNqtSw8PJx99tlnwrZFixaxJk2aMK1WyxhjrFOnTiwxMdHo51Z39epVBoDt27ePMcZYSUkJS0xMZADY9u3bGWOMtWjRgj355JN67xs9ejQbNmyY3rbS0lLm5eXF/v3vfzPGGOvZsyd7/fXX9Y6ZPn06AyD8jHfv3q33OiEhgYWGhrJ79+6Jxiv2cy0qKmJeXl5CvFU2bdrEAgICGGOMZWRkMABsx44dwv6ysjIWHh5u9GfOGGMA2ObNmxljjJWXl7N169YxAOyLL75gjDHWu3dvFhkZKfwdVOnduzd74403GGOM3bt3j3l6erJFixaJfsaVK1cYx3Hs3Llzets//vhj1rlzZ6Pxkbpz+ieF1atX49ixYwgICMCSJUtMHn/gwAFs2bIFHMehRYsWiI+Pt0GUjunWrVv4+eefcfz4cWHb6NGjMX36dMyZMwcKhQJHjx5FVFQUmjZtavHPLy4uxty5c/HTTz9BrVbj/v37KCsrk/yUAAAymQwjRozA5s2bhW/cmzdvxvDhw4U2+alTp2L8+PHYvn07+vTpg2effRZPPPGEyXP369cPMpkMJSUlCAoKwrJlyzBgwABhf81v00eOHMGlS5fg6+urt720tBQZGRkAgPT0dLz22mt6+3v16mX0d/fo0aPo0aMHfHx8TMZc5ezZsygpKcHLL7+s12Sn1WpRWlqKnJwcpKenA9B1bFdxd3fH3/72N9y7d8/kZ8TFxQlPHl5eXvjwww8xfvx4YX+3bt2M9oucPXsWpaWlos1/APDnn3+CMYbo6Gi97RUVFZDL5SbjI3Xj9EmhT58+GDBgAJKTk00eq1ar8eOPP2LevHnw9fXFnTt3bBCh40pJSYFWq8Ujjzyit12r1eKnn37Ciy++WOdzy2SyWm3T1ZsWAOC9997D1q1bsXTpUrRr1w4+Pj6YPn262X8vo0aNwmeffYYTJ04AAE6dOoV//OMfwv4xY8ZgwIAB+PXXX7F7924MHDgQL774Ir755huj592wYQO6deuGwMBABAcH19pf8ybN8zxGjhyJDz/8sNaxKpXKrGuqL57nAQBbtmxB27Zta+1XKpX1/oz58+dj8ODB8PX1RWhoaK0RUOYkMTFV13DgwAF4e3vr7bP1aKuGxOmTQlRUFLKzs/W2ZWVlISUlBXfv3oWHhwfGjx+PJk2aYOfOnejfv7/wTS4gIMAeITuEqg7mmTNn1vrm+umnn2Lt2rV48cUX0a1bN3z11Ve4efOm6NNCVR9CzTbjRo0a4eDBg3rbjh07pvd67969GD58OIYOHSrEdPHiRYSGhpp1LQ8//DC6deuGzZs3gzGGbt26ISoqSu+Yxo0bY8yYMRgzZgyeeeYZvPbaa1i9erXRkTBNmjRBmzZtJMcRHR2NU6dOISIiwuBNKyoqCgcOHMDbb78tbNu/f7/R83br1g3r1q1DUVGR6I22+t9B1Tfohx9+GJ6enrhy5QqeeeYZg7EAupvu008/DUA3SujIkSN46KGHTFwtEBoaatbPR+zzPT09sWPHDnTq1KnW/m7dugEArl+/jueee67On0PM45Kjj9auXYuxY8di4cKFGDlyJNavXw8AyMzMhFqtxuzZszFr1izhm2VDtH37dty4cQPjx49Hhw4d9P7ExsZix44duHbtGl577TW0aNECgwYNQlpaGq5evYqdO3ciNTUVANCiRQvIZDL88ssvyM7OFr7lx8TE4Pz580hOTsbly5exbt06fPfdd3oxtGvXDlu3bsXhw4eRnp6ON998E5mZmXW6nlGjRuH//u//8I9//AOjR4/W2zdp0iT88ssvuHz5Ms6ePYsffvgBzZo1g5+fX50+y5CZM2fi3LlzGDFiBA4fPoyrV69i9+7diI+Px5UrVwAA06dPR2pqKlasWIGMjAxs2LDB5IiriRMngud5DB48GPv378fVq1fxn//8R+iQbdWqFQBg27ZtyMnJwb179+Dr64uZM2di5syZSE5OxoULF3D27Fn885//xAcffAAAaNOmDQYNGoS3334bu3fvRnp6OuLi4lBYWGjRn4shvr6+QlNlcnIyLl68iJMnT2LBggVCfGPHjsW4ceOwefNmXLp0CSdPnsRXX32FhQsX2iTGBsnOfRoWcfv2baGztKSkhL3++uvs3XffFf5MnTqVMcbYggUL2GeffcbKy8vZ7du32YQJEwx23rm6QYMGsb///e+i+8rLy1lwcLDQ4axWq9nIkSOZSqViHh4erF27dmzDhg3C8QsXLmTh4eFMJpOx3r17C9s/+eQTFh4eznx8fNirr77KVq1apdchev36ddavXz/m7e3NwsLC2EcffcTGjh2rdw5THc1VcnJymJubG3Nzc2M5OTl6+yZOnMgiIyOZp6cnUyqV7JlnnmFnzpwxeK6aHc1iWrRowebNm1dr+6lTp9igQYNYYGAg8/T0ZBEREWzcuHEsLy9POGb58uUsPDyceXp6sqeeeopt3LjRaEczY4xduHCBvfDCC8zf3595eXmxTp06sZ9//lnYHx8fz0JCQhgANnr0aGH7unXrWOfOnZmHhwcLDAxk3bt3Z6tXrxb25+bmsiFDhjBvb28WHBzMPvzwQzZq1CizOprFVO9QNrad53m2fPly1rZtW+bm5sYaNWrEXnnlFWF/RUUFW7hwIWvXrh1zc3NjKpWKPfHEE+y7774zGh+pO44x5195LTs7GwsXLsSSJUtQXFyMqVOnio63X7t2LSIjI4WOzLlz5+L111+v1yMwIYS4EpdrPvL29tZrz2aMCSUYunfvjrNnzwIA7t69C7VabXb7NSGEuDKnf1JYvnw50tPTUVhYiICAAAwdOhQdOnTAunXrUFBQgIqKCvTs2ROvvPIKGGP4+uuvceLECchkMrz00kvo2bOnvS+BEEIchtMnBUIIIZZjk+aj1atXIy4uDtOnTzd63KVLl/Dqq68aLDpGCCHEumySFPr06YOZM2caPYbneXz77bfo3LmzLUIihBAiwiaT18QmmNW0fft2PProo7h8+bJZ5zY0rj04OBi5ublmnctZ0LU5J7o25+Oq12VsrQ6HmNGs0Whw+PBhJCYm4osvvjB6bFpaGtLS0gAASUlJouUHAF1VS0P7nB1dm3Oia3M+rnpdxjhEUti4caNeATNjYmJi9Or0G8rirprhAbo2Z0XX5nxc9boc/knh8uXLWLFiBQDd/IHjx49DJpPVqaY7IYSQunOIpFC9wmlycjK6detGCYEQQuzAJkmh+gSzCRMmYOjQocKSgIZqqRNCCLE9mySFqVOnSj62eklhQggh+vicLGDrt2AFGnCBSmDwcMhCwix2fodoPiKEEGIan5MFtuwjIEe3xjgDgCsXwE+ba7HE4HIF8QghxBXxOVlgSxKEhCCofHKwFEoKhBDi4IQnhDzxScCsQGOxz6KkQAghjm7rt7WfEKrz9LLYR1FSIIQQB2fJJwFTKCkQQoiD4wKVxg8oLbHYZ1FSIIQQRzd4OGBkdJHJpGEGSgqEEOLgZCFh4KbNBTp3B9zc9HeGhOmShoXQPAVCCHECspAwYFICTV4jhBBXVNebuywkDIgzvoplfVBSIIQQG7PFzOS6oj4FQgixNbF5BxaemVxXlBQIIcTGWLbarO22RM1HhBBiI1X9CLhxVfyAuwU2jUcMJQVCCLGCmh3JrFc/4OuVxstVBATZLkADKCkQQoiFiXYknzgElJUafR9n505mgPoUCCHE8sQ6kk0kBEtPQqsrelIghBALM6uAnV8AuKguFp+EVleUFAghxMK4QKWuyagmD0/9J4bK8hWOkAyqUFIghBBLGzwcuHJBvwkpJAwYNRnc7zusVqLCEigpEEKIhclCwsBPmytexqJ9R3uHZxQlBUIIsQJr1yiyFhp9RAghREBJgRBCiICajwghpIaq2ciaokLwPn5W6RC29roIdUVJgRBCquFzssAWzQTyc1FetfHiWfDvfWqxm7Yjl862SVJYvXo1jh07hoCAACxZsqTW/n379mHr1q1gjMHLywtxcXFo2bKlLUIjhDgJW32zZqnrgfxc/Y35ubrtkxIsE4+x0tl27py2SVLo06cPBgwYgOTkZNH9jRo1wpw5c+Dr64vjx49j7dq1+PTTT20RGiHECVj6m7XRG/qVC+Jvqra9vvEYmvFs1kxoK7FJR3NUVBR8fX0N7m/Xrp2wPzIyEnl5ebYIixDiLCy4KE3VDZ0d2gNcOA12aA/Yso90icJG8XCBSrO225LD9Sns2rULjzzyiMH9aWlpSEtLAwAkJSUhODhY9DiFQmFwn7Oja3NOdG11pykqfNC+X/1ziwqhNPNz72xehVKRG7rHr98jYNoc5LfvhPtH9tV6n3v7Tgiq/Kz6xlMROxkF1y5Be/uWsE0e2gSBsZOhsPPviEMlhTNnzmD37t2YO3euwWNiYmIQExMjvM7NzRU9Ljg42OA+Z0fX5pzo2uqO9/ET3V7h4yf5c4Umo1N/iu4vva1GeW4u+BdHAhfPAneqNeUEKFH+4kjhs+odj8IdfHwiuGpNWPzg4ShQuAM2+B0JDw83HJrVP12iv/76C2vWrMGMGTPg5yf+AyeENFCGaglJLDVdsw9AlKcX+PVLdEtiFt/T3yer0dJez3gAx53x7BBJITc3F4sXL8akSZOMZjBCSMNktJaQFGJ9ANUpQ4DrV8Bqjjqqkp+rNzKo3vE4MJskheXLlyM9PR2FhYWYMGEChg4dioqKCgBAv3798P333+PevXtYv349AEAulyMpKckWoRFCnITYN2spw0L5nCyw9BPiJ/XyAdcpGqy0BDh52Ojn1xwZ5Kjf9OvLJklh6tSpRvdPmDABEyZMsEUohBAXIWVYqHBM4R3Rc3CdoiGLmw7t4lkmP88RRgbZAtU+IoTYFJ+TBX79EmgXzwK/fol5Q0GrkzIs1FizUbU+AJM3fA9PsOys+sXrJByiT4EQ0jBYchKalAlgBieD+QXor3gm1nHs4QlZ42bg1Td0q6VdvQB29YLDlKOwFnpSIITYjgUnocHTy+R2g5PEorro3dRllctico/2Btp1BPdob3CJn8O9eSv95TPrE6+ToCcFQojN2Ly8gxlDR6t3HFd1YJeePip6WkcoR2EtlBQIISZZqhidoQXtpXTi1orhrnjnMUpLhP+ty9BRKXMaXLnTmZICIcQoixajq+OkL9EYPDxFj615wzZ76KipOQ1mTlJzNpQUCCHGWbDMc50nfYnFUFaqSwzV2/wtcMM22DRUOafBVSapGUJJgRBilKX7Aeoy6Ytlq8V3hDcH16ixRWcVG2ziqpzT4OooKRBCjKpPP0B98TlZusVtrl4U3c81amz5G7UF6ho5M0oKhBDj7HSTNNnh6+ZulRiqN3EpigpRYaU1mh0VJQVCiFF2K/5mqsPXw8NqMVQ1cSlduNy5IZQUCCEm2aP4G8s2UU6Co7m31kBJgRDiUKrmI+DGFeMHNmlpk3gaGkoKhBCrMXfSm6TFcKrcvgU+J6vBtPXbCiUFQohVGJr0ph01GdzvO/QSBQBd8kg/YbDMdS01Fr4hlkFJgRCiR3v+NLBxBVBcBHj7ALHxkLfvaP6JDE16WzkX7H4ZgMpEcf60brlLQ6ueGeHKNYjshXpqCCEC7fnTwLKPgLxsoKRI999lH+m2m8nghLPKhCC4ozGdEBRuoptduQaRvVBSIIQ8sHEFwGv1t/Fa3XZz3S2wSEiQyYHYKbq5EdU1oAlltkTNR4S4MLOrmxYXmbfdGP8g3ZNGffFacKf/BOwxV6IBoqRAiIuqU3VTbx9ds5HYdjNxjcJ0K5VJO7oqQlGsQAO5HeZKNESUFAhxVXWpbhobr+tTqN6EJJPrttdQ/Skk3z8A2vv3gdISXa2kXv106xoo3ICK8gdvcveo3acAAO07gQsINDj6iPoObIeSAiEuqi7VTeXtO0I7ba7J0Uc1n0LuVz8/ABzeBzC+2onlgF8g4OsPZGfqJwZlCLhRb+vKaYjNU6C+A5uipECIi6prdVN5+45A0nrjJzdVl6h6QgAArRYoyNP9AXTrIDRpAa7yhl/VnGW3OktEQEmBEFdVj+qmpjqo6z0/oKwUXEiYaNlrsbWStZQgbEZyUrh16xYOHjyIgoICxMXF4datW6ioqECLFi2sGR8hpI7q+q1btIP6xCFoJ80WmpEMPYWYw1RisegyoEQySfMUDh48iMTERGg0Guzbtw8AUFpaiq+//tqqwRFC6kdW+W1c/u58yOKmS7uZGlr6ctU83Td3QPe0YYEVzsyOo6qjnFiNpCeF7777DgkJCWjZsiUOHjwIAGjRogWuXbsm6UNWr16NY8eOISAgAEuWLKm1nzGGDRs24Pjx4/Dw8MDEiRPRunVr6VdBCLEYg9/gy0r1Ry75B0orXBeg1HU0a3IebJPQjGXpZUAtwex5H05IUlK4c+dOrWYijuPAcZykD+nTpw8GDBiA5ORk0f3Hjx9HVlYWPv/8c2RkZGD9+vX49NNPJZ2bEGJZxpqGWIFGV/Ji5VzxoaViwpqAGz3Z7JupPZcBFdNQmrMkNR+1bt0ae/fu1du2f/9+tGnTRtKHREVFwdfX1+D+P//8E0888QQ4jkPbtm1RVFSE/Px8SecmhFjY4OG60UFibmcCSxOkJwTobuJ1asYSa6Ky5/DUBtKcJelJYcyYMfjkk0+wa9culJWVYf78+cjMzERCQoJFgtBoNAgODhZeq1QqaDQaBAUF1To2LS0NaWlpAICkpCS991WnUCgM7nN2dG3OydGurSIrE0X/WAutJheclw8ABlZSDDdlMBRvzcC9LxbomoyqyOQPhpRKJA9tgsDYyVDU5bqDg1Exd5UQo1wZDJ/X3oQiLNz8c9VR9b8zTVEhysWOKSqE0oH+XutLUlJo0qQJli9fjqNHj6Jbt25QqVTo1q0bPD0NfJuwopiYGMTExAivDa2fGuzCa6vStTknR7o2Y4vZlAPA72kAX2OuQc1CeWI4GdCug+5J4k4+tN4+0GxcWfe2d4U7MHKS7uMBFACADX+G1f/OeB8/0WMqfPwc5u9VqvBww4lVUlLQaDRwd3dHjx49hG337t2DRqOBUln/9j2lUqn3Q83Ly7PIeQlpCOrU+Wlq8lnNhCCFmzswJRGcKkSXcPKygbxssKsXXaPtvR7zPpyJpD6FRYsWQaPR7/HXaDRYvHixRYKIjo7G3r17wRjDxYsX4e3tLdp0RAjRV/WNnx3aA1w4DXZoD9iyjx4MHTXA4iN4/IPAfbxKN4/BRdveZSFh4KbNBfdob6BdR3CP9gbn7IlOhKQnhczMTDRv3lxvW/PmzXHr1i1JH7J8+XKkp6ejsLAQEyZMwNChQ1FRUQEA6NevHx555BEcO3YMU6ZMgbu7OyZOnGjmZRDSQNWl6B0AeHpZNAy35q3AV94cHXEoqaVUn23tqiQlBX9/f2RlZSEs7EFGzMrKgp+feBtbTVOnTjW6n+M4xMXFSToXIeQBc27Aestslt8XeVfdlV84A7wzEmjdzmDCoUqnzkFSUujbty+WLFmCV199FaGhocjKykJqaiqefPJJa8dHCDFC6lh+7aE9wPraE0ctpvy+7s/Jw7rJakHB+ktsumDbu6uSlBReeOEFKBQKbN68GXl5eVCpVHjyySfx3HPPWTs+Qkg1NTuVWa9+Jjs/+ZwsIGWZ7YK8owE6dwfX9mGXnvnrqiQlBZlMhkGDBmHQoEHWjocQUoOQCLLVQOZ1Ye5A1YxajJoM7vcdojdgPicLbElC7VLWliCX60piiyktgWySZeYxEdsymBTS09MRFRUFADhz5ozBE3To0MHyURFCABifTwAAyMkC9/sOoQR19VLT4Dhd0jBj9rFZvHyAe3dFdxnrP2gI9YOcmcGkkJKSIhSv++KLL0SP4TgOq1atsk5khDRgwo3TwPKU1bFstfAetmimflu+tQQFA6FNgPMna+9z9zDYf9BQ6gc5M4NJoXo105UrV0ImkzSlgRBSqeY34orYyboZuhLeZ/TpoKb8PPDrl4CdPAKUFtczahP8AsBFdQEGDwfbtFL8mCYtDN/gTQyhtcZTBD2ZmMdknwLP8xg5ciQ2btwINzc3W8REiNMT+0ZccO0S+PjE+s82rqkgTzd5zRbCmz9oqjI08qlRY4NvNzaE1hpPEfRkYj6TX/9lMhnCw8NRWFhoi3gIcQ0iN3bt7VuSZvU68iQvLlAJPicL2lWfgJ05puu3qK5y5BOfkwV+/RJoF88Cv36JMMPaUF8DF6i0zkxoF51dbU2SRh/16tULCxcuxMCBA6FSqfTWUaCOZkJqMzWpzFiThiWWurSKkDDdEFixfguFG/DwI+CG6SahGvp2bqx+kKHmKHOTZPWfLTKvW+ScDYmkpLBjxw4AwJYtW/S2U0czIeKMTSoz2aQhduOUyaVVKbUWVSNwVes9i3VkV5SD8/TSrQu9fonBb+eyuOkG14022Bxlxkxoqf0xNLvaMElJwdCKaYQQA0Ru7PLQJuAHDwdLXS9602RLEqCNjQf3+w7A119XqTQgCFxIGFjHaGBzsv76BrYUHApZSJhuqKsBVd++TT0lGawfZIkqpFL6Y2h2tVFGk0JBQQE2bdqEGzduoFWrVhg1apTkekeENGSykLAH34hzsoA7+YB/oC4hnD0m/qa8bGDZR2DVnwgYA/MLAFLX2y8h4ME3a2NNW6aOMfXtXO9nVseRQgabhfwCgPDmNPpIAqNJYf369SguLsbTTz+NQ4cOYePGjZg8ebKtYiPEoZka6igLCQM/eDhQubaANi9bwklrNBFpcvQXvLeFmk1V1b9ZDx4OXDxbq09BFhwKVv2YOn7jr28VUoMJKaqLMGqKGGc0KZw/fx4rVqyAj48PHnvsMXzwwQe2iosQhyZ5qKO5w0vtwccPaPMQUFoi1FMyVDZDFhIG/r1PdU88Vy7o3t+6HYImvI+CyjkYlvjGX2cNZCEcazKaFMrLy+Hj4wNAVz67tNR+j6+EOBSJ6xg41CgXjgOYyPfoNg9BXrNOUfuOBk8jCwkDahyvCA7WWybTXusO2DUhuQijSaGiogK7d+8Gq/xFqqiowK5du/SOofLZpCGSuo6BQwwv9fED16Er2J184Pwpe0djdQ1hIRxrMpoUIiMjsXfvXuF1mzZtsG/fPr1jKCkQR2XN8gaSO1PFmjP8A3Xt9mWlujpBwY2AbDVQdM/yw06VIeDena8bObR4lvgxpSWW/Uzi1IwmhTlz5tgoDELMY+qGb/XyBkbarmvFVlnaWlFUiHK5Arhx9UHncUmRbv0Ba3D3AMZMNTkpTsqYfaof1HBImqdAiCORdMOv69rFEhlquwbEZ/OyUZMhP7IH5ccPmax6ajH3y4CvlkHbvLXuaYCTAeCqoqq8ELlulrIRVD+oYaGkQJyPhBu+LRaPr9l2LSxoU3PoaU4WsGoeSu0xzyA/13gpbV6rmyxnpGPZ2gmWOBZKCsTpSLnh16epxBSxphSg8gnB0FwEO048M8VUorRFgiWOg5ICcTqSbvhWGq9uqCkF4c0dfz6CAaYSpTUTLHE8BpPC7du3JZ0gNDTUYsEQIomEG76lx6sbXQktJ8t5R/BISZQ0IaxBMZgUpkyZIukEqampFguGECmk3vDrOl69ZvMQ69UP+Hql4z8JeHiabqZShgDNWgmzl6UkSpoQ1rAYTArVb/a7d+/G6dOnMWTIEISEhCAnJwfff/89OnY00jlFiBVZY4ISn5OlK9+QfhwoLwdQ2Tx04pDpm22TlsDlc0D5fYvGZBwH+PnrRhkxprvha3IMx+rhCYyZCnllp3JV8tNKuNHThLCGQ1KfQmpqKj7//HO4u+tqmzRu3Bhvvvkm4uPj0adPH2vGR4hNGK3DL+Xbd+ZfNk4IAMD0m7LUN3T/9fAE5Aqg+J7+4WWlwkgjGmZKDJGUFBhjyM7ORtOmTYVtOTk54Hle8gedOHECGzZsAM/zeOqpp/DCCy/o7c/NzUVycjKKiorA8zxef/11dO3aVfL5CakLo30FUnCcLhnYau6BFGWlulLRIoQRQzTMlBggKSk8++yzmDt3Lvr06YPg4GDk5uZiz549ePbZZyV9CM/zSElJQUJCAlQqFWbMmIHo6Gi9JPOvf/0Ljz32GPr164ebN29iwYIFlBSIVUldpcsoxhwrIZhQNWKIhpkSQyQlhUGDBqF58+Y4ePAgrl27hsDAQLz11lvo0qWLpA+5dOkSwsLChJFKPXr0wJEjR/SSAsdxKC4uBgAUFxcjKCjIzEshxEzOUNa6rlq3061PbGDEEA0zJYZInqfQpUsXyUmgJo1GA5VKJbxWqVTIyMjQO2bIkCH45JNP8Ouvv6KsrAyzZ88WPVdaWhrS0tIAAElJSQgODhY9TqFQGNzn7OjapKvIykTRP9ZCq8mFXBkMn9fehCIsHACQq8mFHVc9NszDE1AodAXy6oDz9IJywvsAYPDaK2Ino+DaJWhv3xLeJw9tgsDYyboy2GZy1d9JV70uYyQlhfLycnz//ffYv38/CgsLsWnTJpw8eRJqtRoDBgywSCD79+9Hnz598Pzzz+PixYtYuXIllixZAplMpndcTEwMYmJihNe5ueJT+KuauVwRXVttRmcZV35bLgdQeu6UbgF6AOz6ZYvFbVHtO4EbFle7acvdA3BzB2Qy3ZOAJkdXXK8GFt5cWPAGIycBAHgABcCDNQ8U7uDjE8FV+5nxg4fr3leHn7+r/k666nWFh4cb3CcpKWzatAkajQZTpkzBp59+CgBo1qwZNm3aJCkpKJVK5OXlCa/z8vKgVOo/pu7atQszZ84EALRt2xbl5eUoLCxEQIB4hxkhVcyaZVw17PTmNcctPVG5ohlnYm4Av34JmEhSgF+Abp+JoaY0zJSIkZQUDh8+jM8//xyenp7gOA6A7kav0UjrlIqIiIBarUZ2djaUSiUOHDhQa3JccHAwzpw5gz59+uDmzZsoLy+Hv7+/mZdDGiRDI2kMzTKuNg/BIRXeAVv2Ebhpc42vKyw201gZAly/AlZZBI+GmhJzSUoKCoWi1vDTu3fvws/PT9KHyOVyjB07FvPnzwfP8+jbty+aNWuG1NRUREREIDo6GqNGjcKaNWvw888/AwAmTpwoJCBCjNXzN3vEjMmEUKO8tD1IGB4qNtOYlZYAJw+bfS5CqkhKCn//+9+xatUqxMbGAgDy8/OxceNG9OjRQ/IHde3atdYQ02HDhgn/37RpU8ybN0/y+UjDYWqilcElL8VG4Ei64dt9AU0A0pJdzSYgQ6ur0VBTIpXM9CHA66+/jkaNGmH69OkoLi7GlClTEBQUhFdeecXa8RFifKIVoGtGqdk0EhIGblgcMGoyEKDULX8JwFFu+FLUZXiooffQUFMileTmo9jYWMTGxgrNRtS0Q2xF0kSr8OYP+hBat9MlBAD4apn1lru0JgkroomiiqakniQ9KYwZM0b4f39/fyEhxMXFWScqQqox9u1XaFo6eVg3s7jwDnDjKljqerAF7xlfdcyRVa2IZiZZSBi4aXPBPdobaNcR3KO9dR3W1MlMJJL0pKDV1p7iU1FRYVbtI0LqTOzbr4cnWK9+4MSaljQ5uj9Orq79ADTUlNSH0aTw0UcfgeM4lJeXIzExUW9fXl4e2rZta9XgCAF0NzntqMnAqnkP5haUlQJfrwRTuNk3OHNwMqBVWyA7EygpAnheVzvJ0OHUD0DswGhSePLJJwHoahf17dtX2M5xHAICAtChQwfrRkdIJe73HWA1J5s5Vd0iDr7xiSh5+BFhi9GCfNQPQOzEaFKoWishMjISTZo0sUU8xMkZm09QHyxbbYHo7Imh4th+oFpS0JtnkJMF3MkH/APBNWpMK5sRu5HUp/Df//4XPXv2RLt27YRtFy5cwMGDB4W5C4RYeuEWIcFkZwF/XbJssHag1dTu9Kb2f+JoJI0+2r9/PyIiIvS2tW7dGr///rtVgiJOytR8AjNUJRh2aA9w9YKu/b0mmaRfX4dRnn4S2g/joD1/2t6hEGKQpH9VHMfVGmnE8zyYkU4y0vBYdOEWKWsdNGlRe9KaI+O1QF42sOwjSgzEYUlKCu3bt8c///lPITHwPI8tW7agffv2Vg2OOBeDo2Uyr4Nfv0TXHCSRlETChTfXzVhWNQI8vHSlpe3B3ImcvBbYuMI6sRBST5L6FMaMGYOkpCSMHz9eqC8eFBSEDz74wNrxEWciNp8A0FX9PLTHrP4Fg/WMqmHH/wCO7BNvWrKlTn+rXWPJzc144b3iIuvHRUgdSEoKKpUKCxcuxKVLl5CXlweVSoU2bdrUWgCHNGx6o2nST9Reu7hGtc6aI5VYr364c2QPtLfVgKeXrgy0sUlo98usdzFSySv/CY2arBs2a6xaaXXePraJjxAzSV6OUyaT0WQ1YlLVaBrtp++KLmhfNbRUdKTSkd9RylebPR8UDHTuDlw6BxQV2iD6OtBW6G7+mdeBaXMhr3wK4nOywGpVaK0kkwOx8TYOlBBpDCaFadOmYdmyZQCAt956y+AJvvjiC8tHRZzf3QLx7fl5ulXBxJ4k+BrlVPJzgeatdbN/HV2NpyC9p6Zb13WzmOVywNsXiI2HvH1HOwdMiDiDSWH8+PHC/0+ePNkmwRAX4h+kG2lT073K/gWpTh0xWgrCkdTsHK85B8FV1/slrsVgUqg+sigqKsomwRDXwTUKA7t6ofaOigrzTuQkCQF4ULXVGjO6CbEVg0khNTVV0gmqr55GiEBsJJKpETnOLCRMt/6BBWd0E2IPBocP5eXlCX/UajV+/PFHnDlzBllZWThz5gx+/PFHqNXOXo+GWIssJEw3hyBQpetYVbgBnt4GjnbgBZtkcsAvwPhcBFUj3RoGv++w2IxuQuzF4JPCxIkThf9fvnw54uPj8fe//13YdujQIRw8eNC60RGnxedk6VY9K8ir3KDVdSzLZPrzCkLCAE2ubhSPI+K14KK66EZNXb1Ye7+bG7jpn+jKe1tyRjchdiJposHx48fRvXt3vW3R0dE4fvy4VYIizovPydKNLjK06hnP654a/AKApq2BAo3jJoRKrECjq1wqJuoRoWmI1kcmrkBSUggLC8Ovv/6qt23Hjh0IC6N2UvKAXhE7kTkKgopy3f6bV4Dy+7YLsI6qOoxr1VkKCXuwFjRg8BhaF4E4E0mT1yZMmIDFixdj27ZtUCqV0Gg0kMvlmD6dSv6SaqQUsXN0HKc/4qnypq4378DAyCIpxxDi6CQlhVatWmHFihXIyMhAfn4+AgMD0bZtWygUkidEkwbAJdrOO/0NnKeX6E1dytoHtD4CcXZ1uqtHRUWhtLQUFRUV8PT0tHRMxEnUGpPv6SV+YM1v346qsjmIvtmThkxSUrh+/ToWLlwINzc35OXloUePHkhPT8eePXswbdo0a8dIHBCfkwW2eJZQsI4BQIBSN5P5bv6DA5UhupLWWTftEqckCjfg4UcoIRACiUlh3bp1GDZsGJ544gmMGTMGgO5pYc2aNZI/6MSJE9iwYQN4nsdTTz2FF154odYxBw4cwJYtW8BxHFq0aIH4eCoa5qhY6vraFUzvaB5UDRW25Tv86CJUlIPz9KKEQAgkJoWbN2/i8ccf19vm6emJ+/eljRzheR4pKSlISEiASqXCjBkzEB0djaZNmwrHVE2QmzdvHnx9fXHnjpHRK8T+MtLFt9dMAI6eECq5RH8IIRYgaUhqSEgIrly5orft0qVLkoekVh0bGhoKhUKBHj164MiRI3rH7Ny5E/3794evry8AICAgQNK5ie1pz58Giu/ZOwzLMtQfQkgDI+lJYdiwYUhKSsLTTz+NiooK/Pvf/8b//vc/vUqqxmg0GqhUKuG1SqVCRkaG3jGZmZkAgNmzZ4PneQwZMgRdunSpda60tDSkpaUBAJKSkhAcHCx+YQqFwX3OzlbXVpGVicKvVuD+uZNAWSng7q4r+2BsDoKTcnd3R5CVf6b0O+l8XPW6jJGUFLp164aZM2di586diIqKQk5ODt599120bt3aYoHwPA+1Wo3ExERoNBokJiZi8eLF8PHRX6EqJiYGMTExwmtDpYhduUyxLa6tZkcyAKeYaFZX93Ozrf4zpd9J5+Oq1xUeHm5wn8mkwPM84uPjsXTpUsTFxZk6XJRSqUReXp7wOi8vD0qlstYxkZGRUCgUaNSoERo3bgy1Wo02bdrU6TNJ/Yh2JDsyTgYog8XXcJDC0KJAhDQwJvsUZDIZZDIZyutR8jgiIgJqtRrZ2dmoqKjAgQMHEB0drXdM9+7dcfbsWQDA3bt3oVarERoaWufPJHXH52QB6U5W18rPHwg28vviFwC0bld7dFSVgCDrxEWIk5HUfPTMM89g2bJlePHFF6FUKsFVKyMs5cYtl8sxduxYzJ8/HzzPo2/fvmjWrBlSU1MRERGB6OhodO7cGSdPnsS0adMgk8kwYsQI+Pn51f3KSN1t/db51j0oLwcXqIShKXJcVBfI4qbrivWJrPzG0XBUQgBITApfffUVAODUqVO19kldjKdr167o2rWr3rbqC/RwHIfRo0dj9OjRks5HrIdlXrd3CObz9tEVnstIr93sFRT8oCid2OI/VLSOEIGkpCD1xk9syxpLP2rPnwZuXLNMgLYikwOx8bqCdO/O1/WHXKlcCrR1O72ZylS0jhDjjCaFsrIy/Otf/8KNGzfQqlUrvPjii3Bzc7NVbMSIqjLVdV36UXv+NLBxBVBcBHh4AmFNdPWJrl6sOptj8fbVldy+X6a/PVAFvPEO5O07AqgsSDcpweipqGgdIYYZ7WhOSUnB0aNH0aRJExw6dAibN2+2VVzEFLEy1RKXftSeP61bSzgvGygp0q2Odv4UcOF07Zuuo1AGA77+NbaFgHt/gZAQCCH1ZzQpnDhxAgkJCRgxYgRmzJiBo0eP2iouYoKhsgySyjVsXKFbHtOZlBTX7ivQ5ND6x4RYmNGkUFZWhqAg3VC94OBgFBcX2yQoYlq9ln4sLrJwNFYWEqarviqCahYRYllG+xS0Wi3OnDkjvOZ5Xu81AHTo0ME6kRHjJIyiMdgR7eGpazZydBynW/RmWJzuOq5eqH1M7m1oF8+iDmNCLMRoUggICMAXX3whvPb19dV7zXEcVq1aZb3oiEGmRtFoz58GVs3T1SzCg45o7ajJAM/bL3CpPDyBSbOF/gJeLAnK5Lp+kbxsszvaCSHiOMacYUksw6oK6dXkqjVLANPXxudkgX08RUgIelSN6l4KwtpkMijaREEbpBL91l/9yQe5t0Wvg3u0N2QOOrKoIf9OOitXva561T4iTmjrt+IJAXDc/oTKJwNVr74G/xFWH0qqXTxLNClQHwMh9UNJwQXU7Dtg2WrDB5eW2C4wYxQKwMcfUIXoSkyY2R9gqKSFpI52QohBlBScnOgkNjd3w29gdu5P8PED16Fr/TuFqVwFIVZBScEKrFF+wiCxSWyOvO5B05YWafOnchWEWAclBQurb/kJczlbG7olm3eoXAUhlidpjWZihnqUn6gLp2pDV4ZQ8w4hDo6eFCysXuUnqjHVBCXsz84CZDLHn3vg6wc0awW2aSV4auohxGFRUrAwS4yKMdUEVZGVqbffKdy7B5w8DMD6TWqEkLqjpGBplhgVY6AJiqWuB+/pBc35U8CdfMvEazM1UmVVkxr1CRDiUCgpWJglRsUYbGo6exyswsmWyTTC2TrJCWkIKClYQX1HxRhca9iFEgLgZJ3khDQQNPrIEQ0ermtyqs4RVrzjZEDn7kDrdrqyFNW5exgsby2KJpoR4pDoScEBiTVBsTv5utXR7KlTNOSVS12KjY4CoNt26k/x0tx+AUB4c5poRogDo6TgoKqaoKpuvrh2yb4BKUN06xpUMthEFjcd/PolYIf21NrFRXUxOpuZz8nCnc2roL2tpsRBiJ1QUnBgfE4WWNL7wN0C+wXh6QW0061pIHmOQR1GYFUNwy210UxwQog4SgoOjH29yr4JAQC0WuDGVWF9ZCk36zqNwDI2E5yGrRJiM5QU7EBvsRhPL93G0hJd30GvfkDaVuDSOaCo0L6BArriepUJQSDhZm3uCCxLzQQnhNQPJQUbqzlbuToGACJt8Y7I0jdrWh+BEMdgsyGpJ06cQHx8PCZPnowff/zR4HF//PEHhg4disuXL9sqNJvhc7LAliQ4V3kKAyx+sxYbhkvDVgmxOZs8KfA8j5SUFCQkJEClUmHGjBmIjo5G06ZN9Y4rKSnB9u3bERkZaYuwbIavLFGBs8cdawKawg1o3hq4eQ24XyZ+TFAwwHH6TUhWuFlX9UN4/Po9Smn0ESF2Y5OkcOnSJYSFhSE0NBQA0KNHDxw5cqRWUkhNTcXgwYOxbds2W4RlE8aai+zKLwDcjEWQhYRB++l7wNULtY/x8QP33qdgeTnAxhW69Z29fYBRk61ys5aFhCFg2hyUu+BC6YQ4C5skBY1GA5VKJbxWqVTIyMjQO+bKlSvIzc1F165djSaFtLQ0pKWlAQCSkpIQHBwsepxCoTC4z5IqsjJR9I+10GpyIVcGw+e1N6EICxf239m8Shhm6ShkwaEImpcsxHmnWQuUiiQFz26PwSdIiYIVH0Obl63bWFIE+TerEThnhd51Woqt/t7sga7N+bjqdRnjEB3NPM/j66+/xsSJE00eGxMTg5iYGOF1roFvlcHBwQb3WUrNp4ByAKXnToGrNlxTe+Mvq8ZgNr8AsHfmoUDhDlT+fPgBrwDnTtWaV1A24BWUbVwJdvuW3im0t29Bs3GlRZbVrMkWf2/2QtfmfFz1usLDDX+hs0lSUCqVyMvLE17n5eVBqXzQUVlaWoobN27g448/BgAUFBTgs88+w/vvv4+IiAhbhGiSaFkHQyWulyRAGxsP7vcdwF8Z4ie0Ey6qS62mH2PzCrQ0VJSQBsUmSSEiIgJqtRrZ2dlQKpU4cOAApkyZIuz39vZGSkqK8HrOnDkYOXKkQyUEsUVv4Bsg/oa8bGDJLPFKp/YUFGywg9jQvAIaKkpIw2KTpCCXyzF27FjMnz8fPM+jb9++aNasGVJTUxEREYHo6GhbhFF3hmbbOvoSmDU1b21+B7ElFg0ihDgNm/UpdO3aFV27dtXbNmzYMNFj58yZY4OIpDPYVOIfCJSXA3edZBW00hLRzcbWg7bEokGEEOfhEB3Njs5gE0qjxmD5eSJ7HJNYk4+p9aCB+i8aRAhxHrTIjhTGZtuWldonJnMZavIxVoiOENLg0JOCBKKL3vTqp7txlt+3d3jimrYE16SFySYfKkRHCKmOkoJE1ZtQtOdPA6vmgTnwUwLXpIWkeQQ0uogQUh01H5mJz8kCVs1z7GYjD0/po4OoEB0hpBp6UjCh5sgcVlpi34Tg7gF5izbQanJ08yHENGkheXQQjS4ihFRHScEI0ZE5CjfbB+LmDoQ2AVQhAACZtgLapi2Bwjui1U05M2/oNLqIEFKFkoIxYiNzbFn6un0nyKd/AkA/QQkR+Afq/ls9MVDTDyGkHigpGGH3ETisWhewWIK6WwB07g7O04uafgghFkFJwQhDI3Ns+flVDCao0hLIJiXYKCJCiKuj0UfGiI3MsZUazUCGhojS0FFCiCXRk0Il0dLYAKBqBGhyAW2FbQJRKICHu4IbFqffDESF6QghNkBJAQZGGV08q+tULrxjnQ/lOMA/CBgyBtzpP032CVQfOqooKkSFjx/1HxBCLI6SAiDeiZtvxdWWQsL0VmfDo70lva1q6KjSRVeDIoTYHyUF2GiUUYASUIXo5hDQN3xCiIOipAAbjDLq3B1yGiFECHECNPoI0HXWKkOsc+6QMHDD4qxzbkIIsTBKClWYFZ4VfPz0+w4IIcTBNejmI2EYavoJ64wy8vSihEAIcSoNNinwOVlgi2cBmhzrfUhAkPXOTQghVtBgm49Y6nrzEoJMBjRuZtZnmFutlBBC7K3BPikgI934/qBgoHlroLTkwQznrd+CqW9IOz/NNiaEOKEGlxSq+hFQUiR+AMeB6/6E6FwCfvBw4MQh8UV2qFopIcQFNKikULOchShvX4NrG8tCwqCdNLv2cpyVw04pCRBCnF2DSgqi5SxqavOQ0d3y9h3BJ35Oy1cSQlxSg0oKJstZKEMkTTSj5SsJIa7KZknhxIkT2LBhA3iex1NPPYUXXnhBb/9//vMf7Ny5E3K5HP7+/njrrbcQEmLZWcYGy1n4BYCL6kLf+AkhDZ5NkgLP80hJSUFCQgJUKhVmzJiB6OhoNG3aVDimZcuWSEpKgoeHB3bs2IFvvvkG06ZNs2wgBtYkoFnHhBCiY5N5CpcuXUJYWBhCQ0OhUCjQo0cPHDlyRO+YDh06wMPDAwAQGRkJjcbylUtllQmAe7Q30K4juEd7U0IghJBqbPKkoNFooFKphNcqlQoZGRkGj9+1axe6dOkiui8tLQ1paWkAgKSkJAQHB4sep1AoxPcFBwMPLZAevAMyeG0ugK7NObnqtbnqdRnjcB3Ne/fuxZUrVzBnzhzR/TExMYiJiRFeG1psJtiFF6Kha3NOdG3Ox1WvKzw83OA+mzQfKZVK5OXlCa/z8vKgVNZecP7UqVP497//jffffx9ubm62CI0QQkg1NkkKERERUKvVyM7ORkVFBQ4cOIDo6Gi9Y65evYp169bh/fffR0BAgC3CIoQQUoNNmo/kcjnGjh2L+fPng+d59O3bF82aNUNqaioiIiIQHR2Nb775BqWlpVi6dCkA3WPbBx98YIvwCCGEVOIYs8bqMraTmZkput1V2wIBujZnRdfmfFz1uoz1KTh9UiCEEGI5LruewocffmjvEKyGrs050bU5H1e9LmNcNikQQggxHyUFQgghApdNCtUnuLkaujbnRNfmfFz1uoyhjmZCCCECl31SIIQQYj5KCoQQQgQOVxDPXI6weI+1mLq2Kn/88QeWLl2KBQsWICIiwrZB1pGUaztw4AC2bNkCjuPQokULxMfH2z5QM5m6rtzcXCQnJ6OoqAg8z+P1119H165d7ROsmVavXo1jx44hICAAS5YsqbWfMYYNGzbg+PHj8PDwwMSJE9G6dWs7RGo+U9e2b98+bN26FYwxeHl5IS4uDi1btrR9oLbAnJhWq2WTJk1iWVlZrLy8nL377rvsxo0besecPn2alZaWMsYY++9//8uWLl1qj1DNJuXaGGOsuLiYffTRR2zmzJns0qVLdojUfFKuLTMzk7333nussLCQMcZYQUGBPUI1i5Tr+vLLL9l///tfxhhjN27cYBMnTrRHqHVy9uxZdvnyZfbOO++I7j969CibP38+43meXbhwgc2YMcPGEdadqWs7f/688Lt47Ngxp7o2czl185GjLN5jDVKuDQBSU1MxePBgp6oqK+Xadu7cif79+8PX1xcAnKJIopTr4jgOxcXFAIDi4mIEBQXZI9Q6iYqKEv4+xPz555944oknwHEc2rZti6KiIuTn59swwrozdW3t2rUT9kdGRupVfXY1Tp0UxBbvMXbTN7Z4j6ORcm1XrlxBbm6u0zQ/VJFybZmZmVCr1Zg9ezZmzZqFEydO2DhK80m5riFDhmDfvn2YMGECFixYgLFjx9o6TKvRaDR6C9KY+vforHbt2oVHHnnE3mFYjVMnBXNULd4zaNAge4diETzP4+uvv8aoUaPsHYpV8DwPtVqNxMRExMfHY82aNSgqKrJ3WPW2f/9+9OnTB19++SVmzJiBlStXgud5e4dFJDpz5gx2796N4cOH2zsUq3HqpODKi/eYurbS0lLcuHEDH3/8Md5++21kZGTgs88+w+XLl+0Rrlmk/L0plUpER0dDoVCgUaNGaNy4MdRqta1DNYuU69q1axcee+wxAEDbtm1RXl6OwsJCm8ZpLUqlUq+iqKF/j87qr7/+wpo1a/Dee+/Bz8/P3uFYjVMnBVdevMfUtXl7eyMlJQXJyclITk5GZGQk3n//facYfSTl76179+44e/YsAODu3btQq9UIDQ21R7iSSbmu4OBgnDlzBgBw8+ZNlJeXw9/f3x7hWlx0dDT27t0LxhguXrwIb29vp+ozMSY3NxeLFy/GpEmTjJaddgVOP6P52LFj2LRpk7B4z0svvaS3eM+8efNw/fp1BAYGAnCuxXtMXVt1c+bMwciRI50iKQCmr40xhq+//honTpyATCbDSy+9hJ49e9o7bJNMXdfNmzexZs0alJaWAgBGjBiBzp072zlqaZYvX4709HQUFhYiICAAQ4cORUVFBQCgX79+YIwhJSUFJ0+ehLu7OyZOnOg0v4+mru3LL7/EoUOHhD4TuVyOpKQke4ZsNU6fFAghhFiOUzcfEUIIsSxKCoQQQgSUFAghhAgoKRBCCBE4fUE8QghpKEwV7qupLkUlKSkQYgXfffcdsrKyMGXKlHqfa9++fdizZw8SEhIsEBlxZn369MGAAQOQnJxs8li1Wo0ff/wR8+bNg6+vL+7cuSPpMygpEJc0Z84c/PXXX1i7dq2kWey//fYbdu7ciXnz5lk9trNnz2Lu3Llwd3cHx3EICgrCCy+8gL59+4oe//jjj+Pxxx+3elzE8UVFRSE7O1tvW1ZWFlJSUnD37l14eHhg/PjxaNKkSZ2LSlJSIC4nOzsb586dg7e3N/7880+hrIQjCQoKwpdffgnGGI4cOYKlS5ciMjISTZs21TtOq9VCLpfbKUriDNauXYtx48ahcePGyMjIwPr165GYmIjMzEwAwOzZs8HzPIYMGSKpICglBeJy9u7di7Zt26JNmzbYs2ePXlLIzc3Fxo0bce7cOTDG0LNnT/Tv3x/r1q1DRUUFRo4cCblcjo0bN2LOnDl4/PHH8dRTTwGo/TSxYcMGHD58GMXFxQgLC0NsbCweeughs2LlOA7du3eHj48Pbt68iUuXLmHnzp2IiIjA3r170a9fP4SFhel97o0bN7Bx40ZcuXIFCoUCAwcOxEsvvQSe57Ft2zbs3LkTRUVF6NChA958802jJaGJcystLcWFCxewdOlSYVvVTOzqRSU1Gg0SExOxePFi+Pj4GD0nJQXicvbs2YPnnnsOkZGRmDVrFgoKChAYGAie57Fw4UI8/PDDSE5Ohkwmw5UrV9C0aVOMGzfO7OajiIgIvPLKK/D29sYvv/yCpUuXIjk5Ge7u7pLPwfM8/vzzTxQXF6N58+a4ePEiMjIy0KNHD6xbtw5arRYHDhwQji8pKcG8efPw/PPP44MPPoBWq8XNmzcBAL/++iuOHDmCOXPmwN/fHxs2bMD69esxdepUyfEQ58LzPHx8fLBo0aJa+5RKJSIjI2sVlWzTpo3Rc9KQVOJSzp8/j9zcXDz22GNo3bo1QkND8fvvvwPQLYKj0WgwcuRIeHp6wt3dHe3bt6/zZz3xxBPw8/ODXC7H888/j4qKCuGR3ZT8/HzExsbijTfewJYtW/QKrQUFBWHgwIGQy+W1EszRo0cRGBiI559/Hu7u7vDy8kJkZCQA4H//+x9effVVqFQquLm5YciQITh06BC0Wm2dr5E4Nm9vbzRq1AgHDx4EoFsS9dq1awDqXlSSnhSIS/ntt9/QqVMnofJor169hCeH3NxchISEWKyNftu2bdi9ezc0Gg04jkNJSYnkMthVfQpiqi9UU1NeXp7Bf9g5OTlYvHgxOI4TtslkMty5c8elSlg3ZNUL902YMAFDhw7FlClTsG7dOvzwww+oqKhAz5490bJlS3Tu3BknT57EtGnTIJPJMGLECEklvykpEJdx//59HDx4EDzPY9y4cQB07atFRUW4du0agoODkZubK7nz1sPDA2VlZcLrgoIC4f/PnTuHbdu24aOPPkLTpk0hk8kwZswYWLu+pEql0mtOqrnvrbfeqtfTD3FshpoCZ82aVWsbx3EYPXo0Ro8ebdZnUPMRcRmHDx+GTCbDsmXLsGjRIixatAjLli3DQw89hL1796JNmzYICgrCt99+i9LSUty/fx/nz58HAAQGBkKj0QiddADQsmVLHD58GGVlZcjKysKuXbuEfSUlJZDL5fD39wfP8/j++++FtZetqVu3bsjPz8fPP/+M8vJylJSUICMjAwDw9NNP45///CdycnIA6JoMxNb1JsQYelIgLmPPnj3o27dvreaX/v37Y8OGDRg+fDg++OADfPXVV5g4cSI4jkPPnj3Rvn17dOjQQehwlslkSElJwbPPPovLly9j3LhxaNGiBXr16oXTp08DALp06YLOnTsjPj4eHh4eePbZZ402+1iKl5cXEhISsHHjRnz//fdQKBR49tlnERkZiWeeeQYA8MknnyA/Px8BAQF47LHH8Le//c3qcRHXQespEEIIEVDzESGEEAElBUIIIQJKCoQQQgSUFAghhAgoKRBCCBFQUiCEECKgpEAIIURASYEQQojg/wHxoz32qSWHZwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_train, y_train_pred)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2bYCWU10f0s",
        "outputId": "362f3d3b-e347-405d-f87d-cd732598c1b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[32m-\u001b[0m \u001b[32m58.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m \u001b[33m0:00:05\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import shap\n",
        "except ModuleNotFoundError: # Install flax if missing\n",
        "    !pip install --quiet https://github.com/ceshine/shap/archive/master.zip\n",
        "    import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "LMer4Cs-DtBL",
        "outputId": "78d1089e-c30f-4971-a5a8-29c9814da026"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/shap/explainers/deep/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_phase_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyTorchDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/shap/explainers/deep/deep_tf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# if keras is installed and already has a session then use it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SESSION\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'tensorflow_backend'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "e = shap.DeepExplainer(NN_model, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B-THl8eRDtpW",
        "outputId": "d7b1bba1-cc5d-48b9-a794-c9343aa98adb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/shap/explainers/deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/shap/explainers/deep/deep_pytorch.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;31m# run attribution computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0msample_phis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/shap/explainers/deep/deep_pytorch.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minterim_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/shap/explainers/deep/deep_pytorch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minterim_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are trying to call the hook of a dead Module!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/shap/explainers/deep/deep_pytorch.py\u001b[0m in \u001b[0;36mdeeplift_grad\u001b[0;34m(module, grad_input, grad_output)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mop_handler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linear_1d'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop_handler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Warning: unrecognized nn.Module: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/shap/explainers/deep/deep_pytorch.py\u001b[0m in \u001b[0;36mnonlinear_1d\u001b[0;34m(module, grad_input, grad_output)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnonlinear_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mdelta_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mdelta_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1270\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ReLU' object has no attribute 'y'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "shap_values = e.shap_values(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "IJFnGTc5DySx",
        "outputId": "916d6567-8b4f-4f83-c157-343879401fef"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-c8687ef23b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshap_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'shap_values' is not defined"
          ]
        }
      ],
      "source": [
        "shap_values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtEWJnmPD0fP"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\n",
        "    \"mean_abs_shap\": np.mean(np.abs(shap_values), axis=0), \n",
        "    \"stdev_abs_shap\": np.std(np.abs(shap_values), axis=0), \n",
        "    \"name\": features\n",
        "})\n",
        "df.sort_values(\"mean_abs_shap\", ascending=False)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhIWZGadD3QV"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values, features=x_samples, feature_names=features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_xZobpkkFO9"
      },
      "source": [
        "# Price / Sqm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBAtXJCjqlbd"
      },
      "source": [
        "## Data Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dN6YVaEAmIU"
      },
      "outputs": [],
      "source": [
        "train_df2 = train_df.copy()\n",
        "test_df2 = test_df.copy()\n",
        "\n",
        "train_df2[\"floor_area_sqm\"] = train_df2[\"floor_area_sqm\"].apply(lambda x : x * 23.922319549360488 + 95.09078798185942)\n",
        "test_df2[\"floor_area_sqm\"] = test_df2[\"floor_area_sqm\"].apply(lambda x : x * 23.922319549360488 + 95.09078798185942)\n",
        "\n",
        "train_df2[\"resale_price_per_sqm\"] = train_df2[\"resale_price\"] / train_df2[\"floor_area_sqm\"]\n",
        "test_df2[\"resale_price_per_sqm\"] = test_df2[\"resale_price\"] / test_df2[\"floor_area_sqm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EmQ8jLMAhXM"
      },
      "outputs": [],
      "source": [
        "X_train = train_df2.drop([\"resale_price_per_sqm\", \"resale_price\", \"floor_area_sqm\"], axis = 1)\n",
        "X_test = test_df2.drop([\"resale_price_per_sqm\", \"resale_price\", \"floor_area_sqm\"], axis = 1)\n",
        "\n",
        "y_train = train_df2[\"resale_price_per_sqm\"]\n",
        "y_test = test_df2[\"resale_price_per_sqm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "uirmAeUOqlbn",
        "outputId": "2d48a868-c114-408f-aceb-cd9deed558ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c32edf7a-e810-4a57-b905-c4500ad08f45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nearest_mrt_dist</th>\n",
              "      <th>total_resales_in_town</th>\n",
              "      <th>remaining_lease</th>\n",
              "      <th>town_BUKIT MERAH</th>\n",
              "      <th>street_name_ANG MO KIO ST 51</th>\n",
              "      <th>street_name_DAWSON RD</th>\n",
              "      <th>flat_type_3 ROOM</th>\n",
              "      <th>storey_range_01 TO 03</th>\n",
              "      <th>storey_range_04 TO 06</th>\n",
              "      <th>storey_range_19 TO 21</th>\n",
              "      <th>...</th>\n",
              "      <th>town_SENGKANG</th>\n",
              "      <th>town_QUEENSTOWN</th>\n",
              "      <th>street_name_ANG MO KIO AVE 3</th>\n",
              "      <th>storey_range_22 TO 24</th>\n",
              "      <th>storey_range_25 TO 27</th>\n",
              "      <th>storey_range_28 TO 30</th>\n",
              "      <th>storey_range_31 TO 33</th>\n",
              "      <th>storey_range_37 TO 39</th>\n",
              "      <th>town_JURONG EAST</th>\n",
              "      <th>street_name_TELOK BLANGAH ST 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.676490</td>\n",
              "      <td>-0.443615</td>\n",
              "      <td>-1.143025</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>1.693641</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.201327</td>\n",
              "      <td>1.252320</td>\n",
              "      <td>1.352967</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>2.178515</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.384369</td>\n",
              "      <td>-0.198538</td>\n",
              "      <td>-0.290599</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>2.178515</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.731095</td>\n",
              "      <td>1.252320</td>\n",
              "      <td>-0.613741</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.054745</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>0.857112</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>1.842152</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  73 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c32edf7a-e810-4a57-b905-c4500ad08f45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c32edf7a-e810-4a57-b905-c4500ad08f45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c32edf7a-e810-4a57-b905-c4500ad08f45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   nearest_mrt_dist  total_resales_in_town  remaining_lease  town_BUKIT MERAH  \\\n",
              "0         -0.676490              -0.443615        -1.143025         -0.193241   \n",
              "1          0.201327               1.252320         1.352967         -0.193241   \n",
              "2         -0.384369              -0.198538        -0.290599         -0.193241   \n",
              "3          1.731095               1.252320        -0.613741         -0.193241   \n",
              "4          1.054745               1.291532         0.857112         -0.193241   \n",
              "\n",
              "   street_name_ANG MO KIO ST 51  street_name_DAWSON RD  flat_type_3 ROOM  \\\n",
              "0                     -0.050572              -0.044588          1.693641   \n",
              "1                     -0.050572              -0.044588         -0.590444   \n",
              "2                     -0.050572              -0.044588         -0.590444   \n",
              "3                     -0.050572              -0.044588         -0.590444   \n",
              "4                     -0.050572              -0.044588         -0.590444   \n",
              "\n",
              "   storey_range_01 TO 03  storey_range_04 TO 06  storey_range_19 TO 21  ...  \\\n",
              "0              -0.459028              -0.542843              -0.128147  ...   \n",
              "1               2.178515              -0.542843              -0.128147  ...   \n",
              "2               2.178515              -0.542843              -0.128147  ...   \n",
              "3              -0.459028              -0.542843              -0.128147  ...   \n",
              "4              -0.459028               1.842152              -0.128147  ...   \n",
              "\n",
              "   town_SENGKANG  town_QUEENSTOWN  street_name_ANG MO KIO AVE 3  \\\n",
              "0      -0.261617        -0.148379                     -0.089443   \n",
              "1      -0.261617        -0.148379                     -0.089443   \n",
              "2      -0.261617        -0.148379                     -0.089443   \n",
              "3      -0.261617        -0.148379                     -0.089443   \n",
              "4      -0.261617        -0.148379                     -0.089443   \n",
              "\n",
              "   storey_range_22 TO 24  storey_range_25 TO 27  storey_range_28 TO 30  \\\n",
              "0              -0.111079                -0.1001              -0.067497   \n",
              "1              -0.111079                -0.1001              -0.067497   \n",
              "2              -0.111079                -0.1001              -0.067497   \n",
              "3              -0.111079                -0.1001              -0.067497   \n",
              "4              -0.111079                -0.1001              -0.067497   \n",
              "\n",
              "   storey_range_31 TO 33  storey_range_37 TO 39  town_JURONG EAST  \\\n",
              "0              -0.047673              -0.047673         -0.148379   \n",
              "1              -0.047673              -0.047673         -0.148379   \n",
              "2              -0.047673              -0.047673         -0.148379   \n",
              "3              -0.047673              -0.047673         -0.148379   \n",
              "4              -0.047673              -0.047673         -0.148379   \n",
              "\n",
              "   street_name_TELOK BLANGAH ST 31  \n",
              "0                        -0.075507  \n",
              "1                        -0.075507  \n",
              "2                        -0.075507  \n",
              "3                        -0.075507  \n",
              "4                        -0.075507  \n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "xT7-BAnxqlbo",
        "outputId": "1917d796-5113-414f-cd38-b725a18eaa9a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-82351e0b-b7de-4232-ab0f-824c54c56c74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nearest_mrt_dist</th>\n",
              "      <th>total_resales_in_town</th>\n",
              "      <th>remaining_lease</th>\n",
              "      <th>town_BUKIT MERAH</th>\n",
              "      <th>street_name_ANG MO KIO ST 51</th>\n",
              "      <th>street_name_DAWSON RD</th>\n",
              "      <th>flat_type_3 ROOM</th>\n",
              "      <th>storey_range_01 TO 03</th>\n",
              "      <th>storey_range_04 TO 06</th>\n",
              "      <th>storey_range_19 TO 21</th>\n",
              "      <th>...</th>\n",
              "      <th>town_SENGKANG</th>\n",
              "      <th>town_QUEENSTOWN</th>\n",
              "      <th>street_name_ANG MO KIO AVE 3</th>\n",
              "      <th>storey_range_22 TO 24</th>\n",
              "      <th>storey_range_25 TO 27</th>\n",
              "      <th>storey_range_28 TO 30</th>\n",
              "      <th>storey_range_31 TO 33</th>\n",
              "      <th>storey_range_37 TO 39</th>\n",
              "      <th>town_JURONG EAST</th>\n",
              "      <th>street_name_TELOK BLANGAH ST 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004724</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>1.224825</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.306314</td>\n",
              "      <td>0.242602</td>\n",
              "      <td>1.403110</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.341318</td>\n",
              "      <td>0.644528</td>\n",
              "      <td>-0.808741</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>1.842152</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.265904</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>1.386396</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.978913</td>\n",
              "      <td>0.036737</td>\n",
              "      <td>-1.209882</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>1.693641</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  73 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82351e0b-b7de-4232-ab0f-824c54c56c74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82351e0b-b7de-4232-ab0f-824c54c56c74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82351e0b-b7de-4232-ab0f-824c54c56c74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   nearest_mrt_dist  total_resales_in_town  remaining_lease  town_BUKIT MERAH  \\\n",
              "0          0.004724               1.291532         1.224825         -0.193241   \n",
              "1          1.306314               0.242602         1.403110         -0.193241   \n",
              "2         -0.341318               0.644528        -0.808741         -0.193241   \n",
              "3          1.265904               1.291532         1.386396         -0.193241   \n",
              "4         -0.978913               0.036737        -1.209882         -0.193241   \n",
              "\n",
              "   street_name_ANG MO KIO ST 51  street_name_DAWSON RD  flat_type_3 ROOM  \\\n",
              "0                     -0.050572              -0.044588         -0.590444   \n",
              "1                     -0.050572              -0.044588         -0.590444   \n",
              "2                     -0.050572              -0.044588         -0.590444   \n",
              "3                     -0.050572              -0.044588         -0.590444   \n",
              "4                     -0.050572              -0.044588          1.693641   \n",
              "\n",
              "   storey_range_01 TO 03  storey_range_04 TO 06  storey_range_19 TO 21  ...  \\\n",
              "0              -0.459028              -0.542843              -0.128147  ...   \n",
              "1              -0.459028              -0.542843              -0.128147  ...   \n",
              "2              -0.459028               1.842152              -0.128147  ...   \n",
              "3              -0.459028              -0.542843              -0.128147  ...   \n",
              "4              -0.459028              -0.542843              -0.128147  ...   \n",
              "\n",
              "   town_SENGKANG  town_QUEENSTOWN  street_name_ANG MO KIO AVE 3  \\\n",
              "0      -0.261617        -0.148379                     -0.089443   \n",
              "1      -0.261617        -0.148379                     -0.089443   \n",
              "2      -0.261617        -0.148379                     -0.089443   \n",
              "3      -0.261617        -0.148379                     -0.089443   \n",
              "4      -0.261617        -0.148379                     -0.089443   \n",
              "\n",
              "   storey_range_22 TO 24  storey_range_25 TO 27  storey_range_28 TO 30  \\\n",
              "0              -0.111079                -0.1001              -0.067497   \n",
              "1              -0.111079                -0.1001              -0.067497   \n",
              "2              -0.111079                -0.1001              -0.067497   \n",
              "3              -0.111079                -0.1001              -0.067497   \n",
              "4              -0.111079                -0.1001              -0.067497   \n",
              "\n",
              "   storey_range_31 TO 33  storey_range_37 TO 39  town_JURONG EAST  \\\n",
              "0              -0.047673              -0.047673         -0.148379   \n",
              "1              -0.047673              -0.047673         -0.148379   \n",
              "2              -0.047673              -0.047673         -0.148379   \n",
              "3              -0.047673              -0.047673         -0.148379   \n",
              "4              -0.047673              -0.047673         -0.148379   \n",
              "\n",
              "   street_name_TELOK BLANGAH ST 31  \n",
              "0                        -0.075507  \n",
              "1                        -0.075507  \n",
              "2                        -0.075507  \n",
              "3                        -0.075507  \n",
              "4                        -0.075507  \n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7FoJ5lZqlbo"
      },
      "source": [
        "## Make the Deep Neural Network\n",
        " * Define a sequential model\n",
        " * Add some dense layers\n",
        " * Use '**relu**' as the activation function in the hidden layers\n",
        " * Use a '**normal**' initializer as the kernal_intializer \n",
        "           Initializers define the way to set the initial random weights of Keras layers.\n",
        " * We will use mean_absolute_error as a loss function\n",
        " * Define the output layer with only one node\n",
        " * Use 'linear 'as the activation function for the output layer\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fDpPrDgqlbp"
      },
      "outputs": [],
      "source": [
        "NN_model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N40Bt3gwqlbp"
      },
      "source": [
        "**The Input Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CELIlIMxqlbp"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q69JzBQJqlbp"
      },
      "source": [
        "**The Hidden Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nt3ci4rqlbq"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b7NNLiKqlbq"
      },
      "source": [
        "**The Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giwGd8R0qlbq"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwKuKexeqlbq"
      },
      "source": [
        "**Compile the network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ35DiPxqlbr",
        "outputId": "2638dcc2-5604-480b-ec45-14617d3d9280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 128)               9472      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 174,337\n",
            "Trainable params: 174,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NJ_0pF3qlbr"
      },
      "source": [
        "**Define a checkpoint callback :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuqVlO-1qlbr"
      },
      "outputs": [],
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWDhsKYzqlbr"
      },
      "source": [
        "## Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hloNBZnIqlbs",
        "outputId": "737c1b61-3837-4efb-8630-73e425905a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 3527.3071 - mean_absolute_error: 3527.3071\n",
            "Epoch 1: val_loss improved from inf to 657.11688, saving model to Weights-001--657.11688.hdf5\n",
            "89/89 [==============================] - 2s 8ms/step - loss: 3520.9810 - mean_absolute_error: 3520.9810 - val_loss: 657.1169 - val_mean_absolute_error: 657.1169\n",
            "Epoch 2/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 530.3192 - mean_absolute_error: 530.3192\n",
            "Epoch 2: val_loss improved from 657.11688 to 430.65860, saving model to Weights-002--430.65860.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 522.3741 - mean_absolute_error: 522.3741 - val_loss: 430.6586 - val_mean_absolute_error: 430.6586\n",
            "Epoch 3/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 416.5964 - mean_absolute_error: 416.5964\n",
            "Epoch 3: val_loss did not improve from 430.65860\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 415.7528 - mean_absolute_error: 415.7528 - val_loss: 446.1215 - val_mean_absolute_error: 446.1215\n",
            "Epoch 4/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 383.9500 - mean_absolute_error: 383.9500\n",
            "Epoch 4: val_loss improved from 430.65860 to 381.07840, saving model to Weights-004--381.07840.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 385.0210 - mean_absolute_error: 385.0210 - val_loss: 381.0784 - val_mean_absolute_error: 381.0784\n",
            "Epoch 5/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 356.2331 - mean_absolute_error: 356.2331\n",
            "Epoch 5: val_loss improved from 381.07840 to 373.69537, saving model to Weights-005--373.69537.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 359.2411 - mean_absolute_error: 359.2411 - val_loss: 373.6954 - val_mean_absolute_error: 373.6954\n",
            "Epoch 6/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 359.5891 - mean_absolute_error: 359.5891\n",
            "Epoch 6: val_loss improved from 373.69537 to 371.30746, saving model to Weights-006--371.30746.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 360.8372 - mean_absolute_error: 360.8372 - val_loss: 371.3075 - val_mean_absolute_error: 371.3075\n",
            "Epoch 7/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 333.9594 - mean_absolute_error: 333.9594\n",
            "Epoch 7: val_loss did not improve from 371.30746\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 332.7035 - mean_absolute_error: 332.7035 - val_loss: 383.7675 - val_mean_absolute_error: 383.7675\n",
            "Epoch 8/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 324.0896 - mean_absolute_error: 324.0896\n",
            "Epoch 8: val_loss did not improve from 371.30746\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 326.0794 - mean_absolute_error: 326.0794 - val_loss: 375.7841 - val_mean_absolute_error: 375.7841\n",
            "Epoch 9/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 323.9172 - mean_absolute_error: 323.9172\n",
            "Epoch 9: val_loss improved from 371.30746 to 368.91034, saving model to Weights-009--368.91034.hdf5\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 324.6651 - mean_absolute_error: 324.6651 - val_loss: 368.9103 - val_mean_absolute_error: 368.9103\n",
            "Epoch 10/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 318.2132 - mean_absolute_error: 318.2132\n",
            "Epoch 10: val_loss improved from 368.91034 to 351.02304, saving model to Weights-010--351.02304.hdf5\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 317.4535 - mean_absolute_error: 317.4535 - val_loss: 351.0230 - val_mean_absolute_error: 351.0230\n",
            "Epoch 11/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 309.3467 - mean_absolute_error: 309.3467\n",
            "Epoch 11: val_loss improved from 351.02304 to 350.62323, saving model to Weights-011--350.62323.hdf5\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 308.3307 - mean_absolute_error: 308.3307 - val_loss: 350.6232 - val_mean_absolute_error: 350.6232\n",
            "Epoch 12/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 292.8095 - mean_absolute_error: 292.8095\n",
            "Epoch 12: val_loss did not improve from 350.62323\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 292.4939 - mean_absolute_error: 292.4939 - val_loss: 372.6191 - val_mean_absolute_error: 372.6191\n",
            "Epoch 13/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 305.7796 - mean_absolute_error: 305.7796\n",
            "Epoch 13: val_loss did not improve from 350.62323\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 303.9382 - mean_absolute_error: 303.9382 - val_loss: 370.6248 - val_mean_absolute_error: 370.6248\n",
            "Epoch 14/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 295.7383 - mean_absolute_error: 295.7383\n",
            "Epoch 14: val_loss improved from 350.62323 to 335.57965, saving model to Weights-014--335.57965.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 297.1889 - mean_absolute_error: 297.1889 - val_loss: 335.5797 - val_mean_absolute_error: 335.5797\n",
            "Epoch 15/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 291.3194 - mean_absolute_error: 291.3194\n",
            "Epoch 15: val_loss did not improve from 335.57965\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 291.2903 - mean_absolute_error: 291.2903 - val_loss: 335.9833 - val_mean_absolute_error: 335.9833\n",
            "Epoch 16/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 278.9585 - mean_absolute_error: 278.9585\n",
            "Epoch 16: val_loss did not improve from 335.57965\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 279.7863 - mean_absolute_error: 279.7863 - val_loss: 352.7690 - val_mean_absolute_error: 352.7690\n",
            "Epoch 17/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 289.6189 - mean_absolute_error: 289.6189\n",
            "Epoch 17: val_loss did not improve from 335.57965\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 290.2899 - mean_absolute_error: 290.2899 - val_loss: 349.6512 - val_mean_absolute_error: 349.6512\n",
            "Epoch 18/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 284.9958 - mean_absolute_error: 284.9958\n",
            "Epoch 18: val_loss improved from 335.57965 to 335.32092, saving model to Weights-018--335.32092.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 284.7617 - mean_absolute_error: 284.7617 - val_loss: 335.3209 - val_mean_absolute_error: 335.3209\n",
            "Epoch 19/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 275.3290 - mean_absolute_error: 275.3290\n",
            "Epoch 19: val_loss did not improve from 335.32092\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 273.3807 - mean_absolute_error: 273.3807 - val_loss: 354.4266 - val_mean_absolute_error: 354.4266\n",
            "Epoch 20/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 276.5820 - mean_absolute_error: 276.5820\n",
            "Epoch 20: val_loss did not improve from 335.32092\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 276.7962 - mean_absolute_error: 276.7962 - val_loss: 339.3177 - val_mean_absolute_error: 339.3177\n",
            "Epoch 21/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 270.6555 - mean_absolute_error: 270.6555\n",
            "Epoch 21: val_loss did not improve from 335.32092\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 270.0240 - mean_absolute_error: 270.0240 - val_loss: 341.8265 - val_mean_absolute_error: 341.8265\n",
            "Epoch 22/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 261.7876 - mean_absolute_error: 261.7876\n",
            "Epoch 22: val_loss did not improve from 335.32092\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 264.2076 - mean_absolute_error: 264.2076 - val_loss: 348.7661 - val_mean_absolute_error: 348.7661\n",
            "Epoch 23/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 260.0955 - mean_absolute_error: 260.0955\n",
            "Epoch 23: val_loss improved from 335.32092 to 332.35333, saving model to Weights-023--332.35333.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 259.5573 - mean_absolute_error: 259.5573 - val_loss: 332.3533 - val_mean_absolute_error: 332.3533\n",
            "Epoch 24/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 268.4377 - mean_absolute_error: 268.4377\n",
            "Epoch 24: val_loss did not improve from 332.35333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 266.7818 - mean_absolute_error: 266.7818 - val_loss: 338.0597 - val_mean_absolute_error: 338.0597\n",
            "Epoch 25/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 266.1196 - mean_absolute_error: 266.1196\n",
            "Epoch 25: val_loss did not improve from 332.35333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 266.5486 - mean_absolute_error: 266.5486 - val_loss: 347.4384 - val_mean_absolute_error: 347.4384\n",
            "Epoch 26/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 257.5552 - mean_absolute_error: 257.5552\n",
            "Epoch 26: val_loss did not improve from 332.35333\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 257.5552 - mean_absolute_error: 257.5552 - val_loss: 343.4413 - val_mean_absolute_error: 343.4413\n",
            "Epoch 27/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 258.6601 - mean_absolute_error: 258.6601\n",
            "Epoch 27: val_loss did not improve from 332.35333\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 260.4124 - mean_absolute_error: 260.4124 - val_loss: 337.0956 - val_mean_absolute_error: 337.0956\n",
            "Epoch 28/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 262.2550 - mean_absolute_error: 262.2550\n",
            "Epoch 28: val_loss did not improve from 332.35333\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 261.9208 - mean_absolute_error: 261.9208 - val_loss: 383.4818 - val_mean_absolute_error: 383.4818\n",
            "Epoch 29/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 265.3998 - mean_absolute_error: 265.3998\n",
            "Epoch 29: val_loss did not improve from 332.35333\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 264.2461 - mean_absolute_error: 264.2461 - val_loss: 333.2408 - val_mean_absolute_error: 333.2408\n",
            "Epoch 30/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 252.3362 - mean_absolute_error: 252.3362\n",
            "Epoch 30: val_loss did not improve from 332.35333\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 252.3362 - mean_absolute_error: 252.3362 - val_loss: 339.1425 - val_mean_absolute_error: 339.1425\n",
            "Epoch 31/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 267.4353 - mean_absolute_error: 267.4353\n",
            "Epoch 31: val_loss improved from 332.35333 to 328.79468, saving model to Weights-031--328.79468.hdf5\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 267.5207 - mean_absolute_error: 267.5207 - val_loss: 328.7947 - val_mean_absolute_error: 328.7947\n",
            "Epoch 32/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 259.3539 - mean_absolute_error: 259.3539\n",
            "Epoch 32: val_loss improved from 328.79468 to 326.19669, saving model to Weights-032--326.19669.hdf5\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 259.3539 - mean_absolute_error: 259.3539 - val_loss: 326.1967 - val_mean_absolute_error: 326.1967\n",
            "Epoch 33/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 249.6272 - mean_absolute_error: 249.6272\n",
            "Epoch 33: val_loss did not improve from 326.19669\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 249.6272 - mean_absolute_error: 249.6272 - val_loss: 328.9426 - val_mean_absolute_error: 328.9426\n",
            "Epoch 34/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 253.4730 - mean_absolute_error: 253.4730\n",
            "Epoch 34: val_loss did not improve from 326.19669\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 253.1202 - mean_absolute_error: 253.1202 - val_loss: 339.8580 - val_mean_absolute_error: 339.8580\n",
            "Epoch 35/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 262.8706 - mean_absolute_error: 262.8706\n",
            "Epoch 35: val_loss did not improve from 326.19669\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 262.1083 - mean_absolute_error: 262.1083 - val_loss: 358.7399 - val_mean_absolute_error: 358.7399\n",
            "Epoch 36/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 257.0129 - mean_absolute_error: 257.0129\n",
            "Epoch 36: val_loss improved from 326.19669 to 321.95743, saving model to Weights-036--321.95743.hdf5\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 257.4460 - mean_absolute_error: 257.4460 - val_loss: 321.9574 - val_mean_absolute_error: 321.9574\n",
            "Epoch 37/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 246.1768 - mean_absolute_error: 246.1768\n",
            "Epoch 37: val_loss did not improve from 321.95743\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 246.1768 - mean_absolute_error: 246.1768 - val_loss: 334.8615 - val_mean_absolute_error: 334.8615\n",
            "Epoch 38/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 251.6971 - mean_absolute_error: 251.6971\n",
            "Epoch 38: val_loss did not improve from 321.95743\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 251.6971 - mean_absolute_error: 251.6971 - val_loss: 344.3758 - val_mean_absolute_error: 344.3758\n",
            "Epoch 39/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 242.7706 - mean_absolute_error: 242.7706\n",
            "Epoch 39: val_loss did not improve from 321.95743\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 243.2056 - mean_absolute_error: 243.2056 - val_loss: 340.0397 - val_mean_absolute_error: 340.0397\n",
            "Epoch 40/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 244.1766 - mean_absolute_error: 244.1766\n",
            "Epoch 40: val_loss did not improve from 321.95743\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 244.0649 - mean_absolute_error: 244.0649 - val_loss: 349.6403 - val_mean_absolute_error: 349.6403\n",
            "Epoch 41/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 254.4105 - mean_absolute_error: 254.4105\n",
            "Epoch 41: val_loss did not improve from 321.95743\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 254.4582 - mean_absolute_error: 254.4582 - val_loss: 364.9547 - val_mean_absolute_error: 364.9547\n",
            "Epoch 42/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 254.8456 - mean_absolute_error: 254.8456\n",
            "Epoch 42: val_loss improved from 321.95743 to 319.17657, saving model to Weights-042--319.17657.hdf5\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 254.7497 - mean_absolute_error: 254.7497 - val_loss: 319.1766 - val_mean_absolute_error: 319.1766\n",
            "Epoch 43/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 246.1826 - mean_absolute_error: 246.1826\n",
            "Epoch 43: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 246.1826 - mean_absolute_error: 246.1826 - val_loss: 327.2155 - val_mean_absolute_error: 327.2155\n",
            "Epoch 44/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 241.1366 - mean_absolute_error: 241.1366\n",
            "Epoch 44: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 241.2645 - mean_absolute_error: 241.2645 - val_loss: 325.2557 - val_mean_absolute_error: 325.2557\n",
            "Epoch 45/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 248.6976 - mean_absolute_error: 248.6976\n",
            "Epoch 45: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 248.7495 - mean_absolute_error: 248.7495 - val_loss: 321.2839 - val_mean_absolute_error: 321.2839\n",
            "Epoch 46/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 239.3516 - mean_absolute_error: 239.3516\n",
            "Epoch 46: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 239.3516 - mean_absolute_error: 239.3516 - val_loss: 331.3133 - val_mean_absolute_error: 331.3133\n",
            "Epoch 47/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 241.8585 - mean_absolute_error: 241.8585\n",
            "Epoch 47: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 242.7151 - mean_absolute_error: 242.7151 - val_loss: 331.1094 - val_mean_absolute_error: 331.1094\n",
            "Epoch 48/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 239.8244 - mean_absolute_error: 239.8244\n",
            "Epoch 48: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 239.8553 - mean_absolute_error: 239.8553 - val_loss: 323.3973 - val_mean_absolute_error: 323.3973\n",
            "Epoch 49/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 244.0801 - mean_absolute_error: 244.0801\n",
            "Epoch 49: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 244.0801 - mean_absolute_error: 244.0801 - val_loss: 325.2812 - val_mean_absolute_error: 325.2812\n",
            "Epoch 50/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 238.1637 - mean_absolute_error: 238.1637\n",
            "Epoch 50: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 239.1939 - mean_absolute_error: 239.1939 - val_loss: 361.5545 - val_mean_absolute_error: 361.5545\n",
            "Epoch 51/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 243.4644 - mean_absolute_error: 243.4644\n",
            "Epoch 51: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 243.7091 - mean_absolute_error: 243.7091 - val_loss: 397.4208 - val_mean_absolute_error: 397.4208\n",
            "Epoch 52/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 250.0317 - mean_absolute_error: 250.0317\n",
            "Epoch 52: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 249.6976 - mean_absolute_error: 249.6976 - val_loss: 329.7648 - val_mean_absolute_error: 329.7648\n",
            "Epoch 53/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 250.2692 - mean_absolute_error: 250.2692\n",
            "Epoch 53: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 249.6296 - mean_absolute_error: 249.6296 - val_loss: 323.7057 - val_mean_absolute_error: 323.7057\n",
            "Epoch 54/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 233.2192 - mean_absolute_error: 233.2192\n",
            "Epoch 54: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 232.5610 - mean_absolute_error: 232.5610 - val_loss: 326.4893 - val_mean_absolute_error: 326.4893\n",
            "Epoch 55/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 236.0237 - mean_absolute_error: 236.0237\n",
            "Epoch 55: val_loss did not improve from 319.17657\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 236.0237 - mean_absolute_error: 236.0237 - val_loss: 331.6212 - val_mean_absolute_error: 331.6212\n",
            "Epoch 56/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 239.6652 - mean_absolute_error: 239.6652\n",
            "Epoch 56: val_loss improved from 319.17657 to 316.96957, saving model to Weights-056--316.96957.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 238.6092 - mean_absolute_error: 238.6092 - val_loss: 316.9696 - val_mean_absolute_error: 316.9696\n",
            "Epoch 57/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 237.8493 - mean_absolute_error: 237.8493\n",
            "Epoch 57: val_loss did not improve from 316.96957\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 239.1289 - mean_absolute_error: 239.1289 - val_loss: 318.3512 - val_mean_absolute_error: 318.3512\n",
            "Epoch 58/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 232.5978 - mean_absolute_error: 232.5978\n",
            "Epoch 58: val_loss improved from 316.96957 to 315.76181, saving model to Weights-058--315.76181.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 233.4211 - mean_absolute_error: 233.4211 - val_loss: 315.7618 - val_mean_absolute_error: 315.7618\n",
            "Epoch 59/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 230.8515 - mean_absolute_error: 230.8515\n",
            "Epoch 59: val_loss did not improve from 315.76181\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 232.1389 - mean_absolute_error: 232.1389 - val_loss: 339.1671 - val_mean_absolute_error: 339.1671\n",
            "Epoch 60/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 234.5343 - mean_absolute_error: 234.5343\n",
            "Epoch 60: val_loss did not improve from 315.76181\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 234.1944 - mean_absolute_error: 234.1944 - val_loss: 316.9472 - val_mean_absolute_error: 316.9472\n",
            "Epoch 61/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 225.2919 - mean_absolute_error: 225.2919\n",
            "Epoch 61: val_loss improved from 315.76181 to 315.38324, saving model to Weights-061--315.38324.hdf5\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 225.2823 - mean_absolute_error: 225.2823 - val_loss: 315.3832 - val_mean_absolute_error: 315.3832\n",
            "Epoch 62/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 226.7050 - mean_absolute_error: 226.7050\n",
            "Epoch 62: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 227.4158 - mean_absolute_error: 227.4158 - val_loss: 326.5153 - val_mean_absolute_error: 326.5153\n",
            "Epoch 63/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 227.5659 - mean_absolute_error: 227.5659\n",
            "Epoch 63: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 226.9345 - mean_absolute_error: 226.9345 - val_loss: 343.0113 - val_mean_absolute_error: 343.0113\n",
            "Epoch 64/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 233.9322 - mean_absolute_error: 233.9322\n",
            "Epoch 64: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 231.0965 - mean_absolute_error: 231.0965 - val_loss: 319.5599 - val_mean_absolute_error: 319.5599\n",
            "Epoch 65/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 236.7146 - mean_absolute_error: 236.7146\n",
            "Epoch 65: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 236.8779 - mean_absolute_error: 236.8779 - val_loss: 322.9149 - val_mean_absolute_error: 322.9149\n",
            "Epoch 66/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 222.1903 - mean_absolute_error: 222.1903\n",
            "Epoch 66: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 223.0851 - mean_absolute_error: 223.0851 - val_loss: 318.6133 - val_mean_absolute_error: 318.6133\n",
            "Epoch 67/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 233.7139 - mean_absolute_error: 233.7139\n",
            "Epoch 67: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 233.4157 - mean_absolute_error: 233.4157 - val_loss: 328.5573 - val_mean_absolute_error: 328.5573\n",
            "Epoch 68/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 245.5732 - mean_absolute_error: 245.5732\n",
            "Epoch 68: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 241.0698 - mean_absolute_error: 241.0698 - val_loss: 325.5070 - val_mean_absolute_error: 325.5070\n",
            "Epoch 69/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 226.8163 - mean_absolute_error: 226.8163\n",
            "Epoch 69: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 225.8032 - mean_absolute_error: 225.8032 - val_loss: 321.3690 - val_mean_absolute_error: 321.3690\n",
            "Epoch 70/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 227.9698 - mean_absolute_error: 227.9698\n",
            "Epoch 70: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 228.3262 - mean_absolute_error: 228.3262 - val_loss: 323.8720 - val_mean_absolute_error: 323.8720\n",
            "Epoch 71/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 226.4844 - mean_absolute_error: 226.4844\n",
            "Epoch 71: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 225.0247 - mean_absolute_error: 225.0247 - val_loss: 340.5663 - val_mean_absolute_error: 340.5663\n",
            "Epoch 72/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 235.8870 - mean_absolute_error: 235.8870\n",
            "Epoch 72: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 234.7450 - mean_absolute_error: 234.7450 - val_loss: 322.5613 - val_mean_absolute_error: 322.5613\n",
            "Epoch 73/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 226.7412 - mean_absolute_error: 226.7412\n",
            "Epoch 73: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 226.4624 - mean_absolute_error: 226.4624 - val_loss: 328.8935 - val_mean_absolute_error: 328.8935\n",
            "Epoch 74/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 226.1051 - mean_absolute_error: 226.1051\n",
            "Epoch 74: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 225.0636 - mean_absolute_error: 225.0636 - val_loss: 326.4357 - val_mean_absolute_error: 326.4357\n",
            "Epoch 75/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 217.6106 - mean_absolute_error: 217.6106\n",
            "Epoch 75: val_loss did not improve from 315.38324\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 217.7085 - mean_absolute_error: 217.7085 - val_loss: 346.3515 - val_mean_absolute_error: 346.3515\n",
            "Epoch 76/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 222.2561 - mean_absolute_error: 222.2561\n",
            "Epoch 76: val_loss improved from 315.38324 to 313.40759, saving model to Weights-076--313.40759.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 222.5975 - mean_absolute_error: 222.5975 - val_loss: 313.4076 - val_mean_absolute_error: 313.4076\n",
            "Epoch 77/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 222.1324 - mean_absolute_error: 222.1324\n",
            "Epoch 77: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 221.1072 - mean_absolute_error: 221.1072 - val_loss: 317.0616 - val_mean_absolute_error: 317.0616\n",
            "Epoch 78/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 221.7094 - mean_absolute_error: 221.7094\n",
            "Epoch 78: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 222.3625 - mean_absolute_error: 222.3625 - val_loss: 319.3363 - val_mean_absolute_error: 319.3363\n",
            "Epoch 79/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 226.1748 - mean_absolute_error: 226.1748\n",
            "Epoch 79: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 226.5915 - mean_absolute_error: 226.5915 - val_loss: 338.1165 - val_mean_absolute_error: 338.1165\n",
            "Epoch 80/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 223.3687 - mean_absolute_error: 223.3687\n",
            "Epoch 80: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 222.6086 - mean_absolute_error: 222.6086 - val_loss: 355.2349 - val_mean_absolute_error: 355.2349\n",
            "Epoch 81/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 221.1491 - mean_absolute_error: 221.1491\n",
            "Epoch 81: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 224.2012 - mean_absolute_error: 224.2012 - val_loss: 318.8250 - val_mean_absolute_error: 318.8250\n",
            "Epoch 82/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 218.2654 - mean_absolute_error: 218.2654\n",
            "Epoch 82: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 218.7525 - mean_absolute_error: 218.7525 - val_loss: 331.8590 - val_mean_absolute_error: 331.8590\n",
            "Epoch 83/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 216.8879 - mean_absolute_error: 216.8879\n",
            "Epoch 83: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 216.8879 - mean_absolute_error: 216.8879 - val_loss: 318.9517 - val_mean_absolute_error: 318.9517\n",
            "Epoch 84/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 226.9906 - mean_absolute_error: 226.9906\n",
            "Epoch 84: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 228.6777 - mean_absolute_error: 228.6777 - val_loss: 326.4177 - val_mean_absolute_error: 326.4177\n",
            "Epoch 85/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 218.3918 - mean_absolute_error: 218.3918\n",
            "Epoch 85: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 219.9482 - mean_absolute_error: 219.9482 - val_loss: 331.9880 - val_mean_absolute_error: 331.9880\n",
            "Epoch 86/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 216.4293 - mean_absolute_error: 216.4293\n",
            "Epoch 86: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 216.3109 - mean_absolute_error: 216.3109 - val_loss: 325.5680 - val_mean_absolute_error: 325.5680\n",
            "Epoch 87/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 217.1045 - mean_absolute_error: 217.1045\n",
            "Epoch 87: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 216.5886 - mean_absolute_error: 216.5886 - val_loss: 347.5167 - val_mean_absolute_error: 347.5167\n",
            "Epoch 88/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 214.2447 - mean_absolute_error: 214.2447\n",
            "Epoch 88: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 213.2560 - mean_absolute_error: 213.2560 - val_loss: 320.3493 - val_mean_absolute_error: 320.3493\n",
            "Epoch 89/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 204.7409 - mean_absolute_error: 204.7409\n",
            "Epoch 89: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 205.0347 - mean_absolute_error: 205.0347 - val_loss: 321.1408 - val_mean_absolute_error: 321.1408\n",
            "Epoch 90/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 220.3037 - mean_absolute_error: 220.3037\n",
            "Epoch 90: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 218.5338 - mean_absolute_error: 218.5338 - val_loss: 328.0881 - val_mean_absolute_error: 328.0881\n",
            "Epoch 91/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 211.3067 - mean_absolute_error: 211.3067\n",
            "Epoch 91: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 211.9246 - mean_absolute_error: 211.9246 - val_loss: 322.6101 - val_mean_absolute_error: 322.6101\n",
            "Epoch 92/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 218.5508 - mean_absolute_error: 218.5508\n",
            "Epoch 92: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 217.8755 - mean_absolute_error: 217.8755 - val_loss: 333.8477 - val_mean_absolute_error: 333.8477\n",
            "Epoch 93/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 217.8434 - mean_absolute_error: 217.8434\n",
            "Epoch 93: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 219.2995 - mean_absolute_error: 219.2995 - val_loss: 320.5050 - val_mean_absolute_error: 320.5050\n",
            "Epoch 94/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 207.2263 - mean_absolute_error: 207.2263\n",
            "Epoch 94: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 207.8652 - mean_absolute_error: 207.8652 - val_loss: 319.5413 - val_mean_absolute_error: 319.5413\n",
            "Epoch 95/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 201.0866 - mean_absolute_error: 201.0866\n",
            "Epoch 95: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 202.0082 - mean_absolute_error: 202.0082 - val_loss: 318.8751 - val_mean_absolute_error: 318.8751\n",
            "Epoch 96/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 209.9733 - mean_absolute_error: 209.9733\n",
            "Epoch 96: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 209.9733 - mean_absolute_error: 209.9733 - val_loss: 324.7997 - val_mean_absolute_error: 324.7997\n",
            "Epoch 97/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 217.5151 - mean_absolute_error: 217.5151\n",
            "Epoch 97: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 216.3549 - mean_absolute_error: 216.3549 - val_loss: 327.3215 - val_mean_absolute_error: 327.3215\n",
            "Epoch 98/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 221.6907 - mean_absolute_error: 221.6907\n",
            "Epoch 98: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 221.6907 - mean_absolute_error: 221.6907 - val_loss: 327.7827 - val_mean_absolute_error: 327.7827\n",
            "Epoch 99/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 216.4457 - mean_absolute_error: 216.4457\n",
            "Epoch 99: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 217.0272 - mean_absolute_error: 217.0272 - val_loss: 322.7725 - val_mean_absolute_error: 322.7725\n",
            "Epoch 100/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 207.2431 - mean_absolute_error: 207.2431\n",
            "Epoch 100: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 207.5829 - mean_absolute_error: 207.5829 - val_loss: 320.2372 - val_mean_absolute_error: 320.2372\n",
            "Epoch 101/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 216.3898 - mean_absolute_error: 216.3898\n",
            "Epoch 101: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 216.3898 - mean_absolute_error: 216.3898 - val_loss: 336.0307 - val_mean_absolute_error: 336.0307\n",
            "Epoch 102/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 212.7767 - mean_absolute_error: 212.7767\n",
            "Epoch 102: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 212.9016 - mean_absolute_error: 212.9016 - val_loss: 333.4454 - val_mean_absolute_error: 333.4454\n",
            "Epoch 103/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 207.8172 - mean_absolute_error: 207.8172\n",
            "Epoch 103: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 207.3676 - mean_absolute_error: 207.3676 - val_loss: 323.4428 - val_mean_absolute_error: 323.4428\n",
            "Epoch 104/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 209.3343 - mean_absolute_error: 209.3343\n",
            "Epoch 104: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 208.1280 - mean_absolute_error: 208.1280 - val_loss: 329.8928 - val_mean_absolute_error: 329.8928\n",
            "Epoch 105/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 213.4753 - mean_absolute_error: 213.4753\n",
            "Epoch 105: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 213.3412 - mean_absolute_error: 213.3412 - val_loss: 336.3558 - val_mean_absolute_error: 336.3558\n",
            "Epoch 106/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 216.8194 - mean_absolute_error: 216.8194\n",
            "Epoch 106: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 216.8194 - mean_absolute_error: 216.8194 - val_loss: 334.6368 - val_mean_absolute_error: 334.6368\n",
            "Epoch 107/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 207.4250 - mean_absolute_error: 207.4250\n",
            "Epoch 107: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 207.4250 - mean_absolute_error: 207.4250 - val_loss: 345.2873 - val_mean_absolute_error: 345.2873\n",
            "Epoch 108/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 213.5890 - mean_absolute_error: 213.5890\n",
            "Epoch 108: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 213.9322 - mean_absolute_error: 213.9322 - val_loss: 322.3811 - val_mean_absolute_error: 322.3811\n",
            "Epoch 109/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 217.5132 - mean_absolute_error: 217.5132\n",
            "Epoch 109: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 216.8610 - mean_absolute_error: 216.8610 - val_loss: 327.3833 - val_mean_absolute_error: 327.3833\n",
            "Epoch 110/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 220.4695 - mean_absolute_error: 220.4695\n",
            "Epoch 110: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 218.2818 - mean_absolute_error: 218.2818 - val_loss: 319.2542 - val_mean_absolute_error: 319.2542\n",
            "Epoch 111/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 219.7303 - mean_absolute_error: 219.7303\n",
            "Epoch 111: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 221.0173 - mean_absolute_error: 221.0173 - val_loss: 340.2159 - val_mean_absolute_error: 340.2159\n",
            "Epoch 112/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 211.2426 - mean_absolute_error: 211.2426\n",
            "Epoch 112: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 211.5999 - mean_absolute_error: 211.5999 - val_loss: 321.2470 - val_mean_absolute_error: 321.2470\n",
            "Epoch 113/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 216.0894 - mean_absolute_error: 216.0894\n",
            "Epoch 113: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 216.0221 - mean_absolute_error: 216.0221 - val_loss: 320.8576 - val_mean_absolute_error: 320.8576\n",
            "Epoch 114/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 206.5367 - mean_absolute_error: 206.5367\n",
            "Epoch 114: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 206.5013 - mean_absolute_error: 206.5013 - val_loss: 317.5290 - val_mean_absolute_error: 317.5290\n",
            "Epoch 115/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 205.3330 - mean_absolute_error: 205.3330\n",
            "Epoch 115: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 204.7894 - mean_absolute_error: 204.7894 - val_loss: 328.2592 - val_mean_absolute_error: 328.2592\n",
            "Epoch 116/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 220.3811 - mean_absolute_error: 220.3811\n",
            "Epoch 116: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 219.0005 - mean_absolute_error: 219.0005 - val_loss: 317.4740 - val_mean_absolute_error: 317.4740\n",
            "Epoch 117/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 210.9488 - mean_absolute_error: 210.9488\n",
            "Epoch 117: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 210.5647 - mean_absolute_error: 210.5647 - val_loss: 323.3120 - val_mean_absolute_error: 323.3120\n",
            "Epoch 118/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 205.2583 - mean_absolute_error: 205.2583\n",
            "Epoch 118: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 205.6685 - mean_absolute_error: 205.6685 - val_loss: 358.6924 - val_mean_absolute_error: 358.6924\n",
            "Epoch 119/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 214.3340 - mean_absolute_error: 214.3340\n",
            "Epoch 119: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 214.5300 - mean_absolute_error: 214.5300 - val_loss: 340.0142 - val_mean_absolute_error: 340.0142\n",
            "Epoch 120/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 214.2753 - mean_absolute_error: 214.2753\n",
            "Epoch 120: val_loss did not improve from 313.40759\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 214.2753 - mean_absolute_error: 214.2753 - val_loss: 327.3032 - val_mean_absolute_error: 327.3032\n",
            "Epoch 121/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 207.0734 - mean_absolute_error: 207.0734\n",
            "Epoch 121: val_loss improved from 313.40759 to 312.89746, saving model to Weights-121--312.89746.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 206.8537 - mean_absolute_error: 206.8537 - val_loss: 312.8975 - val_mean_absolute_error: 312.8975\n",
            "Epoch 122/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 209.1905 - mean_absolute_error: 209.1905\n",
            "Epoch 122: val_loss did not improve from 312.89746\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 207.2822 - mean_absolute_error: 207.2822 - val_loss: 318.8145 - val_mean_absolute_error: 318.8145\n",
            "Epoch 123/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 201.2688 - mean_absolute_error: 201.2688\n",
            "Epoch 123: val_loss did not improve from 312.89746\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 201.4466 - mean_absolute_error: 201.4466 - val_loss: 313.4132 - val_mean_absolute_error: 313.4132\n",
            "Epoch 124/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 204.6925 - mean_absolute_error: 204.6925\n",
            "Epoch 124: val_loss did not improve from 312.89746\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 205.3054 - mean_absolute_error: 205.3054 - val_loss: 326.6464 - val_mean_absolute_error: 326.6464\n",
            "Epoch 125/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 215.7646 - mean_absolute_error: 215.7646\n",
            "Epoch 125: val_loss did not improve from 312.89746\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 216.0388 - mean_absolute_error: 216.0388 - val_loss: 321.0391 - val_mean_absolute_error: 321.0391\n",
            "Epoch 126/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 207.5658 - mean_absolute_error: 207.5658\n",
            "Epoch 126: val_loss improved from 312.89746 to 310.92303, saving model to Weights-126--310.92303.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 207.5658 - mean_absolute_error: 207.5658 - val_loss: 310.9230 - val_mean_absolute_error: 310.9230\n",
            "Epoch 127/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 199.5245 - mean_absolute_error: 199.5245\n",
            "Epoch 127: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 199.4307 - mean_absolute_error: 199.4307 - val_loss: 318.1872 - val_mean_absolute_error: 318.1872\n",
            "Epoch 128/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 199.1638 - mean_absolute_error: 199.1638\n",
            "Epoch 128: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 198.9535 - mean_absolute_error: 198.9535 - val_loss: 313.3940 - val_mean_absolute_error: 313.3940\n",
            "Epoch 129/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 212.1914 - mean_absolute_error: 212.1914\n",
            "Epoch 129: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 211.9968 - mean_absolute_error: 211.9968 - val_loss: 340.4279 - val_mean_absolute_error: 340.4279\n",
            "Epoch 130/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 208.6798 - mean_absolute_error: 208.6798\n",
            "Epoch 130: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 208.2719 - mean_absolute_error: 208.2719 - val_loss: 322.6853 - val_mean_absolute_error: 322.6853\n",
            "Epoch 131/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 198.7697 - mean_absolute_error: 198.7697\n",
            "Epoch 131: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 196.8379 - mean_absolute_error: 196.8379 - val_loss: 314.2354 - val_mean_absolute_error: 314.2354\n",
            "Epoch 132/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 201.7763 - mean_absolute_error: 201.7763\n",
            "Epoch 132: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 201.8603 - mean_absolute_error: 201.8603 - val_loss: 322.8491 - val_mean_absolute_error: 322.8491\n",
            "Epoch 133/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 202.0692 - mean_absolute_error: 202.0692\n",
            "Epoch 133: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 202.2435 - mean_absolute_error: 202.2435 - val_loss: 328.1010 - val_mean_absolute_error: 328.1010\n",
            "Epoch 134/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 201.0141 - mean_absolute_error: 201.0141\n",
            "Epoch 134: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 201.1697 - mean_absolute_error: 201.1697 - val_loss: 319.7891 - val_mean_absolute_error: 319.7891\n",
            "Epoch 135/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 203.2915 - mean_absolute_error: 203.2915\n",
            "Epoch 135: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 202.9003 - mean_absolute_error: 202.9003 - val_loss: 326.2776 - val_mean_absolute_error: 326.2776\n",
            "Epoch 136/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 201.3923 - mean_absolute_error: 201.3923\n",
            "Epoch 136: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 199.8632 - mean_absolute_error: 199.8632 - val_loss: 319.8300 - val_mean_absolute_error: 319.8300\n",
            "Epoch 137/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 199.0740 - mean_absolute_error: 199.0740\n",
            "Epoch 137: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 199.2318 - mean_absolute_error: 199.2318 - val_loss: 326.8692 - val_mean_absolute_error: 326.8692\n",
            "Epoch 138/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 196.2260 - mean_absolute_error: 196.2260\n",
            "Epoch 138: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 197.5573 - mean_absolute_error: 197.5573 - val_loss: 321.2995 - val_mean_absolute_error: 321.2995\n",
            "Epoch 139/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 192.6661 - mean_absolute_error: 192.6661\n",
            "Epoch 139: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 194.6182 - mean_absolute_error: 194.6182 - val_loss: 316.8245 - val_mean_absolute_error: 316.8245\n",
            "Epoch 140/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 207.2107 - mean_absolute_error: 207.2107\n",
            "Epoch 140: val_loss did not improve from 310.92303\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 206.8615 - mean_absolute_error: 206.8615 - val_loss: 320.0724 - val_mean_absolute_error: 320.0724\n",
            "Epoch 141/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 202.6240 - mean_absolute_error: 202.6240\n",
            "Epoch 141: val_loss improved from 310.92303 to 310.01163, saving model to Weights-141--310.01163.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 201.9920 - mean_absolute_error: 201.9920 - val_loss: 310.0116 - val_mean_absolute_error: 310.0116\n",
            "Epoch 142/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 203.1850 - mean_absolute_error: 203.1850\n",
            "Epoch 142: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 204.2107 - mean_absolute_error: 204.2107 - val_loss: 317.8138 - val_mean_absolute_error: 317.8138\n",
            "Epoch 143/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 202.3170 - mean_absolute_error: 202.3170\n",
            "Epoch 143: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 201.1127 - mean_absolute_error: 201.1127 - val_loss: 344.3827 - val_mean_absolute_error: 344.3827\n",
            "Epoch 144/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 203.5016 - mean_absolute_error: 203.5016\n",
            "Epoch 144: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 203.4746 - mean_absolute_error: 203.4746 - val_loss: 336.0589 - val_mean_absolute_error: 336.0589\n",
            "Epoch 145/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 204.1214 - mean_absolute_error: 204.1214\n",
            "Epoch 145: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 204.2015 - mean_absolute_error: 204.2015 - val_loss: 332.9748 - val_mean_absolute_error: 332.9748\n",
            "Epoch 146/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 203.9237 - mean_absolute_error: 203.9237\n",
            "Epoch 146: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 204.0573 - mean_absolute_error: 204.0573 - val_loss: 317.3266 - val_mean_absolute_error: 317.3266\n",
            "Epoch 147/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 194.2641 - mean_absolute_error: 194.2641\n",
            "Epoch 147: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 194.1252 - mean_absolute_error: 194.1252 - val_loss: 340.3481 - val_mean_absolute_error: 340.3481\n",
            "Epoch 148/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 192.7583 - mean_absolute_error: 192.7583\n",
            "Epoch 148: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 193.1991 - mean_absolute_error: 193.1991 - val_loss: 328.5529 - val_mean_absolute_error: 328.5529\n",
            "Epoch 149/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 203.1894 - mean_absolute_error: 203.1894\n",
            "Epoch 149: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 203.5323 - mean_absolute_error: 203.5323 - val_loss: 316.4991 - val_mean_absolute_error: 316.4991\n",
            "Epoch 150/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 195.8253 - mean_absolute_error: 195.8253\n",
            "Epoch 150: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 195.6170 - mean_absolute_error: 195.6170 - val_loss: 318.8676 - val_mean_absolute_error: 318.8676\n",
            "Epoch 151/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 199.8248 - mean_absolute_error: 199.8248\n",
            "Epoch 151: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 201.0538 - mean_absolute_error: 201.0538 - val_loss: 320.1089 - val_mean_absolute_error: 320.1089\n",
            "Epoch 152/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 197.8579 - mean_absolute_error: 197.8579\n",
            "Epoch 152: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 198.2118 - mean_absolute_error: 198.2118 - val_loss: 319.5243 - val_mean_absolute_error: 319.5243\n",
            "Epoch 153/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 201.9451 - mean_absolute_error: 201.9451\n",
            "Epoch 153: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 201.4048 - mean_absolute_error: 201.4048 - val_loss: 315.4147 - val_mean_absolute_error: 315.4147\n",
            "Epoch 154/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 200.6562 - mean_absolute_error: 200.6562\n",
            "Epoch 154: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 202.3764 - mean_absolute_error: 202.3764 - val_loss: 356.5693 - val_mean_absolute_error: 356.5693\n",
            "Epoch 155/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 197.0689 - mean_absolute_error: 197.0689\n",
            "Epoch 155: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 195.9656 - mean_absolute_error: 195.9656 - val_loss: 314.4528 - val_mean_absolute_error: 314.4528\n",
            "Epoch 156/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 191.3059 - mean_absolute_error: 191.3059\n",
            "Epoch 156: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 192.3602 - mean_absolute_error: 192.3602 - val_loss: 319.2302 - val_mean_absolute_error: 319.2302\n",
            "Epoch 157/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 187.4917 - mean_absolute_error: 187.4917\n",
            "Epoch 157: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 187.4917 - mean_absolute_error: 187.4917 - val_loss: 316.7905 - val_mean_absolute_error: 316.7905\n",
            "Epoch 158/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 193.2567 - mean_absolute_error: 193.2567\n",
            "Epoch 158: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 193.6164 - mean_absolute_error: 193.6164 - val_loss: 312.2775 - val_mean_absolute_error: 312.2775\n",
            "Epoch 159/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 191.0244 - mean_absolute_error: 191.0244\n",
            "Epoch 159: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 192.0525 - mean_absolute_error: 192.0525 - val_loss: 338.1754 - val_mean_absolute_error: 338.1754\n",
            "Epoch 160/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 192.5048 - mean_absolute_error: 192.5048\n",
            "Epoch 160: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 191.6254 - mean_absolute_error: 191.6254 - val_loss: 312.7682 - val_mean_absolute_error: 312.7682\n",
            "Epoch 161/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 188.6582 - mean_absolute_error: 188.6582\n",
            "Epoch 161: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 188.6469 - mean_absolute_error: 188.6469 - val_loss: 316.2678 - val_mean_absolute_error: 316.2678\n",
            "Epoch 162/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 189.4086 - mean_absolute_error: 189.4086\n",
            "Epoch 162: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 191.4970 - mean_absolute_error: 191.4970 - val_loss: 341.6404 - val_mean_absolute_error: 341.6404\n",
            "Epoch 163/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 205.9732 - mean_absolute_error: 205.9732\n",
            "Epoch 163: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 205.7776 - mean_absolute_error: 205.7776 - val_loss: 315.6759 - val_mean_absolute_error: 315.6759\n",
            "Epoch 164/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 189.5420 - mean_absolute_error: 189.5420\n",
            "Epoch 164: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 189.7556 - mean_absolute_error: 189.7556 - val_loss: 314.6153 - val_mean_absolute_error: 314.6153\n",
            "Epoch 165/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 189.6969 - mean_absolute_error: 189.6969\n",
            "Epoch 165: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 188.8554 - mean_absolute_error: 188.8554 - val_loss: 322.4694 - val_mean_absolute_error: 322.4694\n",
            "Epoch 166/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 189.0641 - mean_absolute_error: 189.0641\n",
            "Epoch 166: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 188.6808 - mean_absolute_error: 188.6808 - val_loss: 316.6025 - val_mean_absolute_error: 316.6025\n",
            "Epoch 167/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 189.2765 - mean_absolute_error: 189.2765\n",
            "Epoch 167: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 188.9494 - mean_absolute_error: 188.9494 - val_loss: 313.3265 - val_mean_absolute_error: 313.3265\n",
            "Epoch 168/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 194.0808 - mean_absolute_error: 194.0808\n",
            "Epoch 168: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 192.1112 - mean_absolute_error: 192.1112 - val_loss: 319.9292 - val_mean_absolute_error: 319.9292\n",
            "Epoch 169/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 192.8552 - mean_absolute_error: 192.8552\n",
            "Epoch 169: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 190.1339 - mean_absolute_error: 190.1339 - val_loss: 319.3666 - val_mean_absolute_error: 319.3666\n",
            "Epoch 170/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 206.6682 - mean_absolute_error: 206.6682\n",
            "Epoch 170: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 207.3430 - mean_absolute_error: 207.3430 - val_loss: 325.5252 - val_mean_absolute_error: 325.5252\n",
            "Epoch 171/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 202.0432 - mean_absolute_error: 202.0432\n",
            "Epoch 171: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 201.4659 - mean_absolute_error: 201.4659 - val_loss: 339.3841 - val_mean_absolute_error: 339.3841\n",
            "Epoch 172/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 189.5607 - mean_absolute_error: 189.5607\n",
            "Epoch 172: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 189.4491 - mean_absolute_error: 189.4491 - val_loss: 331.0133 - val_mean_absolute_error: 331.0133\n",
            "Epoch 173/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 189.1190 - mean_absolute_error: 189.1190\n",
            "Epoch 173: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 191.0290 - mean_absolute_error: 191.0290 - val_loss: 330.3776 - val_mean_absolute_error: 330.3776\n",
            "Epoch 174/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 199.9706 - mean_absolute_error: 199.9706\n",
            "Epoch 174: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 200.1110 - mean_absolute_error: 200.1110 - val_loss: 322.3393 - val_mean_absolute_error: 322.3393\n",
            "Epoch 175/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 185.1082 - mean_absolute_error: 185.1082\n",
            "Epoch 175: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 185.5650 - mean_absolute_error: 185.5650 - val_loss: 321.0322 - val_mean_absolute_error: 321.0322\n",
            "Epoch 176/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 183.8755 - mean_absolute_error: 183.8755\n",
            "Epoch 176: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 184.7773 - mean_absolute_error: 184.7773 - val_loss: 312.7413 - val_mean_absolute_error: 312.7413\n",
            "Epoch 177/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 196.0858 - mean_absolute_error: 196.0858\n",
            "Epoch 177: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 196.3392 - mean_absolute_error: 196.3392 - val_loss: 339.7473 - val_mean_absolute_error: 339.7473\n",
            "Epoch 178/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 192.2456 - mean_absolute_error: 192.2456\n",
            "Epoch 178: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 191.9663 - mean_absolute_error: 191.9663 - val_loss: 320.4904 - val_mean_absolute_error: 320.4904\n",
            "Epoch 179/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 186.4991 - mean_absolute_error: 186.4991\n",
            "Epoch 179: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 185.3915 - mean_absolute_error: 185.3915 - val_loss: 321.3483 - val_mean_absolute_error: 321.3483\n",
            "Epoch 180/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 189.8957 - mean_absolute_error: 189.8957\n",
            "Epoch 180: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 190.1581 - mean_absolute_error: 190.1581 - val_loss: 327.3357 - val_mean_absolute_error: 327.3357\n",
            "Epoch 181/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 200.3635 - mean_absolute_error: 200.3635\n",
            "Epoch 181: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 199.9319 - mean_absolute_error: 199.9319 - val_loss: 314.7406 - val_mean_absolute_error: 314.7406\n",
            "Epoch 182/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 192.8862 - mean_absolute_error: 192.8862\n",
            "Epoch 182: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 192.4530 - mean_absolute_error: 192.4530 - val_loss: 311.3873 - val_mean_absolute_error: 311.3873\n",
            "Epoch 183/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 184.4778 - mean_absolute_error: 184.4778\n",
            "Epoch 183: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 183.8795 - mean_absolute_error: 183.8795 - val_loss: 324.9060 - val_mean_absolute_error: 324.9060\n",
            "Epoch 184/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 192.3192 - mean_absolute_error: 192.3192\n",
            "Epoch 184: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 190.0358 - mean_absolute_error: 190.0358 - val_loss: 327.7469 - val_mean_absolute_error: 327.7469\n",
            "Epoch 185/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 193.8603 - mean_absolute_error: 193.8603\n",
            "Epoch 185: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 192.7869 - mean_absolute_error: 192.7869 - val_loss: 314.1216 - val_mean_absolute_error: 314.1216\n",
            "Epoch 186/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 186.8369 - mean_absolute_error: 186.8369\n",
            "Epoch 186: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 187.4863 - mean_absolute_error: 187.4863 - val_loss: 313.0981 - val_mean_absolute_error: 313.0981\n",
            "Epoch 187/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 180.7563 - mean_absolute_error: 180.7563\n",
            "Epoch 187: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 180.7934 - mean_absolute_error: 180.7934 - val_loss: 319.2683 - val_mean_absolute_error: 319.2683\n",
            "Epoch 188/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 195.7993 - mean_absolute_error: 195.7993\n",
            "Epoch 188: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 196.2444 - mean_absolute_error: 196.2444 - val_loss: 348.6053 - val_mean_absolute_error: 348.6053\n",
            "Epoch 189/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 197.5154 - mean_absolute_error: 197.5154\n",
            "Epoch 189: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 197.9746 - mean_absolute_error: 197.9746 - val_loss: 312.5492 - val_mean_absolute_error: 312.5492\n",
            "Epoch 190/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 180.4662 - mean_absolute_error: 180.4662\n",
            "Epoch 190: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 179.9593 - mean_absolute_error: 179.9593 - val_loss: 322.8454 - val_mean_absolute_error: 322.8454\n",
            "Epoch 191/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 192.6576 - mean_absolute_error: 192.6576\n",
            "Epoch 191: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 193.2672 - mean_absolute_error: 193.2672 - val_loss: 335.8419 - val_mean_absolute_error: 335.8419\n",
            "Epoch 192/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 178.5164 - mean_absolute_error: 178.5164\n",
            "Epoch 192: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 178.7473 - mean_absolute_error: 178.7473 - val_loss: 320.3933 - val_mean_absolute_error: 320.3933\n",
            "Epoch 193/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 185.5733 - mean_absolute_error: 185.5733\n",
            "Epoch 193: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 185.7435 - mean_absolute_error: 185.7435 - val_loss: 317.3800 - val_mean_absolute_error: 317.3800\n",
            "Epoch 194/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 175.6756 - mean_absolute_error: 175.6756\n",
            "Epoch 194: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 175.9282 - mean_absolute_error: 175.9282 - val_loss: 331.4190 - val_mean_absolute_error: 331.4190\n",
            "Epoch 195/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 189.5298 - mean_absolute_error: 189.5298\n",
            "Epoch 195: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 189.4744 - mean_absolute_error: 189.4744 - val_loss: 315.6404 - val_mean_absolute_error: 315.6404\n",
            "Epoch 196/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 179.1812 - mean_absolute_error: 179.1812\n",
            "Epoch 196: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 179.0063 - mean_absolute_error: 179.0063 - val_loss: 318.4345 - val_mean_absolute_error: 318.4345\n",
            "Epoch 197/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 189.4217 - mean_absolute_error: 189.4217\n",
            "Epoch 197: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 190.3802 - mean_absolute_error: 190.3802 - val_loss: 316.3427 - val_mean_absolute_error: 316.3427\n",
            "Epoch 198/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 179.7439 - mean_absolute_error: 179.7439\n",
            "Epoch 198: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 179.6491 - mean_absolute_error: 179.6491 - val_loss: 324.6893 - val_mean_absolute_error: 324.6893\n",
            "Epoch 199/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 176.7479 - mean_absolute_error: 176.7479\n",
            "Epoch 199: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 179.0905 - mean_absolute_error: 179.0905 - val_loss: 362.7370 - val_mean_absolute_error: 362.7370\n",
            "Epoch 200/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 198.1370 - mean_absolute_error: 198.1370\n",
            "Epoch 200: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 197.4954 - mean_absolute_error: 197.4954 - val_loss: 314.0925 - val_mean_absolute_error: 314.0925\n",
            "Epoch 201/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 178.8991 - mean_absolute_error: 178.8991\n",
            "Epoch 201: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 179.5443 - mean_absolute_error: 179.5443 - val_loss: 315.0557 - val_mean_absolute_error: 315.0557\n",
            "Epoch 202/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 181.0954 - mean_absolute_error: 181.0954\n",
            "Epoch 202: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 181.1702 - mean_absolute_error: 181.1702 - val_loss: 319.6184 - val_mean_absolute_error: 319.6184\n",
            "Epoch 203/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 186.1110 - mean_absolute_error: 186.1110\n",
            "Epoch 203: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 186.1818 - mean_absolute_error: 186.1818 - val_loss: 325.1223 - val_mean_absolute_error: 325.1223\n",
            "Epoch 204/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 180.6817 - mean_absolute_error: 180.6817\n",
            "Epoch 204: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 179.9445 - mean_absolute_error: 179.9445 - val_loss: 353.5972 - val_mean_absolute_error: 353.5972\n",
            "Epoch 205/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 183.5849 - mean_absolute_error: 183.5849\n",
            "Epoch 205: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 183.8277 - mean_absolute_error: 183.8277 - val_loss: 312.8973 - val_mean_absolute_error: 312.8973\n",
            "Epoch 206/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 182.3741 - mean_absolute_error: 182.3741\n",
            "Epoch 206: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 182.8006 - mean_absolute_error: 182.8006 - val_loss: 315.9356 - val_mean_absolute_error: 315.9356\n",
            "Epoch 207/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 173.4301 - mean_absolute_error: 173.4301\n",
            "Epoch 207: val_loss did not improve from 310.01163\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 172.6808 - mean_absolute_error: 172.6808 - val_loss: 320.3571 - val_mean_absolute_error: 320.3571\n",
            "Epoch 208/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 173.7283 - mean_absolute_error: 173.7283\n",
            "Epoch 208: val_loss improved from 310.01163 to 303.21402, saving model to Weights-208--303.21402.hdf5\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 173.7807 - mean_absolute_error: 173.7807 - val_loss: 303.2140 - val_mean_absolute_error: 303.2140\n",
            "Epoch 209/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 180.9234 - mean_absolute_error: 180.9234\n",
            "Epoch 209: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 180.4752 - mean_absolute_error: 180.4752 - val_loss: 320.2445 - val_mean_absolute_error: 320.2445\n",
            "Epoch 210/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 177.4027 - mean_absolute_error: 177.4027\n",
            "Epoch 210: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 177.0254 - mean_absolute_error: 177.0254 - val_loss: 319.7254 - val_mean_absolute_error: 319.7254\n",
            "Epoch 211/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 175.9766 - mean_absolute_error: 175.9766\n",
            "Epoch 211: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 176.7083 - mean_absolute_error: 176.7083 - val_loss: 320.0102 - val_mean_absolute_error: 320.0102\n",
            "Epoch 212/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 186.4010 - mean_absolute_error: 186.4010\n",
            "Epoch 212: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 186.9426 - mean_absolute_error: 186.9426 - val_loss: 309.7946 - val_mean_absolute_error: 309.7946\n",
            "Epoch 213/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 179.7760 - mean_absolute_error: 179.7760\n",
            "Epoch 213: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 178.5630 - mean_absolute_error: 178.5630 - val_loss: 316.9861 - val_mean_absolute_error: 316.9861\n",
            "Epoch 214/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 171.4968 - mean_absolute_error: 171.4968\n",
            "Epoch 214: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 171.9781 - mean_absolute_error: 171.9781 - val_loss: 313.6862 - val_mean_absolute_error: 313.6862\n",
            "Epoch 215/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 170.2247 - mean_absolute_error: 170.2247\n",
            "Epoch 215: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 172.1109 - mean_absolute_error: 172.1109 - val_loss: 315.2679 - val_mean_absolute_error: 315.2679\n",
            "Epoch 216/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 175.2911 - mean_absolute_error: 175.2911\n",
            "Epoch 216: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 173.8710 - mean_absolute_error: 173.8710 - val_loss: 309.4788 - val_mean_absolute_error: 309.4788\n",
            "Epoch 217/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 169.9588 - mean_absolute_error: 169.9588\n",
            "Epoch 217: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 170.4401 - mean_absolute_error: 170.4401 - val_loss: 318.4751 - val_mean_absolute_error: 318.4751\n",
            "Epoch 218/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 178.8869 - mean_absolute_error: 178.8869\n",
            "Epoch 218: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 178.5288 - mean_absolute_error: 178.5288 - val_loss: 324.1222 - val_mean_absolute_error: 324.1222\n",
            "Epoch 219/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 169.2480 - mean_absolute_error: 169.2480\n",
            "Epoch 219: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 170.2845 - mean_absolute_error: 170.2845 - val_loss: 313.8270 - val_mean_absolute_error: 313.8270\n",
            "Epoch 220/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 171.0729 - mean_absolute_error: 171.0729\n",
            "Epoch 220: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 171.4009 - mean_absolute_error: 171.4009 - val_loss: 317.7614 - val_mean_absolute_error: 317.7614\n",
            "Epoch 221/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 180.1275 - mean_absolute_error: 180.1275\n",
            "Epoch 221: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 179.6170 - mean_absolute_error: 179.6170 - val_loss: 332.5811 - val_mean_absolute_error: 332.5811\n",
            "Epoch 222/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 174.5229 - mean_absolute_error: 174.5229\n",
            "Epoch 222: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 174.8455 - mean_absolute_error: 174.8455 - val_loss: 314.7748 - val_mean_absolute_error: 314.7748\n",
            "Epoch 223/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 172.4270 - mean_absolute_error: 172.4270\n",
            "Epoch 223: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 172.0113 - mean_absolute_error: 172.0113 - val_loss: 319.2439 - val_mean_absolute_error: 319.2439\n",
            "Epoch 224/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 171.5021 - mean_absolute_error: 171.5021\n",
            "Epoch 224: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 171.4861 - mean_absolute_error: 171.4861 - val_loss: 312.3860 - val_mean_absolute_error: 312.3860\n",
            "Epoch 225/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 172.2870 - mean_absolute_error: 172.2870\n",
            "Epoch 225: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 172.1136 - mean_absolute_error: 172.1136 - val_loss: 336.6307 - val_mean_absolute_error: 336.6307\n",
            "Epoch 226/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 187.7865 - mean_absolute_error: 187.7865\n",
            "Epoch 226: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 186.2704 - mean_absolute_error: 186.2704 - val_loss: 325.7361 - val_mean_absolute_error: 325.7361\n",
            "Epoch 227/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 176.2878 - mean_absolute_error: 176.2878\n",
            "Epoch 227: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 176.2231 - mean_absolute_error: 176.2231 - val_loss: 329.2332 - val_mean_absolute_error: 329.2332\n",
            "Epoch 228/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 173.0982 - mean_absolute_error: 173.0982\n",
            "Epoch 228: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 174.4416 - mean_absolute_error: 174.4416 - val_loss: 349.2836 - val_mean_absolute_error: 349.2836\n",
            "Epoch 229/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 176.0285 - mean_absolute_error: 176.0285\n",
            "Epoch 229: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 176.2512 - mean_absolute_error: 176.2512 - val_loss: 315.2063 - val_mean_absolute_error: 315.2063\n",
            "Epoch 230/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 171.1798 - mean_absolute_error: 171.1798\n",
            "Epoch 230: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 171.0479 - mean_absolute_error: 171.0479 - val_loss: 315.8620 - val_mean_absolute_error: 315.8620\n",
            "Epoch 231/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 167.3676 - mean_absolute_error: 167.3676\n",
            "Epoch 231: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 167.7030 - mean_absolute_error: 167.7030 - val_loss: 318.3395 - val_mean_absolute_error: 318.3395\n",
            "Epoch 232/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 170.5172 - mean_absolute_error: 170.5172\n",
            "Epoch 232: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 171.2770 - mean_absolute_error: 171.2770 - val_loss: 319.5730 - val_mean_absolute_error: 319.5730\n",
            "Epoch 233/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 171.5890 - mean_absolute_error: 171.5890\n",
            "Epoch 233: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 173.0396 - mean_absolute_error: 173.0396 - val_loss: 318.4325 - val_mean_absolute_error: 318.4325\n",
            "Epoch 234/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 174.6574 - mean_absolute_error: 174.6574\n",
            "Epoch 234: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 174.4550 - mean_absolute_error: 174.4550 - val_loss: 308.7119 - val_mean_absolute_error: 308.7119\n",
            "Epoch 235/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 169.0299 - mean_absolute_error: 169.0299\n",
            "Epoch 235: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 168.1239 - mean_absolute_error: 168.1239 - val_loss: 321.7174 - val_mean_absolute_error: 321.7174\n",
            "Epoch 236/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 169.5236 - mean_absolute_error: 169.5236\n",
            "Epoch 236: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 169.6307 - mean_absolute_error: 169.6307 - val_loss: 316.8831 - val_mean_absolute_error: 316.8831\n",
            "Epoch 237/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 169.5884 - mean_absolute_error: 169.5884\n",
            "Epoch 237: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 171.8467 - mean_absolute_error: 171.8467 - val_loss: 330.7121 - val_mean_absolute_error: 330.7121\n",
            "Epoch 238/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 169.7254 - mean_absolute_error: 169.7254\n",
            "Epoch 238: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 169.5315 - mean_absolute_error: 169.5315 - val_loss: 310.7288 - val_mean_absolute_error: 310.7288\n",
            "Epoch 239/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 174.5951 - mean_absolute_error: 174.5951\n",
            "Epoch 239: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 173.4667 - mean_absolute_error: 173.4667 - val_loss: 323.0012 - val_mean_absolute_error: 323.0012\n",
            "Epoch 240/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 167.6314 - mean_absolute_error: 167.6314\n",
            "Epoch 240: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 167.3775 - mean_absolute_error: 167.3775 - val_loss: 314.8086 - val_mean_absolute_error: 314.8086\n",
            "Epoch 241/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 172.4022 - mean_absolute_error: 172.4022\n",
            "Epoch 241: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 173.1258 - mean_absolute_error: 173.1258 - val_loss: 317.0359 - val_mean_absolute_error: 317.0359\n",
            "Epoch 242/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 166.5622 - mean_absolute_error: 166.5622\n",
            "Epoch 242: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 165.9666 - mean_absolute_error: 165.9666 - val_loss: 316.8251 - val_mean_absolute_error: 316.8251\n",
            "Epoch 243/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 168.3555 - mean_absolute_error: 168.3555\n",
            "Epoch 243: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 168.6568 - mean_absolute_error: 168.6568 - val_loss: 320.7121 - val_mean_absolute_error: 320.7121\n",
            "Epoch 244/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 167.7056 - mean_absolute_error: 167.7056\n",
            "Epoch 244: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 167.6005 - mean_absolute_error: 167.6005 - val_loss: 314.3091 - val_mean_absolute_error: 314.3091\n",
            "Epoch 245/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 164.8533 - mean_absolute_error: 164.8533\n",
            "Epoch 245: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 167.2849 - mean_absolute_error: 167.2849 - val_loss: 340.7867 - val_mean_absolute_error: 340.7867\n",
            "Epoch 246/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 171.1201 - mean_absolute_error: 171.1201\n",
            "Epoch 246: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 170.0318 - mean_absolute_error: 170.0318 - val_loss: 313.0779 - val_mean_absolute_error: 313.0779\n",
            "Epoch 247/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 159.1928 - mean_absolute_error: 159.1928\n",
            "Epoch 247: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 159.3575 - mean_absolute_error: 159.3575 - val_loss: 313.8851 - val_mean_absolute_error: 313.8851\n",
            "Epoch 248/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 161.4885 - mean_absolute_error: 161.4885\n",
            "Epoch 248: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 162.6879 - mean_absolute_error: 162.6879 - val_loss: 312.0221 - val_mean_absolute_error: 312.0221\n",
            "Epoch 249/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 172.0974 - mean_absolute_error: 172.0974\n",
            "Epoch 249: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 172.0974 - mean_absolute_error: 172.0974 - val_loss: 321.9947 - val_mean_absolute_error: 321.9947\n",
            "Epoch 250/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 157.4085 - mean_absolute_error: 157.4085\n",
            "Epoch 250: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 157.6372 - mean_absolute_error: 157.6372 - val_loss: 315.3757 - val_mean_absolute_error: 315.3757\n",
            "Epoch 251/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 159.2732 - mean_absolute_error: 159.2732\n",
            "Epoch 251: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 158.9358 - mean_absolute_error: 158.9358 - val_loss: 315.3824 - val_mean_absolute_error: 315.3824\n",
            "Epoch 252/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 170.4809 - mean_absolute_error: 170.4809\n",
            "Epoch 252: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 169.7531 - mean_absolute_error: 169.7531 - val_loss: 341.9800 - val_mean_absolute_error: 341.9800\n",
            "Epoch 253/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 166.4045 - mean_absolute_error: 166.4045\n",
            "Epoch 253: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 166.4045 - mean_absolute_error: 166.4045 - val_loss: 327.2268 - val_mean_absolute_error: 327.2268\n",
            "Epoch 254/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 169.6091 - mean_absolute_error: 169.6091\n",
            "Epoch 254: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 169.4675 - mean_absolute_error: 169.4675 - val_loss: 319.1287 - val_mean_absolute_error: 319.1287\n",
            "Epoch 255/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 166.7001 - mean_absolute_error: 166.7001\n",
            "Epoch 255: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 166.5327 - mean_absolute_error: 166.5327 - val_loss: 315.9951 - val_mean_absolute_error: 315.9951\n",
            "Epoch 256/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 163.3050 - mean_absolute_error: 163.3050\n",
            "Epoch 256: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 162.6685 - mean_absolute_error: 162.6685 - val_loss: 315.5436 - val_mean_absolute_error: 315.5436\n",
            "Epoch 257/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 160.6260 - mean_absolute_error: 160.6260\n",
            "Epoch 257: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 160.0833 - mean_absolute_error: 160.0833 - val_loss: 311.0703 - val_mean_absolute_error: 311.0703\n",
            "Epoch 258/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 157.6974 - mean_absolute_error: 157.6974\n",
            "Epoch 258: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 157.6839 - mean_absolute_error: 157.6839 - val_loss: 328.6465 - val_mean_absolute_error: 328.6465\n",
            "Epoch 259/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 169.2283 - mean_absolute_error: 169.2283\n",
            "Epoch 259: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 167.2923 - mean_absolute_error: 167.2923 - val_loss: 312.9011 - val_mean_absolute_error: 312.9011\n",
            "Epoch 260/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 159.4148 - mean_absolute_error: 159.4148\n",
            "Epoch 260: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 158.8727 - mean_absolute_error: 158.8727 - val_loss: 315.5631 - val_mean_absolute_error: 315.5631\n",
            "Epoch 261/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 168.7108 - mean_absolute_error: 168.7108\n",
            "Epoch 261: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 168.7974 - mean_absolute_error: 168.7974 - val_loss: 326.5254 - val_mean_absolute_error: 326.5254\n",
            "Epoch 262/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 169.0667 - mean_absolute_error: 169.0667\n",
            "Epoch 262: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 167.8526 - mean_absolute_error: 167.8526 - val_loss: 310.8818 - val_mean_absolute_error: 310.8818\n",
            "Epoch 263/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 161.1235 - mean_absolute_error: 161.1235\n",
            "Epoch 263: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 160.7176 - mean_absolute_error: 160.7176 - val_loss: 321.4042 - val_mean_absolute_error: 321.4042\n",
            "Epoch 264/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 156.5971 - mean_absolute_error: 156.5971\n",
            "Epoch 264: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 157.5119 - mean_absolute_error: 157.5119 - val_loss: 314.1866 - val_mean_absolute_error: 314.1866\n",
            "Epoch 265/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 161.0365 - mean_absolute_error: 161.0365\n",
            "Epoch 265: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 161.1449 - mean_absolute_error: 161.1449 - val_loss: 309.7570 - val_mean_absolute_error: 309.7570\n",
            "Epoch 266/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 159.7192 - mean_absolute_error: 159.7192\n",
            "Epoch 266: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 159.7192 - mean_absolute_error: 159.7192 - val_loss: 327.4492 - val_mean_absolute_error: 327.4492\n",
            "Epoch 267/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 157.2495 - mean_absolute_error: 157.2495\n",
            "Epoch 267: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 156.3604 - mean_absolute_error: 156.3604 - val_loss: 313.2099 - val_mean_absolute_error: 313.2099\n",
            "Epoch 268/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 156.5993 - mean_absolute_error: 156.5993\n",
            "Epoch 268: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 157.3890 - mean_absolute_error: 157.3890 - val_loss: 321.5005 - val_mean_absolute_error: 321.5005\n",
            "Epoch 269/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 159.4029 - mean_absolute_error: 159.4029\n",
            "Epoch 269: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 158.7543 - mean_absolute_error: 158.7543 - val_loss: 321.1891 - val_mean_absolute_error: 321.1891\n",
            "Epoch 270/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 163.5908 - mean_absolute_error: 163.5908\n",
            "Epoch 270: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 166.9904 - mean_absolute_error: 166.9904 - val_loss: 328.7174 - val_mean_absolute_error: 328.7174\n",
            "Epoch 271/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 161.9701 - mean_absolute_error: 161.9701\n",
            "Epoch 271: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 160.3801 - mean_absolute_error: 160.3801 - val_loss: 314.3434 - val_mean_absolute_error: 314.3434\n",
            "Epoch 272/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 164.6750 - mean_absolute_error: 164.6750\n",
            "Epoch 272: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 164.6750 - mean_absolute_error: 164.6750 - val_loss: 332.2411 - val_mean_absolute_error: 332.2411\n",
            "Epoch 273/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 150.2036 - mean_absolute_error: 150.2036\n",
            "Epoch 273: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 151.6588 - mean_absolute_error: 151.6588 - val_loss: 315.3417 - val_mean_absolute_error: 315.3417\n",
            "Epoch 274/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 160.8428 - mean_absolute_error: 160.8428\n",
            "Epoch 274: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 160.7135 - mean_absolute_error: 160.7135 - val_loss: 316.9893 - val_mean_absolute_error: 316.9893\n",
            "Epoch 275/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 163.6178 - mean_absolute_error: 163.6178\n",
            "Epoch 275: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 162.9704 - mean_absolute_error: 162.9704 - val_loss: 338.4496 - val_mean_absolute_error: 338.4496\n",
            "Epoch 276/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 159.8007 - mean_absolute_error: 159.8007\n",
            "Epoch 276: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 160.3419 - mean_absolute_error: 160.3419 - val_loss: 313.5634 - val_mean_absolute_error: 313.5634\n",
            "Epoch 277/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 150.1874 - mean_absolute_error: 150.1874\n",
            "Epoch 277: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 150.1039 - mean_absolute_error: 150.1039 - val_loss: 346.0909 - val_mean_absolute_error: 346.0909\n",
            "Epoch 278/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 172.2767 - mean_absolute_error: 172.2767\n",
            "Epoch 278: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 170.4044 - mean_absolute_error: 170.4044 - val_loss: 318.0102 - val_mean_absolute_error: 318.0102\n",
            "Epoch 279/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 160.1333 - mean_absolute_error: 160.1333\n",
            "Epoch 279: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 159.5139 - mean_absolute_error: 159.5139 - val_loss: 309.0251 - val_mean_absolute_error: 309.0251\n",
            "Epoch 280/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 157.6893 - mean_absolute_error: 157.6893\n",
            "Epoch 280: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 156.8555 - mean_absolute_error: 156.8555 - val_loss: 313.0392 - val_mean_absolute_error: 313.0392\n",
            "Epoch 281/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 154.0253 - mean_absolute_error: 154.0253\n",
            "Epoch 281: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 153.2132 - mean_absolute_error: 153.2132 - val_loss: 324.9320 - val_mean_absolute_error: 324.9320\n",
            "Epoch 282/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 148.2779 - mean_absolute_error: 148.2779\n",
            "Epoch 282: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 148.0728 - mean_absolute_error: 148.0728 - val_loss: 315.8942 - val_mean_absolute_error: 315.8942\n",
            "Epoch 283/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 154.4566 - mean_absolute_error: 154.4566\n",
            "Epoch 283: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 153.4592 - mean_absolute_error: 153.4592 - val_loss: 316.4147 - val_mean_absolute_error: 316.4147\n",
            "Epoch 284/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 151.4303 - mean_absolute_error: 151.4303\n",
            "Epoch 284: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 150.1768 - mean_absolute_error: 150.1768 - val_loss: 319.3850 - val_mean_absolute_error: 319.3850\n",
            "Epoch 285/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 150.3389 - mean_absolute_error: 150.3389\n",
            "Epoch 285: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 151.0905 - mean_absolute_error: 151.0905 - val_loss: 321.7761 - val_mean_absolute_error: 321.7761\n",
            "Epoch 286/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 158.9444 - mean_absolute_error: 158.9444\n",
            "Epoch 286: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 158.9429 - mean_absolute_error: 158.9429 - val_loss: 331.4250 - val_mean_absolute_error: 331.4250\n",
            "Epoch 287/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 161.7977 - mean_absolute_error: 161.7977\n",
            "Epoch 287: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 161.3995 - mean_absolute_error: 161.3995 - val_loss: 317.9682 - val_mean_absolute_error: 317.9682\n",
            "Epoch 288/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 150.9458 - mean_absolute_error: 150.9458\n",
            "Epoch 288: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 151.2813 - mean_absolute_error: 151.2813 - val_loss: 320.5213 - val_mean_absolute_error: 320.5213\n",
            "Epoch 289/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 151.2819 - mean_absolute_error: 151.2819\n",
            "Epoch 289: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 152.8413 - mean_absolute_error: 152.8413 - val_loss: 321.3004 - val_mean_absolute_error: 321.3004\n",
            "Epoch 290/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 154.8672 - mean_absolute_error: 154.8672\n",
            "Epoch 290: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 154.9005 - mean_absolute_error: 154.9005 - val_loss: 329.8730 - val_mean_absolute_error: 329.8730\n",
            "Epoch 291/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 159.6368 - mean_absolute_error: 159.6368\n",
            "Epoch 291: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 159.2303 - mean_absolute_error: 159.2303 - val_loss: 320.6078 - val_mean_absolute_error: 320.6078\n",
            "Epoch 292/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 151.9615 - mean_absolute_error: 151.9615\n",
            "Epoch 292: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 152.3189 - mean_absolute_error: 152.3189 - val_loss: 342.5155 - val_mean_absolute_error: 342.5155\n",
            "Epoch 293/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 163.1931 - mean_absolute_error: 163.1931\n",
            "Epoch 293: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 164.4891 - mean_absolute_error: 164.4891 - val_loss: 344.4662 - val_mean_absolute_error: 344.4662\n",
            "Epoch 294/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 159.5766 - mean_absolute_error: 159.5766\n",
            "Epoch 294: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 160.1246 - mean_absolute_error: 160.1246 - val_loss: 315.1055 - val_mean_absolute_error: 315.1055\n",
            "Epoch 295/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 155.9715 - mean_absolute_error: 155.9715\n",
            "Epoch 295: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 155.2159 - mean_absolute_error: 155.2159 - val_loss: 318.4234 - val_mean_absolute_error: 318.4234\n",
            "Epoch 296/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 155.2333 - mean_absolute_error: 155.2333\n",
            "Epoch 296: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 155.3985 - mean_absolute_error: 155.3985 - val_loss: 318.0424 - val_mean_absolute_error: 318.0424\n",
            "Epoch 297/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 153.0284 - mean_absolute_error: 153.0284\n",
            "Epoch 297: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 152.9619 - mean_absolute_error: 152.9619 - val_loss: 317.6393 - val_mean_absolute_error: 317.6393\n",
            "Epoch 298/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 154.7017 - mean_absolute_error: 154.7017\n",
            "Epoch 298: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 154.6317 - mean_absolute_error: 154.6317 - val_loss: 321.2354 - val_mean_absolute_error: 321.2354\n",
            "Epoch 299/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 152.6537 - mean_absolute_error: 152.6537\n",
            "Epoch 299: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 152.9380 - mean_absolute_error: 152.9380 - val_loss: 314.3012 - val_mean_absolute_error: 314.3012\n",
            "Epoch 300/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 154.0163 - mean_absolute_error: 154.0163\n",
            "Epoch 300: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 153.8612 - mean_absolute_error: 153.8612 - val_loss: 317.6174 - val_mean_absolute_error: 317.6174\n",
            "Epoch 301/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 150.0694 - mean_absolute_error: 150.0694\n",
            "Epoch 301: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 151.6633 - mean_absolute_error: 151.6633 - val_loss: 324.4052 - val_mean_absolute_error: 324.4052\n",
            "Epoch 302/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 160.2273 - mean_absolute_error: 160.2273\n",
            "Epoch 302: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 157.1979 - mean_absolute_error: 157.1979 - val_loss: 337.3410 - val_mean_absolute_error: 337.3410\n",
            "Epoch 303/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 145.8858 - mean_absolute_error: 145.8858\n",
            "Epoch 303: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 144.7182 - mean_absolute_error: 144.7182 - val_loss: 323.7773 - val_mean_absolute_error: 323.7773\n",
            "Epoch 304/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 150.6176 - mean_absolute_error: 150.6176\n",
            "Epoch 304: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 149.9154 - mean_absolute_error: 149.9154 - val_loss: 319.3412 - val_mean_absolute_error: 319.3412\n",
            "Epoch 305/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 152.9656 - mean_absolute_error: 152.9656\n",
            "Epoch 305: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 153.4614 - mean_absolute_error: 153.4614 - val_loss: 322.1870 - val_mean_absolute_error: 322.1870\n",
            "Epoch 306/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 150.0299 - mean_absolute_error: 150.0299\n",
            "Epoch 306: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 149.1420 - mean_absolute_error: 149.1420 - val_loss: 318.3542 - val_mean_absolute_error: 318.3542\n",
            "Epoch 307/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 148.9073 - mean_absolute_error: 148.9073\n",
            "Epoch 307: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 148.9073 - mean_absolute_error: 148.9073 - val_loss: 319.1064 - val_mean_absolute_error: 319.1064\n",
            "Epoch 308/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 145.4824 - mean_absolute_error: 145.4824\n",
            "Epoch 308: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 145.4824 - mean_absolute_error: 145.4824 - val_loss: 321.8884 - val_mean_absolute_error: 321.8884\n",
            "Epoch 309/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 142.3661 - mean_absolute_error: 142.3661\n",
            "Epoch 309: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 144.5619 - mean_absolute_error: 144.5619 - val_loss: 323.9311 - val_mean_absolute_error: 323.9311\n",
            "Epoch 310/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 143.3524 - mean_absolute_error: 143.3524\n",
            "Epoch 310: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 143.6240 - mean_absolute_error: 143.6240 - val_loss: 319.6487 - val_mean_absolute_error: 319.6487\n",
            "Epoch 311/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 140.9458 - mean_absolute_error: 140.9458\n",
            "Epoch 311: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 143.3371 - mean_absolute_error: 143.3371 - val_loss: 334.2301 - val_mean_absolute_error: 334.2301\n",
            "Epoch 312/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 148.8621 - mean_absolute_error: 148.8621\n",
            "Epoch 312: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 148.5151 - mean_absolute_error: 148.5151 - val_loss: 320.2618 - val_mean_absolute_error: 320.2618\n",
            "Epoch 313/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 156.5298 - mean_absolute_error: 156.5298\n",
            "Epoch 313: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 157.5631 - mean_absolute_error: 157.5631 - val_loss: 320.1589 - val_mean_absolute_error: 320.1589\n",
            "Epoch 314/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 155.0753 - mean_absolute_error: 155.0753\n",
            "Epoch 314: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 154.8387 - mean_absolute_error: 154.8387 - val_loss: 321.8597 - val_mean_absolute_error: 321.8597\n",
            "Epoch 315/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 155.0113 - mean_absolute_error: 155.0113\n",
            "Epoch 315: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 156.9501 - mean_absolute_error: 156.9501 - val_loss: 325.5188 - val_mean_absolute_error: 325.5188\n",
            "Epoch 316/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 155.9652 - mean_absolute_error: 155.9652\n",
            "Epoch 316: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 155.6813 - mean_absolute_error: 155.6813 - val_loss: 318.1429 - val_mean_absolute_error: 318.1429\n",
            "Epoch 317/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 150.4241 - mean_absolute_error: 150.4241\n",
            "Epoch 317: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 148.5211 - mean_absolute_error: 148.5211 - val_loss: 335.1394 - val_mean_absolute_error: 335.1394\n",
            "Epoch 318/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 153.1758 - mean_absolute_error: 153.1758\n",
            "Epoch 318: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 153.1758 - mean_absolute_error: 153.1758 - val_loss: 322.2365 - val_mean_absolute_error: 322.2365\n",
            "Epoch 319/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 155.4997 - mean_absolute_error: 155.4997\n",
            "Epoch 319: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 155.1145 - mean_absolute_error: 155.1145 - val_loss: 314.0890 - val_mean_absolute_error: 314.0890\n",
            "Epoch 320/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 142.1746 - mean_absolute_error: 142.1746\n",
            "Epoch 320: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 142.0900 - mean_absolute_error: 142.0900 - val_loss: 320.5642 - val_mean_absolute_error: 320.5642\n",
            "Epoch 321/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 138.7390 - mean_absolute_error: 138.7390\n",
            "Epoch 321: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 138.4193 - mean_absolute_error: 138.4193 - val_loss: 335.4500 - val_mean_absolute_error: 335.4500\n",
            "Epoch 322/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 146.3439 - mean_absolute_error: 146.3439\n",
            "Epoch 322: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 146.8155 - mean_absolute_error: 146.8155 - val_loss: 329.6175 - val_mean_absolute_error: 329.6175\n",
            "Epoch 323/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 144.5990 - mean_absolute_error: 144.5990\n",
            "Epoch 323: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 144.5309 - mean_absolute_error: 144.5309 - val_loss: 324.5388 - val_mean_absolute_error: 324.5388\n",
            "Epoch 324/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 139.7498 - mean_absolute_error: 139.7498\n",
            "Epoch 324: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 139.4222 - mean_absolute_error: 139.4222 - val_loss: 323.8304 - val_mean_absolute_error: 323.8304\n",
            "Epoch 325/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 142.1693 - mean_absolute_error: 142.1693\n",
            "Epoch 325: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 142.0219 - mean_absolute_error: 142.0219 - val_loss: 330.4171 - val_mean_absolute_error: 330.4171\n",
            "Epoch 326/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 138.4285 - mean_absolute_error: 138.4285\n",
            "Epoch 326: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 138.6530 - mean_absolute_error: 138.6530 - val_loss: 319.7931 - val_mean_absolute_error: 319.7931\n",
            "Epoch 327/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 140.1460 - mean_absolute_error: 140.1460\n",
            "Epoch 327: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 140.5777 - mean_absolute_error: 140.5777 - val_loss: 339.7480 - val_mean_absolute_error: 339.7480\n",
            "Epoch 328/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 152.0769 - mean_absolute_error: 152.0769\n",
            "Epoch 328: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 153.4790 - mean_absolute_error: 153.4790 - val_loss: 335.2119 - val_mean_absolute_error: 335.2119\n",
            "Epoch 329/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 150.3070 - mean_absolute_error: 150.3070\n",
            "Epoch 329: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 150.2573 - mean_absolute_error: 150.2573 - val_loss: 315.1849 - val_mean_absolute_error: 315.1849\n",
            "Epoch 330/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 142.2526 - mean_absolute_error: 142.2526\n",
            "Epoch 330: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 142.1326 - mean_absolute_error: 142.1326 - val_loss: 332.1259 - val_mean_absolute_error: 332.1259\n",
            "Epoch 331/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 136.9407 - mean_absolute_error: 136.9407\n",
            "Epoch 331: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 136.9407 - mean_absolute_error: 136.9407 - val_loss: 329.9336 - val_mean_absolute_error: 329.9336\n",
            "Epoch 332/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 141.0536 - mean_absolute_error: 141.0536\n",
            "Epoch 332: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 141.2601 - mean_absolute_error: 141.2601 - val_loss: 318.7540 - val_mean_absolute_error: 318.7540\n",
            "Epoch 333/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 142.4029 - mean_absolute_error: 142.4029\n",
            "Epoch 333: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 141.9932 - mean_absolute_error: 141.9932 - val_loss: 315.2270 - val_mean_absolute_error: 315.2270\n",
            "Epoch 334/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 138.6754 - mean_absolute_error: 138.6754\n",
            "Epoch 334: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 138.6331 - mean_absolute_error: 138.6331 - val_loss: 317.2852 - val_mean_absolute_error: 317.2852\n",
            "Epoch 335/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 140.6034 - mean_absolute_error: 140.6034\n",
            "Epoch 335: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 140.9370 - mean_absolute_error: 140.9370 - val_loss: 325.2261 - val_mean_absolute_error: 325.2261\n",
            "Epoch 336/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 139.7239 - mean_absolute_error: 139.7239\n",
            "Epoch 336: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 139.8079 - mean_absolute_error: 139.8079 - val_loss: 341.4061 - val_mean_absolute_error: 341.4061\n",
            "Epoch 337/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 152.0725 - mean_absolute_error: 152.0725\n",
            "Epoch 337: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 152.2458 - mean_absolute_error: 152.2458 - val_loss: 324.8895 - val_mean_absolute_error: 324.8895\n",
            "Epoch 338/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 145.2671 - mean_absolute_error: 145.2671\n",
            "Epoch 338: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 144.9673 - mean_absolute_error: 144.9673 - val_loss: 355.9193 - val_mean_absolute_error: 355.9193\n",
            "Epoch 339/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 164.7885 - mean_absolute_error: 164.7885\n",
            "Epoch 339: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 162.1153 - mean_absolute_error: 162.1153 - val_loss: 320.6936 - val_mean_absolute_error: 320.6936\n",
            "Epoch 340/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 145.9904 - mean_absolute_error: 145.9904\n",
            "Epoch 340: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 145.0286 - mean_absolute_error: 145.0286 - val_loss: 323.6434 - val_mean_absolute_error: 323.6434\n",
            "Epoch 341/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 138.7523 - mean_absolute_error: 138.7523\n",
            "Epoch 341: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 138.8339 - mean_absolute_error: 138.8339 - val_loss: 331.0128 - val_mean_absolute_error: 331.0128\n",
            "Epoch 342/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 136.6108 - mean_absolute_error: 136.6108\n",
            "Epoch 342: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 137.9613 - mean_absolute_error: 137.9613 - val_loss: 326.3056 - val_mean_absolute_error: 326.3056\n",
            "Epoch 343/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 136.6613 - mean_absolute_error: 136.6613\n",
            "Epoch 343: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 136.1449 - mean_absolute_error: 136.1449 - val_loss: 320.6413 - val_mean_absolute_error: 320.6413\n",
            "Epoch 344/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 134.0680 - mean_absolute_error: 134.0680\n",
            "Epoch 344: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 134.6021 - mean_absolute_error: 134.6021 - val_loss: 340.0595 - val_mean_absolute_error: 340.0595\n",
            "Epoch 345/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 142.5704 - mean_absolute_error: 142.5704\n",
            "Epoch 345: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 142.6297 - mean_absolute_error: 142.6297 - val_loss: 334.7531 - val_mean_absolute_error: 334.7531\n",
            "Epoch 346/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 135.9603 - mean_absolute_error: 135.9603\n",
            "Epoch 346: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 135.5155 - mean_absolute_error: 135.5155 - val_loss: 323.7350 - val_mean_absolute_error: 323.7350\n",
            "Epoch 347/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 149.9449 - mean_absolute_error: 149.9449\n",
            "Epoch 347: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 149.3618 - mean_absolute_error: 149.3618 - val_loss: 320.9332 - val_mean_absolute_error: 320.9332\n",
            "Epoch 348/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 155.8376 - mean_absolute_error: 155.8376\n",
            "Epoch 348: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 154.0151 - mean_absolute_error: 154.0151 - val_loss: 325.1208 - val_mean_absolute_error: 325.1208\n",
            "Epoch 349/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 137.0169 - mean_absolute_error: 137.0169\n",
            "Epoch 349: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 137.5732 - mean_absolute_error: 137.5732 - val_loss: 323.7017 - val_mean_absolute_error: 323.7017\n",
            "Epoch 350/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 135.1237 - mean_absolute_error: 135.1237\n",
            "Epoch 350: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 135.7206 - mean_absolute_error: 135.7206 - val_loss: 328.4945 - val_mean_absolute_error: 328.4945\n",
            "Epoch 351/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 146.1405 - mean_absolute_error: 146.1405\n",
            "Epoch 351: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 145.2342 - mean_absolute_error: 145.2342 - val_loss: 323.0013 - val_mean_absolute_error: 323.0013\n",
            "Epoch 352/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 142.2944 - mean_absolute_error: 142.2944\n",
            "Epoch 352: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 142.3990 - mean_absolute_error: 142.3990 - val_loss: 326.5474 - val_mean_absolute_error: 326.5474\n",
            "Epoch 353/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 143.9218 - mean_absolute_error: 143.9218\n",
            "Epoch 353: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 143.2055 - mean_absolute_error: 143.2055 - val_loss: 318.7082 - val_mean_absolute_error: 318.7082\n",
            "Epoch 354/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 135.7232 - mean_absolute_error: 135.7232\n",
            "Epoch 354: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 136.1799 - mean_absolute_error: 136.1799 - val_loss: 326.5030 - val_mean_absolute_error: 326.5030\n",
            "Epoch 355/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 141.9988 - mean_absolute_error: 141.9988\n",
            "Epoch 355: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 141.7937 - mean_absolute_error: 141.7937 - val_loss: 338.3416 - val_mean_absolute_error: 338.3416\n",
            "Epoch 356/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 138.1055 - mean_absolute_error: 138.1055\n",
            "Epoch 356: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 138.1321 - mean_absolute_error: 138.1321 - val_loss: 338.8084 - val_mean_absolute_error: 338.8084\n",
            "Epoch 357/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 142.9337 - mean_absolute_error: 142.9337\n",
            "Epoch 357: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 142.9337 - mean_absolute_error: 142.9337 - val_loss: 336.3094 - val_mean_absolute_error: 336.3094\n",
            "Epoch 358/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 132.2259 - mean_absolute_error: 132.2259\n",
            "Epoch 358: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 132.2759 - mean_absolute_error: 132.2759 - val_loss: 321.4810 - val_mean_absolute_error: 321.4810\n",
            "Epoch 359/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 136.6429 - mean_absolute_error: 136.6429\n",
            "Epoch 359: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 136.1139 - mean_absolute_error: 136.1139 - val_loss: 327.1524 - val_mean_absolute_error: 327.1524\n",
            "Epoch 360/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 138.0590 - mean_absolute_error: 138.0590\n",
            "Epoch 360: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 137.6328 - mean_absolute_error: 137.6328 - val_loss: 320.7691 - val_mean_absolute_error: 320.7691\n",
            "Epoch 361/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 141.7621 - mean_absolute_error: 141.7621\n",
            "Epoch 361: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 142.4056 - mean_absolute_error: 142.4056 - val_loss: 319.8609 - val_mean_absolute_error: 319.8609\n",
            "Epoch 362/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 128.6298 - mean_absolute_error: 128.6298\n",
            "Epoch 362: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 128.7907 - mean_absolute_error: 128.7907 - val_loss: 333.1039 - val_mean_absolute_error: 333.1039\n",
            "Epoch 363/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 133.5135 - mean_absolute_error: 133.5135\n",
            "Epoch 363: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 133.5351 - mean_absolute_error: 133.5351 - val_loss: 319.0611 - val_mean_absolute_error: 319.0611\n",
            "Epoch 364/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 134.5644 - mean_absolute_error: 134.5644\n",
            "Epoch 364: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 134.2458 - mean_absolute_error: 134.2458 - val_loss: 324.8961 - val_mean_absolute_error: 324.8961\n",
            "Epoch 365/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 138.4931 - mean_absolute_error: 138.4931\n",
            "Epoch 365: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 137.7015 - mean_absolute_error: 137.7015 - val_loss: 341.2747 - val_mean_absolute_error: 341.2747\n",
            "Epoch 366/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 135.2002 - mean_absolute_error: 135.2002\n",
            "Epoch 366: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 135.1159 - mean_absolute_error: 135.1159 - val_loss: 326.7278 - val_mean_absolute_error: 326.7278\n",
            "Epoch 367/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 143.3449 - mean_absolute_error: 143.3449\n",
            "Epoch 367: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 143.3449 - mean_absolute_error: 143.3449 - val_loss: 323.5109 - val_mean_absolute_error: 323.5109\n",
            "Epoch 368/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 130.9101 - mean_absolute_error: 130.9101\n",
            "Epoch 368: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 130.9101 - mean_absolute_error: 130.9101 - val_loss: 334.9655 - val_mean_absolute_error: 334.9655\n",
            "Epoch 369/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 132.0402 - mean_absolute_error: 132.0402\n",
            "Epoch 369: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 2s 24ms/step - loss: 131.8587 - mean_absolute_error: 131.8587 - val_loss: 317.4812 - val_mean_absolute_error: 317.4812\n",
            "Epoch 370/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 132.9492 - mean_absolute_error: 132.9492\n",
            "Epoch 370: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 133.4791 - mean_absolute_error: 133.4791 - val_loss: 324.7246 - val_mean_absolute_error: 324.7246\n",
            "Epoch 371/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 137.8024 - mean_absolute_error: 137.8024\n",
            "Epoch 371: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 138.1707 - mean_absolute_error: 138.1707 - val_loss: 323.5748 - val_mean_absolute_error: 323.5748\n",
            "Epoch 372/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 135.0077 - mean_absolute_error: 135.0077\n",
            "Epoch 372: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 134.8795 - mean_absolute_error: 134.8795 - val_loss: 335.9914 - val_mean_absolute_error: 335.9914\n",
            "Epoch 373/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 135.3273 - mean_absolute_error: 135.3273\n",
            "Epoch 373: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 135.3760 - mean_absolute_error: 135.3760 - val_loss: 321.9010 - val_mean_absolute_error: 321.9010\n",
            "Epoch 374/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 139.6290 - mean_absolute_error: 139.6290\n",
            "Epoch 374: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 139.5445 - mean_absolute_error: 139.5445 - val_loss: 339.2346 - val_mean_absolute_error: 339.2346\n",
            "Epoch 375/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 136.9382 - mean_absolute_error: 136.9382\n",
            "Epoch 375: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 136.9382 - mean_absolute_error: 136.9382 - val_loss: 323.6183 - val_mean_absolute_error: 323.6183\n",
            "Epoch 376/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 132.0014 - mean_absolute_error: 132.0014\n",
            "Epoch 376: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 132.8852 - mean_absolute_error: 132.8852 - val_loss: 337.8849 - val_mean_absolute_error: 337.8849\n",
            "Epoch 377/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 128.5522 - mean_absolute_error: 128.5522\n",
            "Epoch 377: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 129.9168 - mean_absolute_error: 129.9168 - val_loss: 316.0426 - val_mean_absolute_error: 316.0426\n",
            "Epoch 378/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 126.8891 - mean_absolute_error: 126.8891\n",
            "Epoch 378: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 126.0425 - mean_absolute_error: 126.0425 - val_loss: 330.8826 - val_mean_absolute_error: 330.8826\n",
            "Epoch 379/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 128.1143 - mean_absolute_error: 128.1143\n",
            "Epoch 379: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 127.5665 - mean_absolute_error: 127.5665 - val_loss: 330.9069 - val_mean_absolute_error: 330.9069\n",
            "Epoch 380/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 137.8380 - mean_absolute_error: 137.8380\n",
            "Epoch 380: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 137.3560 - mean_absolute_error: 137.3560 - val_loss: 318.7453 - val_mean_absolute_error: 318.7453\n",
            "Epoch 381/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 126.3887 - mean_absolute_error: 126.3887\n",
            "Epoch 381: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 126.5655 - mean_absolute_error: 126.5655 - val_loss: 328.2498 - val_mean_absolute_error: 328.2498\n",
            "Epoch 382/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 141.9091 - mean_absolute_error: 141.9091\n",
            "Epoch 382: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 141.7891 - mean_absolute_error: 141.7891 - val_loss: 321.3902 - val_mean_absolute_error: 321.3902\n",
            "Epoch 383/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 131.6902 - mean_absolute_error: 131.6902\n",
            "Epoch 383: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 131.8299 - mean_absolute_error: 131.8299 - val_loss: 334.5467 - val_mean_absolute_error: 334.5467\n",
            "Epoch 384/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 133.1237 - mean_absolute_error: 133.1237\n",
            "Epoch 384: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 133.0007 - mean_absolute_error: 133.0007 - val_loss: 320.5385 - val_mean_absolute_error: 320.5385\n",
            "Epoch 385/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 128.3688 - mean_absolute_error: 128.3688\n",
            "Epoch 385: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 128.9159 - mean_absolute_error: 128.9159 - val_loss: 333.0638 - val_mean_absolute_error: 333.0638\n",
            "Epoch 386/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 137.2710 - mean_absolute_error: 137.2710\n",
            "Epoch 386: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 138.2881 - mean_absolute_error: 138.2881 - val_loss: 326.2488 - val_mean_absolute_error: 326.2488\n",
            "Epoch 387/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 131.7942 - mean_absolute_error: 131.7942\n",
            "Epoch 387: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 132.0611 - mean_absolute_error: 132.0611 - val_loss: 322.9420 - val_mean_absolute_error: 322.9420\n",
            "Epoch 388/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 141.9303 - mean_absolute_error: 141.9303\n",
            "Epoch 388: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 141.4321 - mean_absolute_error: 141.4321 - val_loss: 321.1453 - val_mean_absolute_error: 321.1453\n",
            "Epoch 389/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 138.2645 - mean_absolute_error: 138.2645\n",
            "Epoch 389: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 137.9643 - mean_absolute_error: 137.9643 - val_loss: 322.8352 - val_mean_absolute_error: 322.8352\n",
            "Epoch 390/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 129.1773 - mean_absolute_error: 129.1773\n",
            "Epoch 390: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 129.5054 - mean_absolute_error: 129.5054 - val_loss: 319.3754 - val_mean_absolute_error: 319.3754\n",
            "Epoch 391/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 126.3411 - mean_absolute_error: 126.3411\n",
            "Epoch 391: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 125.5219 - mean_absolute_error: 125.5219 - val_loss: 323.5526 - val_mean_absolute_error: 323.5526\n",
            "Epoch 392/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 128.4029 - mean_absolute_error: 128.4029\n",
            "Epoch 392: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 128.6446 - mean_absolute_error: 128.6446 - val_loss: 321.8019 - val_mean_absolute_error: 321.8019\n",
            "Epoch 393/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 124.5346 - mean_absolute_error: 124.5346\n",
            "Epoch 393: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.8543 - mean_absolute_error: 124.8543 - val_loss: 324.8385 - val_mean_absolute_error: 324.8385\n",
            "Epoch 394/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 130.7753 - mean_absolute_error: 130.7753\n",
            "Epoch 394: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 130.6117 - mean_absolute_error: 130.6117 - val_loss: 316.2753 - val_mean_absolute_error: 316.2753\n",
            "Epoch 395/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 131.3832 - mean_absolute_error: 131.3832\n",
            "Epoch 395: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 131.4897 - mean_absolute_error: 131.4897 - val_loss: 332.9061 - val_mean_absolute_error: 332.9061\n",
            "Epoch 396/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 124.8935 - mean_absolute_error: 124.8935\n",
            "Epoch 396: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.8935 - mean_absolute_error: 124.8935 - val_loss: 323.6000 - val_mean_absolute_error: 323.6000\n",
            "Epoch 397/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 129.3524 - mean_absolute_error: 129.3524\n",
            "Epoch 397: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 130.4548 - mean_absolute_error: 130.4548 - val_loss: 328.7557 - val_mean_absolute_error: 328.7557\n",
            "Epoch 398/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 131.1828 - mean_absolute_error: 131.1828\n",
            "Epoch 398: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 131.1828 - mean_absolute_error: 131.1828 - val_loss: 319.2751 - val_mean_absolute_error: 319.2751\n",
            "Epoch 399/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 129.9273 - mean_absolute_error: 129.9273\n",
            "Epoch 399: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 130.1151 - mean_absolute_error: 130.1151 - val_loss: 330.9237 - val_mean_absolute_error: 330.9237\n",
            "Epoch 400/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 132.6700 - mean_absolute_error: 132.6700\n",
            "Epoch 400: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 132.6700 - mean_absolute_error: 132.6700 - val_loss: 330.7028 - val_mean_absolute_error: 330.7028\n",
            "Epoch 401/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 141.6000 - mean_absolute_error: 141.6000\n",
            "Epoch 401: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 141.4655 - mean_absolute_error: 141.4655 - val_loss: 322.7458 - val_mean_absolute_error: 322.7458\n",
            "Epoch 402/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 126.4541 - mean_absolute_error: 126.4541\n",
            "Epoch 402: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 126.2636 - mean_absolute_error: 126.2636 - val_loss: 330.3817 - val_mean_absolute_error: 330.3817\n",
            "Epoch 403/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 122.2918 - mean_absolute_error: 122.2918\n",
            "Epoch 403: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 122.2918 - mean_absolute_error: 122.2918 - val_loss: 317.8115 - val_mean_absolute_error: 317.8115\n",
            "Epoch 404/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 123.9004 - mean_absolute_error: 123.9004\n",
            "Epoch 404: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 123.8261 - mean_absolute_error: 123.8261 - val_loss: 330.0596 - val_mean_absolute_error: 330.0596\n",
            "Epoch 405/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 128.0931 - mean_absolute_error: 128.0931\n",
            "Epoch 405: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 126.9309 - mean_absolute_error: 126.9309 - val_loss: 320.8206 - val_mean_absolute_error: 320.8206\n",
            "Epoch 406/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 122.2794 - mean_absolute_error: 122.2794\n",
            "Epoch 406: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 121.6800 - mean_absolute_error: 121.6800 - val_loss: 324.0611 - val_mean_absolute_error: 324.0611\n",
            "Epoch 407/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 136.2837 - mean_absolute_error: 136.2837\n",
            "Epoch 407: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 135.2496 - mean_absolute_error: 135.2496 - val_loss: 322.2603 - val_mean_absolute_error: 322.2603\n",
            "Epoch 408/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 124.0465 - mean_absolute_error: 124.0465\n",
            "Epoch 408: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 125.4700 - mean_absolute_error: 125.4700 - val_loss: 326.5424 - val_mean_absolute_error: 326.5424\n",
            "Epoch 409/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 124.4996 - mean_absolute_error: 124.4996\n",
            "Epoch 409: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.1969 - mean_absolute_error: 124.1969 - val_loss: 331.7404 - val_mean_absolute_error: 331.7404\n",
            "Epoch 410/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 129.5959 - mean_absolute_error: 129.5959\n",
            "Epoch 410: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 129.5113 - mean_absolute_error: 129.5113 - val_loss: 332.4900 - val_mean_absolute_error: 332.4900\n",
            "Epoch 411/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 127.8227 - mean_absolute_error: 127.8227\n",
            "Epoch 411: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 128.2673 - mean_absolute_error: 128.2673 - val_loss: 338.8340 - val_mean_absolute_error: 338.8340\n",
            "Epoch 412/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 134.8931 - mean_absolute_error: 134.8931\n",
            "Epoch 412: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 135.6604 - mean_absolute_error: 135.6604 - val_loss: 334.6209 - val_mean_absolute_error: 334.6209\n",
            "Epoch 413/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 140.0557 - mean_absolute_error: 140.0557\n",
            "Epoch 413: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 141.1426 - mean_absolute_error: 141.1426 - val_loss: 324.8850 - val_mean_absolute_error: 324.8850\n",
            "Epoch 414/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 132.7925 - mean_absolute_error: 132.7925\n",
            "Epoch 414: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 132.8275 - mean_absolute_error: 132.8275 - val_loss: 342.1691 - val_mean_absolute_error: 342.1691\n",
            "Epoch 415/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 141.4830 - mean_absolute_error: 141.4830\n",
            "Epoch 415: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 141.4903 - mean_absolute_error: 141.4903 - val_loss: 333.4109 - val_mean_absolute_error: 333.4109\n",
            "Epoch 416/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 130.4040 - mean_absolute_error: 130.4040\n",
            "Epoch 416: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 130.2867 - mean_absolute_error: 130.2867 - val_loss: 330.8924 - val_mean_absolute_error: 330.8924\n",
            "Epoch 417/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 122.2739 - mean_absolute_error: 122.2739\n",
            "Epoch 417: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 122.2762 - mean_absolute_error: 122.2762 - val_loss: 329.4154 - val_mean_absolute_error: 329.4154\n",
            "Epoch 418/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 130.0418 - mean_absolute_error: 130.0418\n",
            "Epoch 418: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 130.3287 - mean_absolute_error: 130.3287 - val_loss: 322.7343 - val_mean_absolute_error: 322.7343\n",
            "Epoch 419/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 128.5079 - mean_absolute_error: 128.5079\n",
            "Epoch 419: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 128.9133 - mean_absolute_error: 128.9133 - val_loss: 338.5147 - val_mean_absolute_error: 338.5147\n",
            "Epoch 420/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 127.8942 - mean_absolute_error: 127.8942\n",
            "Epoch 420: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 127.8942 - mean_absolute_error: 127.8942 - val_loss: 350.2971 - val_mean_absolute_error: 350.2971\n",
            "Epoch 421/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 139.1322 - mean_absolute_error: 139.1322\n",
            "Epoch 421: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 139.1196 - mean_absolute_error: 139.1196 - val_loss: 348.8636 - val_mean_absolute_error: 348.8636\n",
            "Epoch 422/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 131.4501 - mean_absolute_error: 131.4501\n",
            "Epoch 422: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 130.5804 - mean_absolute_error: 130.5804 - val_loss: 325.2595 - val_mean_absolute_error: 325.2595\n",
            "Epoch 423/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 124.0595 - mean_absolute_error: 124.0595\n",
            "Epoch 423: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 124.9736 - mean_absolute_error: 124.9736 - val_loss: 326.5248 - val_mean_absolute_error: 326.5248\n",
            "Epoch 424/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 122.1947 - mean_absolute_error: 122.1947\n",
            "Epoch 424: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 122.1947 - mean_absolute_error: 122.1947 - val_loss: 322.5379 - val_mean_absolute_error: 322.5379\n",
            "Epoch 425/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 125.2528 - mean_absolute_error: 125.2528\n",
            "Epoch 425: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 124.6419 - mean_absolute_error: 124.6419 - val_loss: 321.3856 - val_mean_absolute_error: 321.3856\n",
            "Epoch 426/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 126.7391 - mean_absolute_error: 126.7391\n",
            "Epoch 426: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 126.7391 - mean_absolute_error: 126.7391 - val_loss: 340.2703 - val_mean_absolute_error: 340.2703\n",
            "Epoch 427/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 124.7984 - mean_absolute_error: 124.7984\n",
            "Epoch 427: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 123.9458 - mean_absolute_error: 123.9458 - val_loss: 327.8074 - val_mean_absolute_error: 327.8074\n",
            "Epoch 428/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 126.3199 - mean_absolute_error: 126.3199\n",
            "Epoch 428: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 125.8322 - mean_absolute_error: 125.8322 - val_loss: 324.2426 - val_mean_absolute_error: 324.2426\n",
            "Epoch 429/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 132.3530 - mean_absolute_error: 132.3530\n",
            "Epoch 429: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 133.0877 - mean_absolute_error: 133.0877 - val_loss: 321.4547 - val_mean_absolute_error: 321.4547\n",
            "Epoch 430/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 124.3960 - mean_absolute_error: 124.3960\n",
            "Epoch 430: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.4150 - mean_absolute_error: 124.4150 - val_loss: 325.7253 - val_mean_absolute_error: 325.7253\n",
            "Epoch 431/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 125.5810 - mean_absolute_error: 125.5810\n",
            "Epoch 431: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 125.6011 - mean_absolute_error: 125.6011 - val_loss: 329.3279 - val_mean_absolute_error: 329.3279\n",
            "Epoch 432/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 123.0282 - mean_absolute_error: 123.0282\n",
            "Epoch 432: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 123.0800 - mean_absolute_error: 123.0800 - val_loss: 325.0615 - val_mean_absolute_error: 325.0615\n",
            "Epoch 433/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 117.4606 - mean_absolute_error: 117.4606\n",
            "Epoch 433: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 117.7412 - mean_absolute_error: 117.7412 - val_loss: 322.5384 - val_mean_absolute_error: 322.5384\n",
            "Epoch 434/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 119.4569 - mean_absolute_error: 119.4569\n",
            "Epoch 434: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 119.9237 - mean_absolute_error: 119.9237 - val_loss: 349.0137 - val_mean_absolute_error: 349.0137\n",
            "Epoch 435/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 130.8740 - mean_absolute_error: 130.8740\n",
            "Epoch 435: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 130.8025 - mean_absolute_error: 130.8025 - val_loss: 330.0818 - val_mean_absolute_error: 330.0818\n",
            "Epoch 436/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 126.8691 - mean_absolute_error: 126.8691\n",
            "Epoch 436: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 126.4125 - mean_absolute_error: 126.4125 - val_loss: 324.8461 - val_mean_absolute_error: 324.8461\n",
            "Epoch 437/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 130.1310 - mean_absolute_error: 130.1310\n",
            "Epoch 437: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 130.9230 - mean_absolute_error: 130.9230 - val_loss: 322.5067 - val_mean_absolute_error: 322.5067\n",
            "Epoch 438/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 126.6345 - mean_absolute_error: 126.6345\n",
            "Epoch 438: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 128.0878 - mean_absolute_error: 128.0878 - val_loss: 326.2980 - val_mean_absolute_error: 326.2980\n",
            "Epoch 439/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 132.1517 - mean_absolute_error: 132.1517\n",
            "Epoch 439: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 132.8729 - mean_absolute_error: 132.8729 - val_loss: 325.7137 - val_mean_absolute_error: 325.7137\n",
            "Epoch 440/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 123.7629 - mean_absolute_error: 123.7629\n",
            "Epoch 440: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.0721 - mean_absolute_error: 124.0721 - val_loss: 319.9672 - val_mean_absolute_error: 319.9672\n",
            "Epoch 441/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 122.4953 - mean_absolute_error: 122.4953\n",
            "Epoch 441: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 122.7104 - mean_absolute_error: 122.7104 - val_loss: 329.7860 - val_mean_absolute_error: 329.7860\n",
            "Epoch 442/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 119.9499 - mean_absolute_error: 119.9499\n",
            "Epoch 442: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 120.0945 - mean_absolute_error: 120.0945 - val_loss: 318.8174 - val_mean_absolute_error: 318.8174\n",
            "Epoch 443/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 120.9274 - mean_absolute_error: 120.9274\n",
            "Epoch 443: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 120.9274 - mean_absolute_error: 120.9274 - val_loss: 326.7148 - val_mean_absolute_error: 326.7148\n",
            "Epoch 444/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 118.3507 - mean_absolute_error: 118.3507\n",
            "Epoch 444: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 118.7871 - mean_absolute_error: 118.7871 - val_loss: 322.9522 - val_mean_absolute_error: 322.9522\n",
            "Epoch 445/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 114.9334 - mean_absolute_error: 114.9334\n",
            "Epoch 445: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 116.1922 - mean_absolute_error: 116.1922 - val_loss: 325.7178 - val_mean_absolute_error: 325.7178\n",
            "Epoch 446/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 118.2097 - mean_absolute_error: 118.2097\n",
            "Epoch 446: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 118.6455 - mean_absolute_error: 118.6455 - val_loss: 328.5775 - val_mean_absolute_error: 328.5775\n",
            "Epoch 447/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 121.8773 - mean_absolute_error: 121.8773\n",
            "Epoch 447: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 122.8064 - mean_absolute_error: 122.8064 - val_loss: 326.4049 - val_mean_absolute_error: 326.4049\n",
            "Epoch 448/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 124.5321 - mean_absolute_error: 124.5321\n",
            "Epoch 448: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 124.0389 - mean_absolute_error: 124.0389 - val_loss: 321.9141 - val_mean_absolute_error: 321.9141\n",
            "Epoch 449/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 120.8621 - mean_absolute_error: 120.8621\n",
            "Epoch 449: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 121.1853 - mean_absolute_error: 121.1853 - val_loss: 331.6921 - val_mean_absolute_error: 331.6921\n",
            "Epoch 450/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 124.1033 - mean_absolute_error: 124.1033\n",
            "Epoch 450: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 123.8192 - mean_absolute_error: 123.8192 - val_loss: 321.0046 - val_mean_absolute_error: 321.0046\n",
            "Epoch 451/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 131.4389 - mean_absolute_error: 131.4389\n",
            "Epoch 451: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 130.5073 - mean_absolute_error: 130.5073 - val_loss: 322.1883 - val_mean_absolute_error: 322.1883\n",
            "Epoch 452/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 129.2625 - mean_absolute_error: 129.2625\n",
            "Epoch 452: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 129.3947 - mean_absolute_error: 129.3947 - val_loss: 323.2258 - val_mean_absolute_error: 323.2258\n",
            "Epoch 453/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 122.9752 - mean_absolute_error: 122.9752\n",
            "Epoch 453: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 122.8800 - mean_absolute_error: 122.8800 - val_loss: 329.6310 - val_mean_absolute_error: 329.6310\n",
            "Epoch 454/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 118.0557 - mean_absolute_error: 118.0557\n",
            "Epoch 454: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 118.2830 - mean_absolute_error: 118.2830 - val_loss: 323.6814 - val_mean_absolute_error: 323.6814\n",
            "Epoch 455/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 126.8382 - mean_absolute_error: 126.8382\n",
            "Epoch 455: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 126.0407 - mean_absolute_error: 126.0407 - val_loss: 330.5225 - val_mean_absolute_error: 330.5225\n",
            "Epoch 456/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 132.6766 - mean_absolute_error: 132.6766\n",
            "Epoch 456: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 131.2691 - mean_absolute_error: 131.2691 - val_loss: 330.1308 - val_mean_absolute_error: 330.1308\n",
            "Epoch 457/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 121.2023 - mean_absolute_error: 121.2023\n",
            "Epoch 457: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 121.1635 - mean_absolute_error: 121.1635 - val_loss: 324.0991 - val_mean_absolute_error: 324.0991\n",
            "Epoch 458/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 120.2451 - mean_absolute_error: 120.2451\n",
            "Epoch 458: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 119.3284 - mean_absolute_error: 119.3284 - val_loss: 326.1874 - val_mean_absolute_error: 326.1874\n",
            "Epoch 459/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 124.4469 - mean_absolute_error: 124.4469\n",
            "Epoch 459: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.7116 - mean_absolute_error: 124.7116 - val_loss: 324.2853 - val_mean_absolute_error: 324.2853\n",
            "Epoch 460/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 133.2137 - mean_absolute_error: 133.2137\n",
            "Epoch 460: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 133.6822 - mean_absolute_error: 133.6822 - val_loss: 333.5412 - val_mean_absolute_error: 333.5412\n",
            "Epoch 461/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 122.8444 - mean_absolute_error: 122.8444\n",
            "Epoch 461: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 123.2156 - mean_absolute_error: 123.2156 - val_loss: 339.7803 - val_mean_absolute_error: 339.7803\n",
            "Epoch 462/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 123.4404 - mean_absolute_error: 123.4404\n",
            "Epoch 462: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 123.4404 - mean_absolute_error: 123.4404 - val_loss: 330.3474 - val_mean_absolute_error: 330.3474\n",
            "Epoch 463/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 125.3062 - mean_absolute_error: 125.3062\n",
            "Epoch 463: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 126.1912 - mean_absolute_error: 126.1912 - val_loss: 353.9460 - val_mean_absolute_error: 353.9460\n",
            "Epoch 464/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 120.2464 - mean_absolute_error: 120.2464\n",
            "Epoch 464: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 119.9678 - mean_absolute_error: 119.9678 - val_loss: 326.9214 - val_mean_absolute_error: 326.9214\n",
            "Epoch 465/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 123.7120 - mean_absolute_error: 123.7120\n",
            "Epoch 465: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 123.1673 - mean_absolute_error: 123.1673 - val_loss: 323.6807 - val_mean_absolute_error: 323.6807\n",
            "Epoch 466/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 118.3115 - mean_absolute_error: 118.3115\n",
            "Epoch 466: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 118.4006 - mean_absolute_error: 118.4006 - val_loss: 333.4520 - val_mean_absolute_error: 333.4520\n",
            "Epoch 467/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 121.0140 - mean_absolute_error: 121.0140\n",
            "Epoch 467: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 120.8254 - mean_absolute_error: 120.8254 - val_loss: 328.9659 - val_mean_absolute_error: 328.9659\n",
            "Epoch 468/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 123.4293 - mean_absolute_error: 123.4293\n",
            "Epoch 468: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 123.4293 - mean_absolute_error: 123.4293 - val_loss: 330.5590 - val_mean_absolute_error: 330.5590\n",
            "Epoch 469/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 124.9152 - mean_absolute_error: 124.9152\n",
            "Epoch 469: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 124.7112 - mean_absolute_error: 124.7112 - val_loss: 324.2786 - val_mean_absolute_error: 324.2786\n",
            "Epoch 470/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 116.2766 - mean_absolute_error: 116.2766\n",
            "Epoch 470: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 116.6294 - mean_absolute_error: 116.6294 - val_loss: 325.4138 - val_mean_absolute_error: 325.4138\n",
            "Epoch 471/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 117.2548 - mean_absolute_error: 117.2548\n",
            "Epoch 471: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 116.6176 - mean_absolute_error: 116.6176 - val_loss: 319.5251 - val_mean_absolute_error: 319.5251\n",
            "Epoch 472/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 117.8359 - mean_absolute_error: 117.8359\n",
            "Epoch 472: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 116.9627 - mean_absolute_error: 116.9627 - val_loss: 334.8183 - val_mean_absolute_error: 334.8183\n",
            "Epoch 473/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 116.6636 - mean_absolute_error: 116.6636\n",
            "Epoch 473: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 115.7563 - mean_absolute_error: 115.7563 - val_loss: 335.3149 - val_mean_absolute_error: 335.3149\n",
            "Epoch 474/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 119.5352 - mean_absolute_error: 119.5352\n",
            "Epoch 474: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 119.7130 - mean_absolute_error: 119.7130 - val_loss: 342.1276 - val_mean_absolute_error: 342.1276\n",
            "Epoch 475/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 129.3118 - mean_absolute_error: 129.3118\n",
            "Epoch 475: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 129.0439 - mean_absolute_error: 129.0439 - val_loss: 330.1480 - val_mean_absolute_error: 330.1480\n",
            "Epoch 476/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 114.8967 - mean_absolute_error: 114.8967\n",
            "Epoch 476: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 114.8967 - mean_absolute_error: 114.8967 - val_loss: 331.2320 - val_mean_absolute_error: 331.2320\n",
            "Epoch 477/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 118.2269 - mean_absolute_error: 118.2269\n",
            "Epoch 477: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 118.0468 - mean_absolute_error: 118.0468 - val_loss: 329.2717 - val_mean_absolute_error: 329.2717\n",
            "Epoch 478/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 119.2738 - mean_absolute_error: 119.2738\n",
            "Epoch 478: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 119.2568 - mean_absolute_error: 119.2568 - val_loss: 327.7234 - val_mean_absolute_error: 327.7234\n",
            "Epoch 479/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 112.6508 - mean_absolute_error: 112.6508\n",
            "Epoch 479: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 112.4714 - mean_absolute_error: 112.4714 - val_loss: 332.1889 - val_mean_absolute_error: 332.1889\n",
            "Epoch 480/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 116.2192 - mean_absolute_error: 116.2192\n",
            "Epoch 480: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 116.9262 - mean_absolute_error: 116.9262 - val_loss: 332.5862 - val_mean_absolute_error: 332.5862\n",
            "Epoch 481/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 127.7149 - mean_absolute_error: 127.7149\n",
            "Epoch 481: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 127.7962 - mean_absolute_error: 127.7962 - val_loss: 340.9819 - val_mean_absolute_error: 340.9819\n",
            "Epoch 482/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 114.4234 - mean_absolute_error: 114.4234\n",
            "Epoch 482: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 115.4603 - mean_absolute_error: 115.4603 - val_loss: 322.8473 - val_mean_absolute_error: 322.8473\n",
            "Epoch 483/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 116.6237 - mean_absolute_error: 116.6237\n",
            "Epoch 483: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 116.6534 - mean_absolute_error: 116.6534 - val_loss: 329.8866 - val_mean_absolute_error: 329.8866\n",
            "Epoch 484/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 117.8604 - mean_absolute_error: 117.8604\n",
            "Epoch 484: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 117.7792 - mean_absolute_error: 117.7792 - val_loss: 324.8074 - val_mean_absolute_error: 324.8074\n",
            "Epoch 485/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 131.7293 - mean_absolute_error: 131.7293\n",
            "Epoch 485: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 131.5158 - mean_absolute_error: 131.5158 - val_loss: 346.2391 - val_mean_absolute_error: 346.2391\n",
            "Epoch 486/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 135.6732 - mean_absolute_error: 135.6732\n",
            "Epoch 486: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 134.4682 - mean_absolute_error: 134.4682 - val_loss: 342.5807 - val_mean_absolute_error: 342.5807\n",
            "Epoch 487/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 119.5556 - mean_absolute_error: 119.5556\n",
            "Epoch 487: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 119.8781 - mean_absolute_error: 119.8781 - val_loss: 360.3023 - val_mean_absolute_error: 360.3023\n",
            "Epoch 488/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 139.8922 - mean_absolute_error: 139.8922\n",
            "Epoch 488: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 139.7361 - mean_absolute_error: 139.7361 - val_loss: 347.8563 - val_mean_absolute_error: 347.8563\n",
            "Epoch 489/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 122.1276 - mean_absolute_error: 122.1276\n",
            "Epoch 489: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 121.4669 - mean_absolute_error: 121.4669 - val_loss: 331.3643 - val_mean_absolute_error: 331.3643\n",
            "Epoch 490/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 121.6033 - mean_absolute_error: 121.6033\n",
            "Epoch 490: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 121.7286 - mean_absolute_error: 121.7286 - val_loss: 326.7869 - val_mean_absolute_error: 326.7869\n",
            "Epoch 491/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 118.9581 - mean_absolute_error: 118.9581\n",
            "Epoch 491: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 119.2033 - mean_absolute_error: 119.2033 - val_loss: 348.2545 - val_mean_absolute_error: 348.2545\n",
            "Epoch 492/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 112.9408 - mean_absolute_error: 112.9408\n",
            "Epoch 492: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 112.6128 - mean_absolute_error: 112.6128 - val_loss: 329.9318 - val_mean_absolute_error: 329.9318\n",
            "Epoch 493/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 117.3867 - mean_absolute_error: 117.3867\n",
            "Epoch 493: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 117.3867 - mean_absolute_error: 117.3867 - val_loss: 320.7557 - val_mean_absolute_error: 320.7557\n",
            "Epoch 494/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 112.1823 - mean_absolute_error: 112.1823\n",
            "Epoch 494: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 112.9158 - mean_absolute_error: 112.9158 - val_loss: 325.2043 - val_mean_absolute_error: 325.2043\n",
            "Epoch 495/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 125.7990 - mean_absolute_error: 125.7990\n",
            "Epoch 495: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 126.6390 - mean_absolute_error: 126.6390 - val_loss: 332.2654 - val_mean_absolute_error: 332.2654\n",
            "Epoch 496/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 113.4960 - mean_absolute_error: 113.4960\n",
            "Epoch 496: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 113.4391 - mean_absolute_error: 113.4391 - val_loss: 336.2186 - val_mean_absolute_error: 336.2186\n",
            "Epoch 497/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 113.0380 - mean_absolute_error: 113.0380\n",
            "Epoch 497: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 114.2739 - mean_absolute_error: 114.2739 - val_loss: 334.6510 - val_mean_absolute_error: 334.6510\n",
            "Epoch 498/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 113.8857 - mean_absolute_error: 113.8857\n",
            "Epoch 498: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 113.7202 - mean_absolute_error: 113.7202 - val_loss: 330.5605 - val_mean_absolute_error: 330.5605\n",
            "Epoch 499/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 114.6074 - mean_absolute_error: 114.6074\n",
            "Epoch 499: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 115.5944 - mean_absolute_error: 115.5944 - val_loss: 333.3391 - val_mean_absolute_error: 333.3391\n",
            "Epoch 500/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 113.5503 - mean_absolute_error: 113.5503\n",
            "Epoch 500: val_loss did not improve from 303.21402\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 113.2277 - mean_absolute_error: 113.2277 - val_loss: 325.8964 - val_mean_absolute_error: 325.8964\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcbf443d2e0>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O7pDMdKqlbs"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXqCMMRnqlbt",
        "outputId": "758ede63-a2e8-4ff4-c8f1-31ec8d83f6a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "111/111 [==============================] - 0s 2ms/step\n",
            "28/28 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "y_train_pred = NN_model.predict(X_train)\n",
        "y_test_pred = NN_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzJYvIA4qlbt"
      },
      "outputs": [],
      "source": [
        "y_train_pred = y_train_pred.flatten()\n",
        "y_test_pred = y_test_pred.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds7AZ1DeAhXN"
      },
      "outputs": [],
      "source": [
        "y_train = y_train * train_df2[\"floor_area_sqm\"]\n",
        "y_train_pred = y_train_pred * train_df2[\"floor_area_sqm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSKRSfr5AhXN"
      },
      "outputs": [],
      "source": [
        "y_test = y_test * test_df2[\"floor_area_sqm\"]\n",
        "y_test_pred = y_test_pred * test_df2[\"floor_area_sqm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOMevqbLAhXN"
      },
      "outputs": [],
      "source": [
        "train_n = X_train.shape[0]\n",
        "train_p = X_train.shape[1]\n",
        "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
        "train_rmse = mean_squared_error(y_train, y_train_pred, squared = False)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_adj_r2 = 1 - (1 - train_r2) * (train_n - 1) / (train_n - train_p - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsIr6UyAAhXO"
      },
      "outputs": [],
      "source": [
        "test_n = X_test.shape[0]\n",
        "test_p = X_test.shape[1]\n",
        "test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
        "test_rmse = mean_squared_error(y_test, y_test_pred, squared = False)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_adj_r2 = 1 - (1 - test_r2) * (test_n - 1) / (test_n - test_p - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P7aCEJMAhXO",
        "outputId": "293cd80d-59cb-4072-820d-062726ca27a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train MAPE: 2.49%\n",
            "Train RMSE: 24467.297240622403\n",
            "Train R2: 0.9791491195767242\n",
            "Train Adj R2: 0.9787084379696313\n",
            "\n",
            "Test MAPE: 5.45%\n",
            "Test RMSE: 42638.28415732599\n",
            "Test R2: 0.9334055580765319\n",
            "Test Adj R2: 0.9273889810215651\n"
          ]
        }
      ],
      "source": [
        "print(\"Train MAPE: {:.2f}%\".format(train_mape * 100))\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Train R2:\", train_r2)\n",
        "print(\"Train Adj R2:\", train_adj_r2)\n",
        "print()\n",
        "print(\"Test MAPE: {:.2f}%\".format(test_mape * 100))\n",
        "print(\"Test RMSE:\", test_rmse)\n",
        "print(\"Test R2:\", test_r2)\n",
        "print(\"Test Adj R2:\", test_adj_r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp9Udji5AhXO"
      },
      "outputs": [],
      "source": [
        "train_residuals = y_train - y_train_pred\n",
        "test_residuals = y_test - y_test_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ85tBG9lnaW"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "lba-Q5fUAhXO",
        "outputId": "2cafdb65-0386-4324-a848-7e414a6a6d89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#idk residual plot is vs predicted value or actual value lol\n",
        "plt.scatter(y_train_pred, train_residuals)\n",
        "plt.xlabel(\"Predicted Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "r3zIg4uPAhXO",
        "outputId": "906e0f7d-c0e7-4f78-ad88-4b19fb876afe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_train, train_residuals)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "mk33-hNuAhXO",
        "outputId": "cb784403-7145-420e-e551-a5135799d518"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Actual vs Predicted Price')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_train, y_train_pred)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
